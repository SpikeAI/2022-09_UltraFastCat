{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ultrafast image categorization *in vivo* and *in sillico*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we train a network to categorize images which contain or not a given class, such as *animals* or *artifacts*. We will use the [Pytorch](https://pytorch.org/) library to run the networks and the [pandas](https://pandas.pydata.org/docs/getting_started/index.html) library to collect and display the results. \n",
    "\n",
    "I uses transfer learning to train the DCNN, starting from a VGG16 network, taken from the `torchvision.models` library, pre-trained on the [Imagenet](http://image-net.org/) dataset which allows to perform label detection on naturals images for $K = 1000$ labels. Then re-train the whole network to perfom the same task but in a sub-set of $K = 1$ synset from the ImageNet dataset. The dataset I used here to train the networks is not multilabel (as we have the information about the occurence of only one synset on the scene), in order to train networks on independant task I choose to limit the output of the DCNN to $K = 1$ synset. \n",
    "\n",
    "We are going to adopt differents strategies of transfer learning:\n",
    "\n",
    "* VGG General : Substitute the last layer of the Pytorch VGG16 network ($K = 1000$ labels) with a new layer build from a specific subset ($K = 10$ labels).\n",
    "* VGG Gray : Same architecture as the VGG General network but trained with images in grayscale.\n",
    "* VGG Scale : Same architecture as the VGG General network but trained with images of different size.\n",
    "\n",
    "The first part consist of the training of the network and the last part of this notebook is dedicated to the test of the robustness of the resulted networks while appliying various geometric tranformations to the input. Finally I analyse the similarities in the performances of the networks compared with physiological data. \n",
    "\n",
    "This notebook was done by  [Jean-Nicolas Jérémie](https://github.com/JNJER) under the supervision of [Laurent PERRINET](https://laurentperrinet.github.io/) at the Neurosciences Institute of Timone (INT). It is curated in the following [github repo](https://github.com/JNJER/2022-09_UltraFastCat)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first install requirements"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "* normalize the fonts\n",
    "\n",
    "* rename experiments + backport on manuscript + make an acronym table:\n",
    " * `Gen` >> `TLC` (transfer Learning on classification layers)\n",
    " * `Full`>> `TLF` (transfer Learning on all layers)\n",
    " * `VGG16` >> `LUT` (look up table)\n",
    " * `Custom` >> `Aug` (Augment)\n",
    " * `Naive` >> `SL` Supervised Learning from Scratch  ou `TLA` (transfer Learning on all layers) - or `Naive` is good :-)\n",
    "\n",
    "* remove `HACK`s\n",
    "* restore an image size at 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install --upgrade -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of the libraries/variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our coding strategy is to include all scripts in this notebook. This notebook contains the scripts necessary to set the variables of the definition and the training of the networks and assumes you have generated the dataset based on the [ImageNet database](https://www.image-net.org/) dedicated to our specific ecologic task. \n",
    "\n",
    "The set of labels of the ImageNet database is based on a large lexical database of English: [Wordnet](https://wordnet.princeton.edu/) . The nouns, verbs, adjectives, and adverbs in this database are grouped into a graphical set of cognitive synonyms (synset), each expressing a distinct concept. These synsets are linked between them by employing a small number of conceptual relations. I used the hyperonym link, for instance, a German shepherd is kind of a dog and a dog is a kind of an animal thus defining an hyperonym path. So the synset 'animal' is in the hyperonym path of the synset 'German sheperd'. Based on this relation, we selected a specific subset of labels in the ImageNet database to build our datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cache = 'cached_data'\n",
    "%mkdir -p {data_cache}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, a `UltraFastCat/init.py` to define all our usefull variables like the new labels to learn, the number of training images or the root. Also, we importe libraries to train the networks and display the results. Then we define the `transform` functions for the datasets. \n",
    "HACK : To perform image augmentation we apply the Pytorch `AutoAugment` function to the `train` and `val` dataset. I also add a grayscale, shuffle and resize function in order to test different training strategies and test the networks on various conditions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: make a task of categorizing an animal in an image where the distractors is not an artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.0.0.dev20230220', device(type='mps'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import torch\n",
    "import torch.nn.functional as nnf\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "# from torchvision.datasets import ImageFolder\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(42*42)\n",
    "torch.set_printoptions(precision=3, linewidth=140, sci_mode=False)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "torch.__version__, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import argparse\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager\n",
    "import matplotlib\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "fontsize = 15\n",
    "font = font_manager.FontProperties(weight='normal', size=fontsize)\n",
    "import seaborn as sns\n",
    "\n",
    "# matplotlib parameters\n",
    "colors = ['b', 'r', 'k', 'g', 'm', 'y']\n",
    "fig_width = 15\n",
    "phi = (np.sqrt(5)+1)/2 # golden ratio for the figures :-)\n",
    "\n",
    "#to plot & display \n",
    "def pprint(message): #display function\n",
    "    print('-'*len(message))\n",
    "    print(message)\n",
    "    print('-'*len(message))\n",
    "\n",
    "do_savefig = True\n",
    "# figpath = '../2022-12-15_Jérémie-etal-Vision_630dcf8012267a5bb40967dd/figures'\n",
    "figpath = 'figures'\n",
    "%mkdir -p {figpath}\n",
    "opts_savefig = dict(dpi='figure', bbox_inches='tight', pad_inches=0, edgecolor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating file cached_data/2023-03-05_config_args.json\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import os\n",
    "import time\n",
    "\n",
    "from time import strftime, gmtime\n",
    "datetag = strftime(\"%Y-%m-%d\", gmtime())\n",
    "\n",
    "HOST = os.uname()[1].replace('.local', '')\n",
    "\n",
    "HOST, datetag = \"obiwan\", \"2023-03-05\"\n",
    "\n",
    "# to store results\n",
    "import pandas as pd\n",
    "\n",
    "def arg_parse(args=[]):\n",
    "    DEBUG = 8\n",
    "    DEBUG = 1\n",
    "    parser = argparse.ArgumentParser(description='DCNN_transfer_learning/init.py set root')\n",
    "    parser.add_argument(\"--datasets_root\", dest = 'datasets_root', help = \"Directory containing datasets to perform the training\",\n",
    "                        default = '../data/', type = str)\n",
    "    parser.add_argument(\"--tasks\", dest = 'tasks', help =  \"The different tasks we explore here\",\n",
    "                        default = ['animal', 'artifact', 'random'], type = list)\n",
    "    parser.add_argument(\"--folders\", dest = 'folders', help =  \"Set the training, validation and testing folders relative to the root of each task\",\n",
    "                        default = ['test', 'val', 'train'], type = list)\n",
    "    parser.add_argument(\"--N_images\", dest = 'N_images', help =\"Set the number of images per classe in the train folder\",\n",
    "                        default = [600//DEBUG, 400//DEBUG, 1000//DEBUG], type = list)\n",
    "    parser.add_argument(\"--goals\", dest = 'goals', help =  \"Set the different categories for the tasks\",\n",
    "                        default = ['target', 'distractor'], type = list)\n",
    "    parser.add_argument(\"--image_size\", dest = 'image_size', help = \"Set the default image_size of the input\",\n",
    "                    default = 224)\n",
    "    parser.add_argument(\"--num_epochs\", dest = 'num_epochs', help = \"Set the number of epoch to train the models\",\n",
    "                    default = 25//DEBUG)\n",
    "    parser.add_argument(\"--batch_size\", dest = 'batch_size', help=\"Set the batch size\", default = 32)\n",
    "    parser.add_argument(\"--lr\", dest = 'lr', help=\"Set the learning rate\", default = 0.00005*DEBUG)\n",
    "    parser.add_argument(\"--momentum\", dest = 'momentum', help=\"Set the momentum\", default = 0.90)\n",
    "    parser.add_argument(\"--url_loader\", dest = 'url_loader', help = \"Set the file containing ImageNet urls\",\n",
    "                        default = '../DataSetMaker/Imagenet_urls_ILSVRC_2016.json', type = str)\n",
    "    parser.add_argument(\"--model_names\", dest = 'model_names', help = \"Modes for the new trained networks\",\n",
    "                        default = ['vgg16_augment', 'vgg16_full','vgg16_tlc', 'vgg16_naive'], type = list)\n",
    "    return parser.parse_args(args = args)\n",
    "\n",
    "args = arg_parse()\n",
    "\n",
    "json_fname = os.path.join(data_cache, datetag + '_config_args.json')\n",
    "print(f'Creating file {json_fname}')\n",
    "with open(json_fname, 'wt') as f:\n",
    "    json.dump(vars(args), f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(datasets_root='../data/', tasks=['animal', 'artifact', 'random'], folders=['test', 'val', 'train'], N_images=[600, 400, 1000], goals=['target', 'distractor'], image_size=224, num_epochs=25, batch_size=32, lr=5e-05, momentum=0.9, url_loader='../DataSetMaker/Imagenet_urls_ILSVRC_2016.json', model_names=['vgg16_augment', 'vgg16_full', 'vgg16_tlc', 'vgg16_naive'])\n"
     ]
    }
   ],
   "source": [
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4', download_dir=args.datasets_root)\n",
    "nltk.download('wordnet', download_dir=args.datasets_root)\n",
    "nltk.data.path.append(args.datasets_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCCN training\n",
    "print(f'On date {datetag}, Running benchmark on host {HOST} with device {device}')\n",
    "\n",
    "paths = {}\n",
    "for task in args.tasks :\n",
    "    paths[task] = {}\n",
    "    for folder in args.folders:\n",
    "        paths[task][folder] = os.path.join(args.datasets_root, task, folder) # data path\n",
    "\n",
    "\n",
    "match = {}\n",
    "\n",
    "#----------------Get the label for the Imagenet categorization------------------------\n",
    "with open(args.url_loader) as json_file:\n",
    "    Imagenet_urls_ILSVRC_2016 = json.load(json_file)\n",
    "match = {}\n",
    "for task in args.tasks :\n",
    "    match[task] = []\n",
    "for i_img, img_id in enumerate(Imagenet_urls_ILSVRC_2016):\n",
    "    syn_= wn.synset_from_pos_and_offset('n', int(img_id.replace('n','')))\n",
    "    sem_ = syn_.hypernym_paths()[0]\n",
    "    for task in args.tasks[:-1]:\n",
    "        for i in np.arange(len(sem_)):\n",
    "            if sem_[i].lemmas()[0].name() in task :\n",
    "                match[task].append(i_img)\n",
    "    \n",
    "match['random'] = []\n",
    "random_file = '../data/random/random_choice_distractors.json'\n",
    "if os.path.exists(random_file):\n",
    "    print('Random mode : Using last random selection')\n",
    "    with open(random_file) as f:\n",
    "        class_wnids = list(json.load(f))\n",
    "    for i_img, img_id in enumerate(Imagenet_urls_ILSVRC_2016):\n",
    "        if img_id[:9] not in class_wnids:\n",
    "            match['random'].append(i_img)\n",
    "else:\n",
    "    print('Missing file: something is wrong here')\n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# class ShufflePatches(object): #https://stackoverflow.com/questions/66962837/shuffle-patches-in-image-batch\n",
    "#     def __init__(self, patch_size):\n",
    "#         self.ps = patch_size\n",
    "\n",
    "#     def __call__(self, x):\n",
    "#         # divide the batch of images into non-overlapping patches\n",
    "#         u = nnf.unfold(x.unsqueeze(dim=0), kernel_size=self.ps, stride=self.ps, padding=0)\n",
    "#         # permute the patches of each image in the batch\n",
    "#         pu = torch.cat([b_[:, torch.randperm(b_.shape[-1])][None,...] for b_ in u], dim=0)\n",
    "#         # fold the permuted patches back together\n",
    "#         f = nnf.fold(pu, x.shape[-2:], kernel_size=self.ps, stride=self.ps, padding=0)\n",
    "#         return f.squeeze(dim=0)\n",
    "\n",
    "# normalization used to train VGG\n",
    "# see https://pytorch.org/hub/pytorch_vision_vgg/\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "transforms_norm = transforms.Normalize(mean=mean, std=std) # to normalize colors on the ImageNet dataset\n",
    "\n",
    "\n",
    "# import sklearn.metrics\n",
    "# from scipy import stats\n",
    "# from scipy.special import logit, expit\n",
    "\n",
    "image_datasets = {}\n",
    "dataloaders = {}\n",
    "dataset_sizes = {}\n",
    "\n",
    "# VGG-16 datasets initialisation\n",
    "def datasets_transforms(paths=paths, image_size=args.image_size, p=0, shuffle=args.image_size , num_workers=0, angle=0, batch_size=args.batch_size, **kwargs):\n",
    "\n",
    "\n",
    "    if p != 0 : \n",
    "        transforms_seq = transforms.Compose([\n",
    "                            # transforms.AutoAugment(), # https://pytorch.org/vision/master/transforms.html#torchvision.transforms.AutoAugmentdistractor\n",
    "                            transforms.RandomHorizontalFlip(p=p),\n",
    "                            transforms.RandomVerticalFlip(p=p),\n",
    "                            transforms.RandomRotation(degrees=180, expand=True),\n",
    "                            transforms.RandomGrayscale(p=p),\n",
    "#                             https://pytorch.org/vision/master/generated/torchvision.transforms.RandomResizedCrop.html#torchvision.transforms.RandomResizedCrop\n",
    "#                             scale (tuple of python:float) – Specifies the lower and upper bounds for the random area of the crop, before resizing. The scale is defined with respect to the area of the original image.\n",
    "# ratio (tuple of python:float) – lower and upper bounds for the random aspect ratio of the crop, before resizing.\n",
    "                            transforms.RandomResizedCrop(image_size, scale=(0.7, 1.0)),\n",
    "                            # transforms.Resize((int(image_size), int(image_size))),              \n",
    "                            transforms.ToTensor(),\n",
    "                            transforms_norm ])\n",
    "    else:\n",
    "        transforms_seq = transforms.Compose([\n",
    "                            transforms.Resize((int(image_size), int(image_size))),\n",
    "                            transforms.ToTensor(),    # Convert the image to pyTorch Tensor data type.\n",
    "                            transforms_norm ])\n",
    "    \n",
    "    data_transforms = {\n",
    "        'train': transforms_seq,\n",
    "        'val': transforms_seq,\n",
    "        'test': transforms_seq}\n",
    "    \n",
    "\n",
    "    for task in args.tasks:\n",
    "        image_datasets[task] = {\n",
    "            folder: datasets.ImageFolder(\n",
    "                paths[task][folder], \n",
    "                transform=data_transforms[folder]\n",
    "            )\n",
    "            for folder in args.folders\n",
    "        }\n",
    "\n",
    "        dataloaders[task] = {\n",
    "            folder: torch.utils.data.DataLoader(\n",
    "                image_datasets[task][folder], batch_size=batch_size,\n",
    "                shuffle=False if folder == \"test\" else True, num_workers=num_workers\n",
    "            )\n",
    "            for folder in args.folders\n",
    "        }\n",
    "\n",
    "        dataset_sizes[task] = {folder: len(image_datasets[task][folder]) for folder in args.folders}\n",
    "\n",
    "    return dataset_sizes, dataloaders, image_datasets, data_transforms\n",
    "\n",
    "(dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(paths=paths, image_size=args.image_size)\n",
    "\n",
    "for task in args.tasks :\n",
    "    pprint(f'Pre-selected tasks {task} : ')\n",
    "    for folder in args.folders : print(f\"Loaded {dataset_sizes[task][folder]} images under {folder}\")\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "#----------------------------Display randomly picked images------------------------------------\n",
    "print('\\n')\n",
    "pprint(f'Some sample images picked at random :')\n",
    "# import imageio\n",
    "import imageio.v2 as imageio\n",
    "N_image_i = 4\n",
    "image_plot_paths = {}\n",
    "task_folder = {}\n",
    "x = 0\n",
    "folder = 'train'\n",
    "fig, axs = plt.subplots(len(args.tasks)*len(args.goals), N_image_i, figsize=(fig_width, fig_width))\n",
    "for task in args.tasks:\n",
    "    for goal in args.goals:\n",
    "        task_goal_folder = os.path.join(paths[task][folder], goal)\n",
    "        image_plot_paths = os.listdir(task_goal_folder)\n",
    "        for i_image in np.arange(N_image_i):\n",
    "            ax = axs[x][i_image]\n",
    "            path = os.path.join(task_goal_folder, random.choice(image_plot_paths))\n",
    "            ax.imshow(imageio.imread(path))\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])  \n",
    "            if i_image%5 == 0:\n",
    "                ax.set_ylabel(task + ' ' + goal, font=font)\n",
    "        x +=1\n",
    "fig.set_facecolor(color='white')\n",
    "#-------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training process\n",
    "\n",
    "This part of the notebook focus on the training process of the network. \n",
    "\n",
    "The following script is a classic Training script with pyTorch. Since I only have one synset to discriminate in the scene I use `criterion = nn.BCEWithLogitsLoss()` to compute the loss during the training process. For further statistical analyses, we extract factors (like the accuracy and loss) in a `pandas`' `DataFrame` object. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, num_epochs, dataloaders, lr=args.lr, momentum=args.momentum, batch_size=args.batch_size, # HACK beta2=args.beta2, \n",
    "                log_interval=100):#, **kwargs):\n",
    "    model.to(device)\n",
    "    # https://towardsdatascience.com/cross-entropy-for-classification-d98e7f974451\n",
    "    # https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#bcewithlogitsloss\n",
    "    criterion = nn.BCEWithLogitsLoss() #binary_cross_entropy_with_logits\n",
    "    # HACK if beta2 > 0.: \n",
    "    #     optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(momentum, beta2)) \n",
    "    # else:\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "        \n",
    "    # model.train()\n",
    "    df_train = pd.DataFrame([], columns=['epoch', 'avg_loss', 'avg_acc', 'avg_loss_val', 'avg_acc_val']) \n",
    "    for epoch in range(num_epochs):\n",
    "        loss_train = 0\n",
    "        acc_train = 0\n",
    "        num_train = 0\n",
    "        for i, (images, labels) in enumerate(dataloaders['train']):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(images).squeeze(1)\n",
    "            loss = criterion(outputs, labels.float())            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item() * images.size(0)\n",
    "            \n",
    "            preds = (outputs > 0).long()\n",
    "            acc_train += (preds == labels).sum().item() # count correct predictions\n",
    "            num_train += len(labels)\n",
    "        avg_loss = loss_train / num_train\n",
    "        avg_acc = acc_train / num_train\n",
    "           \n",
    "        with torch.no_grad():\n",
    "            loss_val = 0\n",
    "            acc_val = 0\n",
    "            num_val = 0\n",
    "            for i, (images, labels) in enumerate(dataloaders['val']):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(images).squeeze(1)\n",
    "\n",
    "                loss = criterion(outputs, labels.float())\n",
    "                loss_val += loss.item() * images.size(0)\n",
    "\n",
    "                preds = (outputs > 0).long()\n",
    "                acc_val += (preds == labels).sum().item() # count correct predictions\n",
    "                num_val += len(labels)\n",
    "            avg_loss_val = loss_val / num_val\n",
    "            avg_acc_val = acc_val / num_val\n",
    "        \n",
    "        df_train.loc[epoch] = {'epoch':epoch, 'avg_loss':avg_loss, 'avg_acc':avg_acc, 'avg_loss_val':avg_loss_val, 'avg_acc_val':avg_acc_val}\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} : train= loss: {avg_loss:.4f} / acc : {avg_acc:.4f} - val= loss : {avg_loss_val:.4f} / acc : {avg_acc_val:.4f}\")\n",
    "\n",
    "    # if device=='cuda': torch.cuda.empty_cache()\n",
    "    # model.cpu()\n",
    "    return model, df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and saving the networks\n",
    "models_vgg = {}\n",
    "opt = {}\n",
    "all_models = []   # HACK\n",
    "# model_filenames = {}\n",
    "\n",
    "# updating dictionary of studied models with the pretrained VGG16 and freezing all layers\n",
    "model_name = 'vgg_16'\n",
    "models_vgg[model_name] = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.DEFAULT)\n",
    "for param in models_vgg[model_name].features.parameters():\n",
    "    param.requires_grad = False \n",
    "for param in models_vgg[model_name].classifier.parameters():\n",
    "    param.requires_grad = False \n",
    "\n",
    "# now loading pretrained VGG16 and retrain them\n",
    "for task in args.tasks :\n",
    "    for model in args.model_names:\n",
    "        model_name = f'{model}_{task}'\n",
    "\n",
    "        all_models.append(model_name)\n",
    "        filename = f'{data_cache}/{datetag}_{HOST}_train_{model_name}.json'\n",
    "        # model_filenames[model_name] = filename.replace('json', 'pt')\n",
    "        model_filename = filename.replace('json', 'pt')\n",
    "        print(filename)\n",
    "        \n",
    "        if model=='naive':\n",
    "            models_vgg[model_name] = torchvision.models.vgg16()\n",
    "        else:\n",
    "            models_vgg[model_name] = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.DEFAULT)\n",
    "            \n",
    "        if model=='full':\n",
    "            for param in models_vgg[model_name].features.parameters():\n",
    "                param.requires_grad = True\n",
    "        else: # model = 'tlc' or 'augment'\n",
    "            for param in models_vgg[model_name].features.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        p = 0.5 if 'augment' in model_name else 0\n",
    "                \n",
    "        num_features = models_vgg[model_name].classifier[-1].in_features\n",
    "        features = list(models_vgg[model_name].classifier.children())[:-1] # Remove last layer\n",
    "        features.extend([nn.Linear(num_features, 1)]) # Add our layer with `n_output=1` outputs\n",
    "        models_vgg[model_name].classifier = nn.Sequential(*features) # Replace the model classifier\n",
    "        \n",
    "        if os.path.isfile(filename):\n",
    "            models_vgg[model_name].load_state_dict(torch.load(model_filename))\n",
    "        else:\n",
    "            print(\"Re-training pretrained model...\", model_filename)\n",
    "            print(f\"Traning {model_name}, image_size={args.image_size}\")\n",
    "            since = time.time()\n",
    "            (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(paths=paths, \n",
    "                                                                                                image_size=args.image_size, \n",
    "                                                                                                batch_size=args.batch_size, \n",
    "                                                                                                p=p)\n",
    "            models_vgg[model_name], df_train = train_model(models_vgg[model_name], num_epochs=args.num_epochs,\n",
    "                                                           dataloaders=dataloaders[task], lr=args.lr, \n",
    "                                                           momentum=args.momentum)\n",
    "            elapsed_time = time.time() - since\n",
    "            print(f\"Training completed in {elapsed_time // 60:.0f}m {elapsed_time % 60:.0f}s\")\n",
    "            torch.save(models_vgg[model_name].state_dict(), model_filename)\n",
    "            df_train.to_json(filename)\n",
    "            print()\n",
    "models_vgg.keys()            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models, args.model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model_name in all_models:\n",
    "    filename = f'{data_cache}/{datetag}_{HOST}_train_{model_name}.json'\n",
    "    df_train = pd.read_json(filename)\n",
    "    fig, axs = plt.subplots(figsize=(fig_width, fig_width/phi/2))\n",
    "    ax = df_train['avg_loss'].plot(lw=2, marker='.', markersize=10)\n",
    "    ax = df_train['avg_loss_val'].plot(lw=2, marker='.', markersize=10)\n",
    "    ax.legend([\"avg_loss\", \"avg_loss_val\"], fontsize=18);\n",
    "    ax.set_xlabel(\"Epoch\", size=18)\n",
    "    ax.spines['left'].set_position(('axes', -0.01))\n",
    "    ax.set_xlim(-0.5, args.num_epochs)\n",
    "    ax.grid(which='both')\n",
    "    for side in ['top', 'right'] :ax.spines[side].set_visible(False)\n",
    "    # ax.set_ylim(0., 1.1)\n",
    "    axs.set_title(f'Average values of the loss by epoch : {filename}', fontsize=20)\n",
    "    ax.get_legend().remove()\n",
    "    fig.legend(bbox_to_anchor=(1.05, .5), loc='lower right', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model_name in all_models:\n",
    "    filename = f'{data_cache}/{datetag}_{HOST}_train_{model_name}.json'\n",
    "    df_train = pd.read_json(filename)\n",
    "    fig, axs = plt.subplots(figsize=(fig_width, fig_width/phi/2))\n",
    "    ax = df_train['avg_acc'].plot(lw=2, marker='.', markersize=10)\n",
    "    ax = df_train['avg_acc_val'].plot(lw=2, marker='.', markersize=10)\n",
    "    ax.set_xlabel(\"Epoch\", size=18)\n",
    "    ax.spines['left'].set_position(('axes', -0.01))\n",
    "    # ax.set_ylim(0.70, .998)\n",
    "    ax.set_yscale(\"logit\", one_half=\"1/2\", use_overline=True)\n",
    "    ax.grid(which='both')\n",
    "    ax.set_xlim(-0.5, args.num_epochs+.5)\n",
    "    for side in ['top', 'right'] :ax.spines[side].set_visible(False)\n",
    "    axs.set_title(f'Average values of the accuracy by epoch : {filename}', size=20)\n",
    "    ax.legend([\"avg_acc\", \"avg_acc_val\"], fontsize=18);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scan of some parameters\n",
    "\n",
    "We check the scan of parameters on the generic 'TLC' model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_dicts = {'batch_size' : [8, 13, 21, 34, 55],\n",
    "             'lr': args.lr * np.logspace(-1, 1, 7, base=10),\n",
    "             'momentum': 1 - np.logspace(-2, -.5, 7, base=10),\n",
    "            #  'beta2': 1 - np.logspace(-6, -3, 7, base=10)\n",
    "            }\n",
    "\n",
    "N_val_avg=10\n",
    "\n",
    "\n",
    "for key in scan_dicts:\n",
    "    filename = f'{data_cache}/{datetag}_train_scan_{key}_{HOST}.json'\n",
    "    print(f'{filename=}')\n",
    "    if os.path.isfile(filename):\n",
    "        df_scan = pd.read_json(filename)\n",
    "    else:\n",
    "        i_trial = 0\n",
    "        measure_columns = [key, 'avg_loss_val', 'avg_acc_val', 'time']\n",
    "\n",
    "        df_scan = pd.DataFrame([], columns=measure_columns) \n",
    "        for i_trial, value in enumerate(scan_dicts[key]):\n",
    "            new_kwarg = {key: value}\n",
    "            print('trial', i_trial, ' /', len(scan_dicts[key]))\n",
    "            print('new_kwarg', new_kwarg)\n",
    "            # Training and saving the network\n",
    "            models_vgg_ = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.DEFAULT)\n",
    "            # Freeze training for all layers\n",
    "            # Newly created modules have requires_grad=True by default\n",
    "            for param in models_vgg_.features.parameters():\n",
    "                param.requires_grad = False \n",
    "\n",
    "            num_features = models_vgg_.classifier[-1].in_features\n",
    "            features = list(models_vgg_.classifier.children())[:-1] # Remove last layer\n",
    "            features.extend([nn.Linear(num_features, 1)]) # Add our layer with `n_output=1` outputs\n",
    "            models_vgg_.classifier = nn.Sequential(*features) # Replace the model classifier\n",
    "\n",
    "            since = time.time()\n",
    "            (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(paths=paths, image_size=args.image_size, p=0, **new_kwarg)\n",
    "            models_vgg_, df_train = train_model(models_vgg_, num_epochs=args.num_epochs, dataloaders=dataloaders['animal'], **new_kwarg)\n",
    "            elapsed_time = time.time() - since\n",
    "\n",
    "            print(f\"Training completed in {elapsed_time // 60:.0f}m {elapsed_time % 60:.0f}s\")\n",
    "            print(df_train)\n",
    "            df_scan.loc[i_trial] = {key:value, 'avg_loss_val':df_train.iloc[-N_val_avg:-1]['avg_loss_val'].mean(), \n",
    "                                'avg_acc_val':df_train.iloc[-N_val_avg:-1]['avg_acc_val'].mean(), 'time':elapsed_time}\n",
    "            print(df_scan.loc[i_trial])\n",
    "            i_trial += 1\n",
    "        df_scan.to_json(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "subplotpars = matplotlib.figure.SubplotParams(left=0.1, right=.95, bottom=0.25, top=.975, hspace=.6)\n",
    "dfs_ = {}\n",
    "for key in scan_dicts:\n",
    "    filename = f'{data_cache}/{datetag}_train_scan_{key}_{HOST}.json'\n",
    "    dfs_[str(key)]  = pd.read_json(filename)\n",
    "\n",
    "fig, axs = plt.subplots(len(dfs_), 1, figsize=(fig_width, fig_width*len(dfs_)/(phi*2)), subplotpars=subplotpars)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.tick_params(axis='both', which='major', labelsize=10)\n",
    "\n",
    "for ax, df_train, key in zip(axs, dfs_, scan_dicts):\n",
    "    # ax.plot(range(len(scan_dicts[key])), dfs_[df_train][\"avg_acc_val\"], alpha=0.5, lw=2, marker='.')\n",
    "    ax.plot(scan_dicts[key], dfs_[df_train][\"avg_acc_val\"], alpha=0.5, lw=2, marker='.')\n",
    "    ax.set_ylabel(f\"Accuracy for {key}\", size=18)\n",
    "    ax.set_xlabel(f\"Parameter : {key}\", size=18)\n",
    "    # ax.set_xticks(range(len(scan_dicts[key])))\n",
    "    # ax.set_xticklabels(scan_dicts[key], size=10)\n",
    "    # ax.set_xticks(range(len(scan_dicts[key])))\n",
    "    # ax.set_xticklabels([f'{s:.4f}' for s in scan_dicts[key]], rotation=60, size=20)\n",
    "    if type(scan_dicts[key][0]) == np.float64: ax.xaxis.set_major_formatter(FormatStrFormatter('%.4f'))\n",
    "    ax.spines['left'].set_position(('axes', -0.01))\n",
    "    # ax.set_ylim(0.8, .97)\n",
    "    ax.set_yscale(\"logit\", one_half=\"1/2\", use_overline=True)\n",
    "    ax.grid(which='both')\n",
    "    for side in ['top', 'right'] :ax.spines[side].set_visible(False)\n",
    "    #ax.get_legend().remove()\n",
    "axs[0].set_title(f'Average values of the accuracy for different parameters :', size=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfs_ = {}\n",
    "for key in scan_dicts:\n",
    "    filename = f'{data_cache}/{datetag}_train_scan_{key}_{HOST}.json'\n",
    "    dfs_[str(key)]  = pd.read_json(filename)\n",
    "fig, axs = plt.subplots(len(dfs_), 1, figsize=(fig_width, fig_width*len(dfs_)/(phi*2)), subplotpars=subplotpars)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.tick_params(axis='both', which='major', labelsize=10)\n",
    "for ax, df_train, key in zip(axs, dfs_, scan_dicts):\n",
    "    ax.plot(scan_dicts[key], dfs_[df_train][\"avg_loss_val\"], alpha=0.5, lw=2, marker='.')\n",
    "    ax.set_ylabel(f\"Loss value for {key}\", size=18)    \n",
    "    ax.set_xlabel(f\"Parameter :{key}\", size=16)\n",
    "    # ax.set_xticks(range(len(scan_dicts[key])))\n",
    "    # ax.set_xticklabels(scan_dicts[key], size=10)\n",
    "    if type(scan_dicts[key][0]) == np.float64: ax.xaxis.set_major_formatter(FormatStrFormatter('%.2e'))\n",
    "    ax.grid(which='both')\n",
    "    for side in ['top', 'right'] :ax.spines[side].set_visible(False)\n",
    "axs[0].set_title(f'Average values of the loss for different parameters :', size=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the new networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the networks seems ready for the comparison. The second part of this notebook offers a comparison between:\n",
    "\n",
    "- A pre-trained image recognition's networks, here VGG, trained on the [ImageNet](http://image-net.org/) dataset which allows working on naturals images for $1000$ labels, taken from the `torchvision.models` library\n",
    "\n",
    "- And three re-trained version of the same network VGG16 based on a Wordnet semantic from the ImageNet database which allows working on naturals images for $1$ synsets. \n",
    "\n",
    "As a control I re-train networks on the artifact synset too. Another interesting analysis will be the study of the link between the performances on these two conditions. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exp1\"></a>\n",
    "## Experiment 1: Image processing and recognition for different labels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_vgg.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'{data_cache}/{datetag}_{HOST}_results_basic.json'\n",
    "print(f'{filename=}')\n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    df = pd.read_json(filename)\n",
    "else:\n",
    "    i_trial = 0\n",
    "    df = pd.DataFrame([], columns=['model', 'model_task', 'task', 'goal', 'likelihood', 'fps', 'time', 'i_image', 'filename', 'top_1']) \n",
    "        # image preprocessing\n",
    "    (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(paths=paths, image_size=args.image_size, batch_size=1)\n",
    "    for task in args.tasks:\n",
    "        pprint(task)\n",
    "        for i_image, (data, label) in enumerate(dataloaders[task]['test']):\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            for model_name in models_vgg.keys():\n",
    "                model = models_vgg[model_name].to(device)\n",
    "                with torch.no_grad():\n",
    "                    goal = 'target' if 'target' in image_datasets[task]['test'].imgs[i_image][0] else 'distractor'\n",
    "                    model_task = 'animal' if 'animal' in model_name else 'artifact'\n",
    "                    tic = time.time()\n",
    "                    out = model(data).cpu().squeeze(0)\n",
    "                    if model_name == 'vgg_16':\n",
    "                        model_task = 'Imagenet_challenge'\n",
    "                        percentage = torch.nn.functional.softmax(out, dim=0)\n",
    "                        likelihood = np.asarray(percentage[match[task]]).sum()\n",
    "                    else:\n",
    "                        likelihood = torch.sigmoid(out).numpy()[0]\n",
    "                    top_1 = 'target' if likelihood>0.50 else 'distractor'\n",
    "                    elapsed_time = time.time() - tic\n",
    "                df.loc[i_trial] = { 'model':model_name,'model_task':model_task, 'task':task, 'top_1':top_1, 'goal':goal, \n",
    "                                    'likelihood':likelihood, 'time':elapsed_time, 'fps': 1/elapsed_time,\n",
    "                                    'i_image':i_image, 'filename':image_datasets[task]['test'].imgs[i_image][0]}\n",
    "                print(f'The {model_name} model categorize {model_task} with {likelihood*100:.3f} % likelihood ({top_1}) in {elapsed_time:.3f} seconds, groundtruth : {task}, {goal}')\n",
    "                i_trial += 1\n",
    "    df.to_json(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'{data_cache}/{datetag}_{HOST}_results_basic.json'\n",
    "df = pd.read_json(filename)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computation Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(models_vgg), 1, figsize=(fig_width, fig_width*phi/2), sharex=True, sharey=True)\n",
    "cmap = plt.cm.get_cmap('inferno')\n",
    "color_dict = pd.Series({k:cmap(i/len(df['model'].unique())) for i,k in enumerate(df['model'].unique())})\n",
    "for ax, color, model_name in zip(axs, color_dict, models_vgg):\n",
    "    df[df['model']==model_name]['time'].plot.hist(bins=50, lw=1, label=model_name,ax=ax, color=[color_dict[model_name],], density=True)\n",
    "    ax.set_xlim(df['time'].quantile(.01), df['time'].quantile(.99))\n",
    "    ax.set_ylim(0, 600)\n",
    "    ax.legend(bbox_to_anchor=(1.21, .35), loc='lower right', fontsize=15)\n",
    "    ax.grid(which='both', axis='y')\n",
    "    ax.set_xlabel('Processing time (s): ' + model_name, size=22)\n",
    "    ax.set_ylabel('')\n",
    "axs[0].set_title('Distribution of the Processing time (s). Processed on : ' + HOST, size=20)\n",
    "axs[len(models_vgg)//2].set_ylabel('Frequency', fontsize=22);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy comparison for all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc = pd.DataFrame({model_name: {task: accuracy_score(df[(df['model']==model_name) & (df['task']==task)][\"top_1\"], \n",
    "                                                             df[(df['model']==model_name) & (df['task']==task)][\"goal\"])\n",
    "                                    for task in args.tasks} \n",
    "                       for model_name in df['model'].unique()})\n",
    "\n",
    "ax = df_acc.T.plot.bar(rot=90, figsize=(fig_width, fig_width//4), fontsize=fontsize)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.hlines(xmin=-.5, xmax=len(models_vgg)-.5, y=1/2, ls='--', ec='k', label='chance level')\n",
    "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
    "for container in ax.containers: ax.bar_label(container, padding=-36, color='black', fontsize=fontsize, fmt='%.2f', weight='bold', rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1.2, .35), loc='lower right', fontsize=18)\n",
    "ax.set_title(f'Animal vs Artifact images f1 score for re-trained models', size=22)\n",
    "ax.set_ylabel('Accuracy', size=20)\n",
    "ax.set_xlabel('Model', size=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_dict = df_acc.to_dict()\n",
    "model_tested = ['Vgg ImageNet', 'Vgg Animal', 'Vgg Artifact']\n",
    "fig, axs = plt.subplots(1, len(model_tested), sharex=False, sharey=True, figsize=(fig_width, fig_width//4), gridspec_kw={'width_ratios': [3, 3,1]})\n",
    "cmap = plt.cm.get_cmap('viridis_r')\n",
    "color_dict = pd.Series({k:cmap(i/len(model_tested)) for i,k in enumerate(model_tested)})\n",
    "titles = [\"(A) Animal dataset\", \"(B) Artifact dataset\" , \"(C) Random dataset\" ]\n",
    "\n",
    "F1_Random = .612  # HACK\n",
    "data_rand = { 'x_abs': ['Vgg Random'],\n",
    "        'y_val' : [ F1_Random]}\n",
    "data_ani = { 'x_abs': ['Vgg ImageNet', 'Vgg Animal', 'Vgg Artifact'],\n",
    "             'y_val' : [acc_dict['vgg_16']['animal'], acc_dict['vgg16_tlc_animal']['animal'], acc_dict['vgg16_tlc_artifact']['animal']]}\n",
    "data_art = { 'x_abs': ['Vgg ImageNet', 'Vgg Animal', 'Vgg Artifact'],\n",
    "             'y_val' : [acc_dict['vgg_16']['artifact'], acc_dict['vgg16_tlc_animal']['artifact'], acc_dict['vgg16_tlc_artifact']['artifact']]}\n",
    "df_rand = pd.DataFrame(data_rand)\n",
    "df_ani = pd.DataFrame(data_ani)\n",
    "df_art = pd.DataFrame(data_art)\n",
    "\n",
    "axs[2].bar(df_rand['x_abs'], df_rand['y_val'],  align='center', color= color_dict[0])\n",
    "axs[0].bar(df_ani['x_abs'], df_ani['y_val'], color= color_dict[1])\n",
    "axs[1].bar(df_art['x_abs'], df_art['y_val'], color= color_dict[2])\n",
    "axs[2].hlines(xmin=-.5, xmax=.5, y=1/2, ls='--', ec='gray')\n",
    "axs[0].hlines(xmin=-.5, xmax=2.5, y=1/2, ls='--', ec='gray')\n",
    "axs[1].hlines(xmin=-.5, xmax=2.5, y=1/2, ls='--', ec='gray')\n",
    "axs[0].set_xticklabels(labels=['Vgg ImageNet','Vgg Animal','Vgg Artifact'], font=font)\n",
    "axs[0].set_yticklabels(labels=[0, .25, .5, .75, 1], font=font)\n",
    "axs[1].set_xticklabels(labels=['Vgg ImageNet','Vgg Animal','Vgg Artifact'], font=font)\n",
    "axs[0].set_ylabel('Accuracy', font=font)\n",
    "\n",
    "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
    "for j, ax in enumerate(axs):\n",
    "    ax.set_title(titles[j], font=font)\n",
    "    ax.tick_params(axis='both', labelsize=15)\n",
    "    for i, container in enumerate(ax.containers):\n",
    "        ax.bar_label(container, padding=-15, color='black', fmt='%.3f', rotation=0, label_type = 'edge', fontsize=fontsize, weight='bold')\n",
    "        \n",
    "plt.xticks(font=font)\n",
    "plt.yticks(font=font)        \n",
    "plt.tight_layout()\n",
    "if do_savefig: fig.savefig(os.path.join(figpath, 'mean_accuracy.pdf'), **opts_savefig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### limiting our classification to Strictly animal / artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'{data_cache}/{datetag}_{HOST}_results_strictly_basic.json'\n",
    "print(f'{filename=}')\n",
    "paths['animal']['test'] = '../data/strictly_animal/test'\n",
    "paths['artifact']['test'] = '../data/strictly_artifact/test'\n",
    "if os.path.isfile(filename):\n",
    "    df = pd.read_json(filename)\n",
    "else:\n",
    "    i_trial = 0\n",
    "    df = pd.DataFrame([], columns=['model', 'model_task', 'task', 'goal', 'likelihood', 'fps', 'time', 'i_image', 'filename', 'top_1', 'device_type']) \n",
    "        # image preprocessing\n",
    "    (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(image_size=args.image_size, batch_size=1)\n",
    "    for task in args.tasks:\n",
    "        pprint(task)\n",
    "        for i_image, (data, label) in enumerate(dataloaders[task]['test']):\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            for model_name in models_vgg.keys():\n",
    "                model = models_vgg[model_name].to(device)\n",
    "                with torch.no_grad():\n",
    "                    goal = 'target' if 'target' in image_datasets[task]['test'].imgs[i_image][0] else 'distractor'\n",
    "                    tic = time.time()\n",
    "                    out = model(data).squeeze(0).cpu()\n",
    "                    model_task = 'animal' if 'animal' in model_name else 'artifact'\n",
    "                    if model_name == 'vgg_16':\n",
    "                        model_task = 'Imagenet_challenge'\n",
    "                        percentage = torch.nn.functional.softmax(out, dim=0)\n",
    "                        likelihood = np.asarray(percentage[match[task]]).sum()\n",
    "                    else:\n",
    "                        likelihood = torch.sigmoid(out).numpy()[0]\n",
    "                    elapsed_time = time.time() - tic\n",
    "                    top_1 = 'target' if likelihood>0.50 else 'distractor'\n",
    "                    elapsed_time = time.time() - tic\n",
    "                df.loc[i_trial] = {'model':model_name,'model_task':model_task, 'task':task, 'top_1':top_1, 'goal':goal, 'likelihood':likelihood, 'time':elapsed_time, 'fps': 1/elapsed_time,\n",
    "                                        'i_image':i_image, 'filename':image_datasets[task]['test'].imgs[i_image][0], 'device_type':device.type}\n",
    "                print(f'The {model_name} model categorize {model_task} with {likelihood:.3f} % likelihood ({top_1}) in {elapsed_time:.3f} seconds, groundtruth : {task}, {goal}')\n",
    "                i_trial += 1\n",
    "    df.to_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_strictly = pd.read_json(filename)\n",
    "df_strictly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc_strictly = pd.DataFrame({model_name: {task: accuracy_score(df_strictly[(df_strictly['model']==model_name) & (df_strictly['task']==task)][\"top_1\"], \n",
    "                                                               df_strictly[(df_strictly['model']==model_name) & (df_strictly['task']==task)][\"goal\"])\n",
    "                                    for task in args.tasks} \n",
    "                       for model_name in df_strictly['model'].unique()})\n",
    "\n",
    "ax = df_acc_strictly.T.plot.bar(rot=20, figsize=(fig_width, fig_width//4), fontsize=20)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.hlines(xmin=-.5, xmax=len(models_vgg)-.5, y=1/2, ls='--', ec='k', label='chance level')\n",
    "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
    "for container in ax.containers: ax.bar_label(container, padding=-50, color='black', fontsize=14, fmt='%.3f')\n",
    "plt.legend(bbox_to_anchor=(1.2, .35), loc='lower right', fontsize=18)\n",
    "ax.set_title(f'Animal vs Artifact images f1 score for re-trained models', size=22)\n",
    "ax.set_ylabel('Accuracy', size=20)\n",
    "ax.set_xlabel('Model', size=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc = pd.DataFrame({model_name: {task: accuracy_score(df[(df['model']==model_name) & (df['task']==task)][\"top_1\"], \n",
    "                                                             df[(df['model']==model_name) & (df['task']==task)][\"goal\"])\n",
    "                                    for task in args.tasks} \n",
    "                       for model_name in df['model'].unique()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_dict = df_acc.to_dict()\n",
    "acc_dict_s = df_acc_strictly.to_dict()\n",
    "fig_width = 15\n",
    "model_tested = [ 'Random', 'Animal', 'Artifact']\n",
    "fig, axs = plt.subplots(1, 5, sharex=False, sharey=True, figsize=(fig_width, fig_width/5), gridspec_kw={'width_ratios': [2, 2, 2, 2, 1]})\n",
    "cmap = plt.cm.get_cmap('viridis')\n",
    "color_dict = pd.Series({k:cmap((i+.5)/len(model_tested)) for i, k in enumerate(model_tested)})\n",
    "\n",
    "titles = [\"(A) Animal set\",  \"(B) Strictly Animal set\", \"(C) Artifact set\" , \"(D) Strictly Artifact set\", \"(E) Random set\" ]\n",
    "\n",
    "\n",
    "F1_Random = .612 # HACK \n",
    "data_rand = { 'x_abs': ['Vgg Random'],\n",
    "        'y_val' : [ F1_Random]} \n",
    "data_ani = { 'x_abs': ['Animal', 'Artifact'],\n",
    "             'y_val' : [acc_dict['vgg16_tlc_animal']['animal'], acc_dict['vgg16_tlc_artifact']['animal']]}\n",
    "data_art = { 'x_abs': ['Vgg Animal', 'Vgg Artifact'],\n",
    "             'y_val' : [acc_dict['vgg16_tlc_animal']['artifact'], acc_dict['vgg16_tlc_artifact']['artifact']]}\n",
    "data_ani_s = { 'x_abs': ['Vgg Animal', 'Vgg Artifact'],\n",
    "             'y_val' : [acc_dict_s['vgg16_tlc_animal']['animal'], acc_dict_s['vgg16_tlc_artifact']['animal']]}\n",
    "data_art_s = { 'x_abs': ['Vgg Animal', 'Vgg Artifact'],\n",
    "             'y_val' : [acc_dict_s['vgg16_tlc_animal']['artifact'], acc_dict_s['vgg16_tlc_artifact']['artifact']]}\n",
    "\n",
    "# HACK à simplifier!\n",
    "df_rand = pd.DataFrame(data_rand)\n",
    "df_ani = pd.DataFrame(data_ani)\n",
    "df_art = pd.DataFrame(data_art)\n",
    "df_ani_s = pd.DataFrame(data_ani_s)\n",
    "df_art_s = pd.DataFrame(data_art_s)\n",
    "\n",
    "\n",
    "\n",
    "axs[4].bar(df_rand['x_abs'], df_rand['y_val'],  align='center', color= color_dict[0])\n",
    "axs[0].bar(df_ani['x_abs'], df_ani['y_val'], color= (color_dict[1], color_dict[2]))\n",
    "axs[2].bar(df_art['x_abs'], df_art['y_val'], color= (color_dict[1], color_dict[2]))\n",
    "axs[1].bar(df_ani_s['x_abs'], df_ani_s['y_val'], color= (color_dict[1], color_dict[2]))\n",
    "axs[3].bar(df_art_s['x_abs'], df_art_s['y_val'], color= (color_dict[1], color_dict[2]))\n",
    "\n",
    "\n",
    "axs[0].set_yticklabels(labels=[0, .25, .5, .75, 1], font=font)\n",
    "\n",
    "axs[0].set_ylabel('Accuracy', fontsize = fontsize)\n",
    "\n",
    "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
    "for j, ax in enumerate(axs):\n",
    "    axs[j].hlines(xmin=-.5, xmax=1.5, y=1/2, ls='--', ec='gray') if j != 4 else axs[j].hlines(xmin=-.5, xmax=.5, y=1/2, ls='--', ec='gray')  \n",
    "    axs[j].set_xticklabels(labels=['Animal','Artifact'], font=font) if j != 4 else axs[j].set_xticklabels(labels=['VGG Random'], font=font) \n",
    "    ax.set_title(titles[j], fontsize = 13)\n",
    "    ax.tick_params(axis='both', labelsize=13)\n",
    "    for i, container in enumerate(ax.containers):\n",
    "        ax.bar_label(container, padding=-15, color='black', fontsize=fontsize, fmt='%.3f', rotation=0, label_type = 'edge', fontweight='bold')\n",
    "\n",
    "    \n",
    "plt.xticks(font=font)\n",
    "plt.yticks(font=font)        \n",
    "plt.tight_layout()\n",
    "if do_savefig: fig.savefig(os.path.join(figpath, 'mean_accuracy.pdf'), **opts_savefig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "We tested the networks Vgg TLC, Vgg Scale and Vgg Gray trained to detect an animal on the dataset where the targets are 'animal' (respectively train to detect an artifact on dataset where targets are artifacts). Then we tested the networks trained to detect an 'animal' on the dataset where the targets are 'artifact' and vice versa. Here, by exposing the predictions for the 'animal' and 'artifact' labels, we highlight a bias in the composition of the dataset. Although the outputs are independent, 'animal' images confidently correspond to 'non-artifact' images (and vice versa), thus facilitating the overall detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exp2\"></a>\n",
    "## Experiment 2: Image processing and recognition for differents flip transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flips = {}\n",
    "flips['Horizontal'] = transforms.RandomHorizontalFlip(p=1) # HACK :p=1 is always flipping\n",
    "flips['Vertical'] = transforms.RandomVerticalFlip(p=1) # HACK :p=1 is always flipping\n",
    "\n",
    "filename = f'{data_cache}/{datetag}_{HOST}_results_flip.json'\n",
    "print(f'{filename=}')\n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    df = pd.read_json(filename)\n",
    "else:\n",
    "    i_trial = 0\n",
    "    df_flip = pd.DataFrame([], columns=['model', 'model_task', 'task', 'goal', 'likelihood', 'fps', 'time', 'i_image', 'filename', 'top_1', 'flip'])   \n",
    "    # image preprocessing\n",
    "    (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(paths=paths, image_size=args.image_size, batch_size=1)\n",
    "    for flip in flips:\n",
    "        pprint(flip)\n",
    "        for task in args.tasks:\n",
    "            for i_image, (data, label) in enumerate(dataloaders[task]['test']):\n",
    "                data = flips[flip](data)\n",
    "                data, label, = data.to(device), label.to(device)\n",
    "                for model_name in models_vgg.keys():\n",
    "                    model = models_vgg[model_name].to(device)\n",
    "                    with torch.no_grad():\n",
    "                        goal = 'target' if 'target' in image_datasets[task]['test'].imgs[i_image][0] else 'distractor'\n",
    "                        tic = time.time()\n",
    "                        out = model(data).squeeze(0).cpu()\n",
    "                        model_task = 'animal' if 'animal' in model_name else 'artifact'\n",
    "                        if model_name == 'vgg_16':\n",
    "                            model_task = 'Imagenet_challenge'\n",
    "                            percentage = torch.nn.functional.softmax(out, dim=0)\n",
    "                            likelihood = np.asarray(percentage[match[task]]).sum()\n",
    "                        else:\n",
    "                            likelihood = torch.sigmoid(out).numpy()[0]\n",
    "                        elapsed_time = time.time() - tic\n",
    "                        top_1 = 'target' if likelihood>0.50 else 'distractor'\n",
    "                    df_flip.loc[i_trial] = {'model':model_name,'model_task':model_task, 'task':task, 'top_1':top_1, 'goal':goal,        \n",
    "                                            'likelihood':likelihood, 'time':elapsed_time, 'fps': 1/elapsed_time,\n",
    "                                            'i_image':i_image, 'filename':image_datasets[task]['test'].imgs[i_image][0], 'flip':flip}\n",
    "                    print(f'The {model_name} model categorize {model_task} with {likelihood*100:.3f} % likelihood ({top_1}) in {elapsed_time:.3f} seconds, groundtruth : {task}, {goal}')\n",
    "                    i_trial += 1\n",
    "\n",
    "    df_flip.to_json(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = f'{data_cache}/{datetag}_{HOST}_results_flip.json'\n",
    "df_flip = pd.read_json(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computation Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(fig_width, fig_width/phi))\n",
    "for color, model_name in zip(colors, models_vgg.keys()):\n",
    "    axs = sns.violinplot(x=\"flip\", y=\"time\", data=df_flip, inner=\"quartile\", hue='model')\n",
    "    axs.set_title('Processing time (s) for each network at different image size. Processed on : ' + HOST, size=20)\n",
    "    axs.set_ylabel('Computation time (s)', size=18)\n",
    "    axs.set_xlabel('Image size', size=18)\n",
    "    axs.set_yscale('log')\n",
    "    axs.grid(which='both', axis='y')\n",
    "    for side in ['top', 'right'] :axs.spines[side].set_visible(False)\n",
    "h, l = axs.get_legend_handles_labels()\n",
    "axs.legend(h[:5], l[:5], loc='upper center', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy comparison for all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HACK : redondant\n",
    "# flips = {}\n",
    "# flips['Horizontal'] = transforms.RandomHorizontalFlip(p=1)\n",
    "# flips['Vertical'] = transforms.RandomVerticalFlip(p=1)\n",
    "\n",
    "for task in args.tasks:\n",
    "    df_acc = pd.DataFrame({model_name: {flip: accuracy_score(df_flip[(df_flip['model']==model_name) & (df_flip['task']==task) & (df_flip['flip']==flip) ][\"top_1\"], \n",
    "                                                                   df_flip[(df_flip['model']==model_name) & (df_flip['task']==task) & (df_flip['flip']==flip)][\"goal\"])\n",
    "                                        for flip in flips} \n",
    "                           for model_name in models_vgg.keys()})\n",
    "\n",
    "    ax = df_acc.T.plot.bar(rot=30, figsize=(fig_width, fig_width//4), fontsize=fontsize)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.hlines(xmin=-.5, xmax=len(models_vgg.keys())-.5, y=1/2, ls='--', ec='gray', label='chance level')\n",
    "    # https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
    "    for container in ax.containers: ax.bar_label(container, padding=-50, color='black', fontsize=fontsize, weight='bold', fmt='%.3f', rotation=90)\n",
    "    plt.legend(bbox_to_anchor=(1.2, .35), loc='lower right', fontsize=14)\n",
    "    ax.set_title(f'Animal vs Artifact images f1 score for re-trained models, task: {task}', size=22)\n",
    "    ax.set_ylabel('F1 score', size=20)\n",
    "    ax.set_xlabel('Model', size=20);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "We tested the Vgg TLC network on the ImageNet dataset with either a vertical or horizontal reflection applied on the input. The networks keeps a good mean accuracy on the flipped dataset (about 95%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exp4\"></a>\n",
    "## Experiment 4: Image processing and recognition on grayscale images :\n",
    "\n",
    "Again, same likelihood indicators but now with a grayscale transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = f'{data_cache}/{datetag}_{HOST}_results_grayscale.json'\n",
    "print(f'{filename=}')\n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    df_gray = pd.read_json(filename)\n",
    "else:\n",
    "    i_trial = 0\n",
    "    df_gray = pd.DataFrame([], columns=['model', 'model_task', 'task', 'goal', 'likelihood', 'fps', 'time', 'i_image', 'filename', 'top_1']) \n",
    "        # image preprocessing\n",
    "    (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(paths=paths, image_size=args.image_size, batch_size=1, p=1)\n",
    "    for task in args.tasks:\n",
    "        pprint(task)\n",
    "        for i_image, (data, label) in enumerate(dataloaders[task]['test']):\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            for model_name in models_vgg.keys():\n",
    "                model = models_vgg[model_name].to(device)\n",
    "                with torch.no_grad():\n",
    "                    goal = 'target' if 'target' in image_datasets[task]['test'].imgs[i_image][0] else 'distractor'\n",
    "                    model_task = 'animal' if 'animal' in model_name else 'artifact'\n",
    "                    tic = time.time()\n",
    "                    out = model(data).squeeze(0).cpu()\n",
    "                    if model_name == 'vgg_16':\n",
    "                        model_task = 'Imagenet_challenge'\n",
    "                        percentage = torch.nn.functional.softmax(out, dim=0)\n",
    "                        likelihood = np.asarray(percentage[match[task]]).sum()\n",
    "                    else:\n",
    "                        likelihood = torch.sigmoid(out).numpy()[0]\n",
    "                    top_1 = 'target' if likelihood>0.50 else 'distractor'\n",
    "                    elapsed_time = time.time() - tic\n",
    "                df_gray.loc[i_trial] = {'model':model_name,'model_task':model_task, 'task':task, 'top_1':top_1, 'goal':goal, 'likelihood':likelihood, 'time':elapsed_time, 'fps': 1/elapsed_time,\n",
    "                                        'i_image':i_image, 'filename':image_datasets[task]['test'].imgs[i_image][0]}\n",
    "                print(f'The {model_name} model categorize {model_task} with {likelihood*100:.3f} % likelihood ({top_1}) in {elapsed_time:.3f} seconds, groundtruth : {task}, {goal}')\n",
    "                i_trial += 1\n",
    "    df_gray.to_json(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = f'{data_cache}/{datetag}_{HOST}_results_grayscale.json'\n",
    "df_gray = pd.read_json(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(models_vgg), 1, figsize=(fig_width, fig_width*phi/2))\n",
    "for color, df_, label, legend in zip(['gray', 'red'], [df_gray, df], ['black', 'color'], ['Grayscale', 'Regular']):\n",
    "    for ax, model_name in zip(axs, models_vgg):\n",
    "        ax.set_ylabel('Frequency', fontsize=20) \n",
    "        df_[df_['model']==model_name]['time'].plot.hist(bins=150, lw=1, label=str(legend+ ' ' + model_name), ax=ax, color=color, density=True)\n",
    "        ax.legend(loc='upper right', fontsize=20)\n",
    "        ax.set_xlim(df_gray['time'].quantile(.01), df_gray['time'].quantile(.99))\n",
    "        ax.legend(bbox_to_anchor=(1.18, .35), loc='lower right')\n",
    "axs[-1].set_xlabel('Processing time (s)', size=18)\n",
    "axs[0].set_title('Processed on : ' + HOST, size=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in args.tasks:\n",
    "    df_acc = pd.DataFrame({model_name: {label: accuracy_score(df_[(df_['model']==model_name) & (df_['task']==task)][\"top_1\"], \n",
    "                                                                   df_[(df_['model']==model_name) & (df_['task']==task)][\"goal\"])\n",
    "                                        for label, df_ in zip(['original', 'gray'], [df, df_gray])} \n",
    "                           for model_name in models_vgg.keys()})\n",
    "\n",
    "    ax = df_acc.T.plot.bar(rot=30, figsize=(fig_width, fig_width//4), fontsize=18)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.hlines(xmin=-.5, xmax=len(models_vgg)-.5, y=1/2, ls='--', ec='gray', label='chance level')\n",
    "    # https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
    "    for container in ax.containers: ax.bar_label(container, padding=-50, color='black', fontsize=fontsize, weight='bold', fmt='%.3f', rotation=90)\n",
    "    plt.legend(bbox_to_anchor=(1.16, .35), loc='lower right', fontsize=16)\n",
    "    ax.grid(which='both', axis='y')\n",
    "    for side in ['top', 'right'] :ax.spines[side].set_visible(False)\n",
    "    ax.set_title(f'Experiment - color vs gray images, task: {task}', size=20)\n",
    "    ax.set_ylabel('F1 Score', size=18)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "We tested the networks on the ImageNet dataset with a grayscale filter applied on the input. The networks keeps a good mean accuracy on the grayscale dataset (about 94%). Note that the Vgg Gray networks achieves the task with a gray scale applied with a sightly better mean accuracy ([experiment 4](#exp4)) but not on the regular 'test' dataset ([experiment 1](#exp1))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"exp5\"></a>\n",
    "## Experiment 5: Image processing and recognition on rotated images :\n",
    "\n",
    "In addition we tested all networks on our IMAGENET dataset while rotating the image around the center by -180° to +180°."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "angles = np.arange(-180, 181, 15)\n",
    "filename = f'{data_cache}/{datetag}_{HOST}_results_rotate.json'\n",
    "print(f'{filename=}')\n",
    "   \n",
    "if os.path.isfile(filename):\n",
    "    df_angle = pd.read_json(filename)\n",
    "else:\n",
    "    i_trial = 0\n",
    "    df_angle = pd.DataFrame([], columns=['mean_prediction', 'model', 'var'])\n",
    "    (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(paths=paths, image_size=args.image_size, batch_size=args.batch_size)\n",
    "    # image preprocessing\n",
    "    for task in args.tasks:\n",
    "        print(task)\n",
    "        for angle in angles:\n",
    "            print(f'Rotation by {angle=}°')\n",
    "            for model_name in models_vgg.keys():\n",
    "                model = models_vgg[model_name].to(device)\n",
    "                acc_= 0\n",
    "                for i_image, (data, label) in enumerate(dataloaders[task]['test']):\n",
    "                    data, label, = data.to(device), label.to(device)\n",
    "                    data = transforms.functional.rotate(data, angle=float(angle), expand=True)\n",
    "                    data = transforms.functional.resize(data, size=(args.image_size, args.image_size), antialias=True)\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(data)\n",
    "                        if model_name == 'vgg_16':\n",
    "                            percentage = torch.nn.functional.softmax(outputs, dim=1)\n",
    "                            preds = torch.sum(percentage[:, match[task]], dim=1)\n",
    "                            acc_ += torch.sum(torch.round(preds) == label.data)\n",
    "                        else:\n",
    "                            preds = torch.round(torch.sigmoid(outputs[:,0].data))\n",
    "                            acc_ += torch.sum(preds == label.data)\n",
    "                avg_acc = acc_ / dataset_sizes[task]['test']\n",
    "                df_angle.loc[i_trial] = {'model':model_name, 'mean_prediction':float(avg_acc), 'var':angle}  \n",
    "                print(model_name, float(avg_acc), angle)\n",
    "                #print(f'The {model_name} model categorize {model_task} with {likelihood*100:.3f} % likelihood ({top_1}) in {elapsed_time:.3f} seconds, groundtruth : {task}, {goal}')\n",
    "                i_trial += 1\n",
    "                # if device=='cuda': torch.cuda.empty_cache()\n",
    "    df_angle.to_json(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = f'{data_cache}/{datetag}_{HOST}_results_rotate.json'\n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    df_angle = pd.read_json(filename)\n",
    "    df_angle_animal = df_angle[:len(df_angle)//2]\n",
    "    df_angle_artifact = df_angle[len(df_angle)//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(filename):\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    cmap = plt.cm.get_cmap('viridis')\n",
    "    color_dict = pd.Series({k:cmap(i/len(df_angle_artifact['model'].unique())) for i,k in enumerate(df_angle_artifact['model'].unique())})\n",
    "\n",
    "    for model_name in df_angle['model'].unique():     \n",
    "        df_angle_artifact[(df_angle_artifact['model']==model_name)].plot.scatter(x=\"var\", y=\"mean_prediction\", c=[color_dict[model_name],],\n",
    "                                                            marker='o', lw=0, label=model_name, ax=ax)\n",
    "    ax.tick_params(axis='x', labelsize=14)\n",
    "    ax.tick_params(axis='y', labelsize=14)\n",
    "    ax.set_ylim(0.5, 1)\n",
    "    ax.set_xlim(-185, 185)\n",
    "    ax.set_xticks([-180, -90, 0, 90, 180])\n",
    "    ax.set_ylabel('Mean Accuracy', font=font)\n",
    "    ax.set_xlabel('Rotation (°)', font=font)\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1.28, .5), loc='center', fontsize=10, prop=font)\n",
    "    plt.xticks(font=font)\n",
    "    plt.yticks(font=font);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(filename):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    cmap = plt.cm.get_cmap('viridis')\n",
    "    color_dict = pd.Series({k:cmap(i/len(df_angle_animal['model'].unique())) for i,k in enumerate(df_angle_animal['model'].unique())})\n",
    "\n",
    "    for model_name in df_angle['model'].unique():     \n",
    "        df_angle_animal[(df_angle_animal['model']==model_name)].plot.scatter(x=\"var\", y=\"mean_prediction\", c=[color_dict[model_name],],\n",
    "                                                            marker='o', lw=0, label=model_name, ax=ax)\n",
    "    ax.tick_params(axis='x', labelsize=14)\n",
    "    ax.tick_params(axis='y', labelsize=14)\n",
    "    ax.set_ylim(0.5, 1)\n",
    "    ax.set_xlim(-185, 185)\n",
    "    ax.set_xticks([-180, -90, 0, 90, 180])\n",
    "    ax.set_ylabel('Mean Accuracy', font=font)\n",
    "    ax.set_xlabel('Rotation (°)', font=font)\n",
    "    plt.legend(bbox_to_anchor=(1.28, .5), loc='center', fontsize=10, prop=font)\n",
    "    plt.xticks(font=font)\n",
    "    plt.yticks(font=font);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "color_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(filename):\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n",
    "    grphe1 = ['vgg_16', 'vgg16_tlc_animal']\n",
    "    grphe2 = ['vgg16_tlc_animal', 'vgg16_augment_animal']\n",
    "    all_ = ['temp', 'vgg_16', 'temp', 'vgg16_tlc_animal', 'temp', 'vgg16_augment_animal']\n",
    "    all_ = ['temp', 'vgg_16', 'temp', 'vgg16_tlc_animal', 'temp', 'vgg16_augment_animal']\n",
    "\n",
    "    cmap = plt.cm.get_cmap('viridis')\n",
    "    color_dict = pd.Series({k:cmap(i/len(all_)) for i,k in enumerate(all_)})\n",
    "    label_dict = {'vgg_16':'VGG LUT', 'vgg16_tlc_animal':'VGG TLC', 'vgg16_augment_animal':'VGG Augment'}\n",
    "\n",
    "    graphes = [grphe1, grphe2]\n",
    "    for ax, graphe in zip(axs, graphes):\n",
    "        for model_name in graphe:\n",
    "            df_angle_model = (df_angle_animal['model']==model_name)\n",
    "            # df_angle_animal[df_angle_model].plot.scatter(x=\"var\", y=\"mean_prediction\", \n",
    "            #                                              c=[color_dict[model_name]],\n",
    "            #                                              marker='o', label=label_dict[model_name], lw=2., ax=ax)\n",
    "            df_angle_animal[df_angle_model].plot(x=\"var\", y=\"mean_prediction\", \n",
    "                                                         color=[color_dict[model_name]],\n",
    "                                                         marker='o', label=label_dict[model_name], lw=2., ax=ax)\n",
    "                                                                \n",
    "        ax.tick_params(axis='x', labelsize=14)\n",
    "        ax.tick_params(axis='y', labelsize=14)\n",
    "        ax.set_ylim(0.5, 1)\n",
    "        ax.set_xlim(-185, 185)\n",
    "        ax.set_xticks([-180, -90, 0, 90, 180])\n",
    "        ax.set_ylabel('Accuracy', font=font)\n",
    "        ax.set_xlabel('Rotation (°)', font=font)\n",
    "        ax.yaxis.set_major_locator(matplotlib.ticker.FixedLocator([.5, .75, 1]))\n",
    "        for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "            label.set_font(font)\n",
    "        # ax.get_legend().remove()\n",
    "        # ax_dict['top'].legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left',\n",
    "        #               ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "        ax.legend(loc='lower left', fontsize=18, edgecolor='none')\n",
    "\n",
    "    if do_savefig: fig.savefig(os.path.join(figpath, 'full_rot.pdf'), **opts_savefig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "The image size or a gray filter during training does not seem to impact the robustness to rotation as all three networks follow the same accuracy level for the categorization. Furthermore,\n",
    "these three networks maintain a good accuracy during the process (VGG TLC 90.272 +/- 2.05, VGG GRAY 89.464 +/-2.72, VGG SCALE 89.222: +/- 2.722) (see Figure 5). As the networks seems robust to relection [experiment 2](#exp2), to the resolution [experiment 3](#exp3) (in a short range centered in the training resolution) and to grayscale [experiment 4](#exp4), the robustness of the categorization to different image transformations by the models are similar to that reported in psychophysics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imagenet rotation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CleanRotation_function(image, angle=0, radius=0.5, mask_exponent=3, image_size=args.image_size):\n",
    "\n",
    "    image_rot = transforms.functional.rotate(image, angle=angle, expand=False)\n",
    "\n",
    "    X, Y = np.meshgrid(np.linspace(-radius, radius, image_size, endpoint=True), \n",
    "           np.linspace(-radius, radius, image_size, endpoint=True))\n",
    "    R = np.sqrt(X**2 + Y**2)\n",
    "    mask =  ((R < 0.5) * ((np.cos(np.pi*R/radius)+1)/2)**(1./mask_exponent) )> 0\n",
    "    \n",
    "    return image_rot[:,::]*torch.from_numpy(mask)\n",
    "\n",
    "def imshow(img_list, title=None): #allow to display the input image\n",
    "    images = torchvision.utils.make_grid(img_list)\n",
    "    fig = plt.figure(figsize=(5*len(img_list),5))\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = images.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    plt.xticks([]) ; plt.yticks([])\n",
    "    if title is not None: plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "filename = f'{data_cache}/{datetag}_{HOST}_results_rotate_imagenet_new.json'\n",
    "print(f'{filename=}')\n",
    "\n",
    "tasks = args.tasks\n",
    "tasks.append('random')\n",
    "paths['random'] = {}\n",
    "for folder in args.folders:\n",
    "    paths['random'][folder] = os.path.join(args.datasets_root, task, folder) # data path\n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    df_angle = pd.read_json(filename)\n",
    "else:\n",
    "    i_trial = 0\n",
    "    df_angle = pd.DataFrame([], columns=['mean_prediction', 'model', 'var', 'effect'])\n",
    "    (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(paths=paths, image_size=args.image_size, batch_size=25)\n",
    "    # image preprocessing\n",
    "    for task in tasks:\n",
    "        print(task)\n",
    "        for effect in ['basic', 'rot', 'rot_custom']:\n",
    "            model_name = 'vgg_16'\n",
    "            model = models_vgg[model_name].to(device)\n",
    "            acc_= 0\n",
    "            for i_image, (data, label) in enumerate(dataloaders[task]['test']):\n",
    "                if effect == 'rot':\n",
    "                    data = transforms.functional.rotate(data, angle=float(random.choice(angles)), expand=True)\n",
    "                    data = transforms.functional.resize(data, size=(args.image_size, args.image_size), antialias=True)\n",
    "                if effect == 'rot_custom': # HACK not necessary anymore\n",
    "                    data = CleanRotation_function(data, angle=float(random.choice(angles)), image_size=args.image_size)\n",
    "                data, label, = data.to(device), label.to(device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(data)\n",
    "                    percentage = torch.nn.functional.softmax(outputs, dim=1)\n",
    "                    preds = torch.sum(percentage[:, match[task]], dim=1)\n",
    "                    acc_ += torch.sum(torch.round(preds) == label.data)\n",
    "            avg_acc = acc_ / dataset_sizes[task]['test']\n",
    "            df_angle.loc[i_trial] = {'model':model_name, 'mean_prediction':float(avg_acc), 'var':task, 'effect':effect}  \n",
    "            print(model_name, float(avg_acc), task, effect)\n",
    "            #print(f'The {model_name} model categorize {model_task} with {likelihood*100:.3f} % likelihood ({top_1}) in {elapsed_time:.3f} seconds, groundtruth : {task}, {goal}')\n",
    "            i_trial += 1\n",
    "            # if device=='cuda': torch.cuda.empty_cache()\n",
    "        df_angle.to_json(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'{data_cache}/{datetag}_{HOST}_results_rotate_imagenet_new.json'\n",
    "if os.path.isfile(filename):\n",
    "    df_rotate_imagenet = pd.read_json(filename)\n",
    "    df_rotate_imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(filename):\n",
    "\n",
    "    acc_dict = df_rotate_imagenet.to_dict()\n",
    "    data_rand = { 'x_abs': ['Random', 'Random + Rotation'],\n",
    "            'y_val' : [acc_dict['mean_prediction'][6], acc_dict['mean_prediction'][8]]}\n",
    "    data_ani = { 'x_abs': ['Animal', 'Animal + Rotation'],\n",
    "            'y_val' : [acc_dict['mean_prediction'][0], acc_dict['mean_prediction'][2]]}\n",
    "    data_art = { 'x_abs': ['Artifact', 'Artifact + Rotation'],\n",
    "            'y_val' : [acc_dict['mean_prediction'][3], acc_dict['mean_prediction'][5]]}\n",
    "\n",
    "    df_rand = pd.DataFrame(data_rand)\n",
    "    df_ani = pd.DataFrame(data_ani)\n",
    "    df_art = pd.DataFrame(data_art)\n",
    "\n",
    "    x= ['Vgg Imagenet', 'Vgg Animal', 'Vgg Artifact']\n",
    "    fig, axs = plt.subplots(1, 3, sharex=False, sharey=True, figsize=(fig_width, fig_width//4))\n",
    "    cmap = plt.cm.get_cmap('viridis')\n",
    "    color_dict = pd.Series({k:cmap(i/len(x)) for i,k in enumerate(x)})\n",
    "    titles = [\"(A) Animal dataset\", \"(B) Artifact dataset\" ,\"(C) Random dataset\" ]\n",
    "\n",
    "    axs[2].bar(df_rand['x_abs'], df_rand['y_val'],  align='center', color=color_dict[0])\n",
    "    axs[0].bar(df_ani['x_abs'], df_ani['y_val'], color=color_dict[1])\n",
    "    axs[1].bar(df_art['x_abs'], df_art['y_val'], color=color_dict[2])\n",
    "    axs[0].hlines(xmin=-.5, xmax=1.5, y=1/2, ls='--', ec='gray')\n",
    "    axs[1].hlines(xmin=-.5, xmax=1.5, y=1/2, ls='--', ec='gray')\n",
    "    axs[2].hlines(xmin=-.5, xmax=1.5, y=1/2, ls='--', ec='gray')\n",
    "    axs[1].set_xticklabels(labels=['Artifact', 'Artifact + Rotation'], font=font)\n",
    "    axs[1].set_yticklabels(labels=[0, .25, .5, .75, 1], font=font)\n",
    "    axs[0].set_xticklabels(labels=['Animal', 'Animal + Rotation'], font=font)\n",
    "    axs[0].set_ylabel('Accuracy', font=font)\n",
    "    axs[0].set_yticklabels(labels=[0, .25, .5, .75, 1], font=font)\n",
    "\n",
    "    # https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
    "    for j, ax in enumerate(axs):\n",
    "            ax.set_title(titles[j], font=font)\n",
    "            ax.tick_params(axis='both', labelsize=15)    \n",
    "            for i, container in enumerate(ax.containers):\n",
    "                    ax.bar_label(container, padding=-15, color='black', fmt='%.3f', rotation=0, label_type = 'edge', fontsize=fontsize, weight='bold')\n",
    "\n",
    "    plt.xticks(font=font)\n",
    "    plt.yticks(font=font)\n",
    "    plt.tight_layout()\n",
    "    if do_savefig: fig.savefig(os.path.join(figpath, 'robustness_imagenet.pdf'), **opts_savefig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zoom experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scales = np.arange(0.1, 1.1, 0.05)\n",
    "filename = f'{data_cache}/{datetag}_{HOST}_results_zoom.json'\n",
    "print(f'{filename=}')\n",
    "     \n",
    "if os.path.isfile(filename):\n",
    "    df_scale = pd.read_json(filename)\n",
    "else:\n",
    "    i_trial = 0\n",
    "    df_scale = pd.DataFrame([], columns=['mean_prediction', 'model', 'var'])\n",
    "    (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(paths=paths, image_size=args.image_size, batch_size=batch_size)\n",
    "    # image preprocessing\n",
    "    task = 'animal'\n",
    "    print(task)\n",
    "    for scale in scales:\n",
    "        print(f'Zoom by {scale=}°')\n",
    "        for model_name in models_vgg.keys():\n",
    "            model = models_vgg[model_name].to(device)\n",
    "            acc_= 0\n",
    "            for i_image, (data, label) in enumerate(dataloaders[task]['test']):\n",
    "                data = torchvision.transforms.functional.affine(data, angle = 0, translate=(0,0), shear = 0 , scale=scale)\n",
    "                data, label, = data.to(device), label.to(device)\n",
    "                # data = torchvision.transforms.functional.affine(data, angle=0, translate=(0,0), shear=0, scale=scale)\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(data)\n",
    "                    if model_name == 'vgg_16':\n",
    "                        percentage = torch.nn.functional.softmax(outputs, dim=1)\n",
    "                        preds = torch.sum(percentage[:,match[task]], dim=1)\n",
    "                        acc_ += torch.sum(torch.round(preds) == label.data)\n",
    "                    else:\n",
    "                        preds = torch.round(torch.sigmoid(outputs[:,0].data))\n",
    "                        acc_ += torch.sum(preds == label.data)\n",
    "            avg_acc = acc_ / dataset_sizes[task]['test']\n",
    "            df_scale.loc[i_trial] = {'model':model_name, 'mean_prediction':float(avg_acc), 'var':scale}  \n",
    "            print(model_name, float(avg_acc), scale)\n",
    "            i_trial += 1\n",
    "            # if device=='cuda': torch.cuda.empty_cache()\n",
    "    df_scale.to_json(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'{data_cache}/{datetag}_{HOST}_results_scale.json'\n",
    "filename = f'{data_cache}/{datetag}_{HOST}_results_zoom.json'\n",
    "\n",
    "df_scale = pd.read_json(filename)\n",
    "df_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "scales = np.arange(0.1, 1.1, 0.1)\n",
    "cmap = plt.cm.get_cmap('viridis')\n",
    "color_dict = pd.Series({k:cmap(i/len(df_scale['model'].unique())) for i,k in enumerate(df_scale['model'].unique())})\n",
    "full_label = {\"vgg_16\":'Vgg16 pytorch', \"vgg16_tlc_animal\":'Vgg16 animal on animal', \"vgg16_tlc_artifact\":'Vgg16 artifact on animal'}\n",
    "for model_name in df_scale['model'].unique():     \n",
    "    df_scale[(df_scale['model']==model_name)].plot(x=\"var\", y=\"mean_prediction\", color=[color_dict[model_name],],\n",
    "                                                         marker='o', lw=1, ax=ax, label=full_label[model_name])\n",
    "ax.legend(loc='center right', fontsize=12)\n",
    "ax.tick_params(axis='x', labelsize=14)\n",
    "ax.tick_params(axis='y', labelsize=14)\n",
    "ax.set_xlim(0.1, 1)\n",
    "ax.set_xticks([0.09, .5, 1])\n",
    "ax.set_xscale('log')\n",
    "ax.hlines(xmin=-180, xmax=180, y=1/2, ls='--', ec='gray', label='Chance level')\n",
    "ax.set_ylabel('Mean Accuracy', font=font)\n",
    "ax.set_xlabel('Scale factor', font=font)\n",
    "ax.get_legend().remove()\n",
    "plt.xticks(font=font)\n",
    "plt.yticks(font=font)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.28, .5), loc='center', fontsize=10, prop=font)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 6, a comparison between performances in biology and in models: Test on Serre (2007)'s dataset\n",
    "\n",
    " As a control, we tested the networks on the dataset of [Serre & al](http://vision.stanford.edu/teaching/cs131_fall1415/lectures/Serre2007.pdf). This dataset contains a total of $600$ targets (images containing an animal) and $600$ distractors (images not containing an animal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls -R ../data/Serre_2007/val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls ../data/animal/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'{data_cache}/{datetag}_{HOST}_results_serre.json'\n",
    "print(f'{filename=}')\n",
    "# serre = ['target', 'distractor']\n",
    "# paths['serre07'] = {}\n",
    "# paths['serre07']['test'] = '../data/Serre_2007/'\n",
    "# task = 'animal'\n",
    "(dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(paths=paths, image_size=args.image_size, batch_size=1)\n",
    "image_dataset_serre07 = datasets.ImageFolder('../data/Serre_2007/', transform=data_transforms['test'])\n",
    "dataloaders_serre07 = torch.utils.data.DataLoader(image_dataset_serre07, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "# Output's set up\n",
    "if os.path.isfile(filename):\n",
    "    df_serre = pd.read_json(filename)\n",
    "else:\n",
    "    df_serre = pd.DataFrame([], columns=['model', 'model_task', 'goal', 'likelihood', 'fps', 'time', 'i_image', 'filename', 'top_1']) \n",
    "    i_trial = 0\n",
    "    for i_image, (data, label) in enumerate(dataloaders_serre07):#dataloaders['serre07']['test']):\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        for model_name in models_vgg.keys():\n",
    "            model = models_vgg[model_name].to(device)\n",
    "            with torch.no_grad():\n",
    "                goal = 'target' if 'target' in image_dataset_serre07.imgs[i_image][0] else 'distractor'\n",
    "                model_task = 'animal' if 'animal' in model_name else 'artifact'\n",
    "                tic = time.time()\n",
    "                out = model(data).squeeze(0).cpu()\n",
    "                if model_name == 'vgg_16':\n",
    "                    model_task = 'Imagenet_challenge'\n",
    "                    percentage = torch.nn.functional.softmax(out, dim=0)\n",
    "                    likelihood = np.asarray(percentage[match['animal']]).sum()\n",
    "                else:\n",
    "                    likelihood = torch.sigmoid(out).numpy()[0]\n",
    "\n",
    "                top_1 = 'target' if likelihood>0.50 else 'distractor'\n",
    "                elapsed_time = time.time() - tic\n",
    "            df_serre.loc[i_trial] = {'model':model_name, 'model_task':model_task, 'likelihood':likelihood, 'time':elapsed_time, \n",
    "                                        'fps': 1/elapsed_time, 'goal': goal, 'i_image':i_image, 'top_1':top_1,\n",
    "                                        'filename':image_dataset_serre07.imgs[i_image][0]}\n",
    "            print(f'The {model_name} model categorize {model_task} with {likelihood*100:.3f} % likelihood ({top_1}) in {elapsed_time:.3f} seconds, groundtruth : animal, {goal}')\n",
    "            i_trial += 1\n",
    "    df_serre.to_json(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'{data_cache}/{datetag}_{HOST}_results_serre.json'\n",
    "df_serre = pd.read_json(filename)\n",
    "df_serre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the images with the highest likelihood of the networks :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_image_i = 4\n",
    "N_image_j = 4\n",
    "fig, axs = plt.subplots(N_image_i, N_image_j, figsize=(fig_width*1.3, fig_width))\n",
    "for idx, i_image in enumerate(df_serre.sort_values(by=['likelihood'], ascending=False).head(N_image_i*N_image_j).index):\n",
    "    ax = axs[idx%N_image_i][idx//N_image_i]\n",
    "    img_address = image_dataset_serre07.imgs[df_serre.loc[idx]['i_image']][0]\n",
    "    ax.imshow(imageio.imread(img_address))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    color = 'g' if df_serre.loc[idx]['top_1'] == df_serre.loc[idx]['goal'] else 'r'\n",
    "    ax.set_xlabel(df_serre.loc[idx]['top_1'] + ' | ' + df_serre.loc[idx]['model'], color=color)\n",
    "    likelihood = df_serre.loc[idx]['likelihood']\n",
    "    ax.set_ylabel(f'P ={likelihood * 100:2.3f}%', color=color)\n",
    "fig.set_facecolor(color='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the images with the lowest likelihood of the networks :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(N_image_i, N_image_j, figsize=(fig_width*1.3, fig_width))\n",
    "for i_image, idx in enumerate(df_serre.sort_values(by=['likelihood'], ascending=True).head(N_image_i*N_image_j).index):\n",
    "    ax = axs[i_image%N_image_i][i_image//N_image_i]\n",
    "    img_address = image_dataset_serre07.imgs[df_serre.loc[idx]['i_image']][0]\n",
    "    ax.imshow(imageio.imread(img_address))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    color = 'g' if df_serre.loc[idx]['top_1'] == df_serre.loc[idx]['goal'] else 'r'\n",
    "    ax.set_xlabel(df_serre.loc[idx]['top_1'] + ' | ' + df_serre.loc[idx]['model'], color=color)\n",
    "    likelihood = df_serre.loc[idx]['likelihood']\n",
    "    ax.set_ylabel(f'P ={likelihood:2.3f}%', color=color)\n",
    "fig.set_facecolor(color='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc = pd.DataFrame({'accuracy': [accuracy_score(df_serre[df_serre['model']==model_name][\"top_1\"], df_serre[df_serre['model']==model_name][\"goal\"]) for model_name in models_vgg.keys()]}, index=models_vgg)\n",
    "ax = df_acc.plot.bar(rot=30, figsize=(fig_width, fig_width//4), fontsize=fontsize)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.hlines(xmin=-.5, xmax=len(models_vgg)-.5, y=1/2, ls='--', ec='k', label='chance level')\n",
    "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
    "ax.bar_label(ax.containers[0], padding=-24, color='black', fontsize=fontsize, weight='bold', fmt='%.3f')\n",
    "plt.legend(bbox_to_anchor=(1.1, .5), loc='lower right')\n",
    "ax.grid(which='both', axis='y')\n",
    "for side in ['top', 'right'] :ax.spines[side].set_visible(False)\n",
    "ax.set_title('Average accuracy top_1 : for each models - experiment 1', size=20)\n",
    "ax.set_xlabel('Model', size=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "df_mean = pd.DataFrame([], columns=['heuristic', 'var', 'goal'])\n",
    "cmap = plt.cm.get_cmap('viridis')\n",
    "color_dict = pd.Series({k:cmap(i/len(df_serre['goal'].unique())) for i,k in enumerate(df_serre['goal'].unique())})\n",
    "model_animal = []\n",
    "model_artifact = []\n",
    "data_animal = df_serre.loc[df_serre['model'] == 'vgg16_tlc_animal']\n",
    "data_artifact = df_serre.loc[df_serre['model'] == 'vgg16_tlc_artifact']\n",
    "for i_image in df_serre['i_image'].unique():\n",
    "    heuristic = 1 - (1 - float(data_animal[(data_animal['i_image']==i_image)][\"likelihood\"])) * float(data_artifact[(data_artifact['i_image']==i_image)][\"likelihood\"])\n",
    "    df_mean.loc[i_image] = {'heuristic':heuristic, 'var':i_image, 'goal':list(data_animal.goal)[i_image]}      \n",
    "for goal in df_serre['goal'].unique():\n",
    "    df_mean[(df_mean['goal']==goal)].plot.scatter(x=\"var\", y=\"heuristic\",  c=[color_dict[goal],],\n",
    "                                                         alpha=0.5, marker='o', lw=0, ax=ax, label=goal)\n",
    "ax.tick_params(axis='x', labelsize=14)\n",
    "ax.tick_params(axis='y', labelsize=14)\n",
    "ax.set_ylabel('Heuristic', size= 20)\n",
    "ax.set_xlabel('i image', size= 20)\n",
    "plt.legend(bbox_to_anchor=(1.1, .5), loc='center', fontsize=14,)\n",
    "ax.set_title('Heuristic : likelihood = 1 - (1 - likelihood_animal)  * likelihood_artifact', size=20);\n",
    "plt.tight_layout();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with psychophysical data from Serre & al 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_serre_07 = pd.read_csv('https://raw.githubusercontent.com/laurentperrinet/PerrinetBednar15/master/database/AnimalnessIndex.csv', header=None)#, index_col=['A', 'B', 'C'])\n",
    "\n",
    "likes_ = []\n",
    "goals = []\n",
    "top1_human = []\n",
    "top1_model = []\n",
    "data_vgg_gen = df_serre.loc[df_serre['model'] == 'vgg16_tlc_animal']\n",
    "for i in range(len(df_serre_07)):\n",
    "    top_1_human = 'target' if df_serre_07.iloc[i, 1]>50 else 'distractor'\n",
    "    top_1_model = 'target' if df_serre_07.iloc[i, 2]>50 else 'distractor'\n",
    "    top1_human.append(top_1_human)\n",
    "    top1_model.append(top_1_model)\n",
    "    for j in range(len(data_vgg_gen)):\n",
    "        if df_serre_07.iloc[i, 0] in data_vgg_gen.iloc[j, 7] :\n",
    "            goals.append(data_vgg_gen.iloc[j, 2])\n",
    "            if data_vgg_gen.iloc[j, 2] == 'target':\n",
    "                likes_.append(round(data_vgg_gen.iloc[j, 3]))\n",
    "            else:\n",
    "                likes_.append(100-round(data_vgg_gen.iloc[j, 3] * 100))\n",
    "                df_serre_07.iloc[i, 1] = 100 - df_serre_07.iloc[i, 1]\n",
    "                df_serre_07.iloc[i, 2] = 100 - df_serre_07.iloc[i, 2]\n",
    "\n",
    "df_serre_07.insert( 3, 'human', top1_human)\n",
    "df_serre_07.insert( 4, 'model', top1_model)\n",
    "df_serre_07.insert(5, 'TLC', likes_)\n",
    "df_serre_07.insert(6, 'goals', goals)\n",
    "df_serre_07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmap = plt.cm.get_cmap('viridis')\n",
    "color_dict = pd.Series({k:cmap(i/3) for i,k in enumerate(range(3))})\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "names_ = ['Model Serre_07', 'Human', 'Vgg_tlc_Animal']\n",
    "plt.scatter(df_serre_07[1], df_serre_07[2], label='Model Serre_07', color=color_dict[0])\n",
    "plt.scatter(df_serre_07[1], df_serre_07['TLC'], label='Vgg_tlc_Animal', color=color_dict[2])\n",
    "#plt.scatter(df_serre_07[1], df_serre_07[4], label='Vgg_tlc_Animal', color=color_dict[4])\n",
    "ax.tick_params(axis='y', labelsize=14)\n",
    "plt.xticks(font=font)\n",
    "plt.yticks(font=font)\n",
    "plt.legend(bbox_to_anchor=(1.17, .5), loc='center', fontsize=20)\n",
    "ax.set_ylabel(\"\"\"Model's likelihood\"\"\", font=font)\n",
    "ax.set_xlabel(\"\"\"Human's Accuracy\"\"\", font=font)\n",
    "plt.tight_layout()\n",
    "# if do_savefig: fig.savefig(os.path.join(figpath, 'physio_comparison.pdf'), **opts_savefig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "https://github.com/laurentperrinet/PerrinetBednar15/blob/master/notebooks/5%20notebook_figure_animalness.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The networks obtain an accuracy on the 'animal' synset similar to those found in the model and neurophysiological data of [Serre & al (2007)](http://vision.stanford.edu/teaching/cs131_fall1415/lectures/Serre2007.pdf) (about 83\\%). Yet, the bias found in is also present in the dataset used by [Serre & al (2007)](http://vision.stanford.edu/teaching/cs131_fall1415/lectures/Serre2007.pdf).  when we compare the performances of humans on this dataset with the performances achieved by the network on an image-by-image basis we found in  [experiment 1](#exp1). However, we also found a high correspondence (about 84%) between the correct predictions of human and our model. Indeed for some images the networks failed the categorization but the human succeeds and vice versa. For some images both the network and human succeed or failed to categorize an animal. These exemples could be the reflection of the specific features on which humans or our models rely to perform their categorization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 7: Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HACK redundant with the other train_model function\n",
    "\n",
    "# # https://towardsdatascience.com/cross-entropy-for-classification-d98e7f974451\n",
    "# criterion = nn.BCEWithLogitsLoss() #binary_cross_entropy_with_logits\n",
    "\n",
    "# Training and saving the network\n",
    "\n",
    "# def train_model(model, num_epochs, dataloaders, lr=args.lr, momentum=args.momentum, beta2=args.beta2, log_interval=100, **kwargs):\n",
    "    \n",
    "#     model.to(device)\n",
    "#     if beta2 > 0.: \n",
    "#         optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(momentum, beta2)) \n",
    "#     else:\n",
    "#         optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum) # to set training variables\n",
    "\n",
    "#     df_train = pd.DataFrame([], columns=['epoch', 'avg_loss', 'avg_acc', 'avg_loss_val', 'avg_acc_val']) \n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         loss_train = 0\n",
    "#         acc_train = 0\n",
    "#         for i, (images, labels) in enumerate(dataloaders['train']):\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs[:,0], labels.float())\n",
    "\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             loss_train += loss.item() * images.size(0)\n",
    "#             preds = torch.round(torch.sigmoid(outputs[:,0].data))\n",
    "#             acc_train += torch.sum(preds == labels.data)\n",
    "            \n",
    "#         avg_loss = loss_train / dataset_sizes['animal']['train']\n",
    "#         avg_acc = acc_train / dataset_sizes['animal']['train']\n",
    "           \n",
    "#         with torch.no_grad():\n",
    "#             loss_val = 0\n",
    "#             acc_val = 0\n",
    "#             for i, (images, labels) in enumerate(dataloaders['val']):\n",
    "#                 images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "#                 outputs = model(images)\n",
    "#                 loss = criterion(outputs[:,0], labels.float())\n",
    "\n",
    "#                 loss_val += loss.item() * images.size(0)\n",
    "#                 preds = torch.round(torch.sigmoid(outputs[:,0].data))\n",
    "#                 acc_val += torch.sum(preds == labels.data)\n",
    "        \n",
    "#             avg_loss_val = loss_val / dataset_sizes['animal']['val']\n",
    "#             avg_acc_val = acc_val / dataset_sizes['animal']['val']\n",
    "        \n",
    "#         df_train.loc[epoch] = {'epoch':epoch, 'avg_loss':avg_loss, 'avg_acc':float(avg_acc),\n",
    "#                                'avg_loss_val':avg_loss_val, 'avg_acc_val':float(avg_acc_val)}\n",
    "#         print(f\"Epoch {epoch+1}/{num_epochs} : train= loss: {avg_loss:.4f} / acc : {avg_acc:.4f} - val= loss : {avg_loss_val:.4f} / acc : {avg_acc_val:.4f}\")\n",
    "\n",
    "#     model.cpu()\n",
    "#     # if device=='cuda': torch.cuda.empty_cache()\n",
    "#     return model, df_train\n",
    "\n",
    "# Downloading the model\n",
    "task = 'animal'\n",
    "models_vgg_gen = torchvision.models.vgg16(pretrained=True)  \n",
    "num_features = models_vgg_gen.classifier[-1].in_features\n",
    "features = list(models_vgg_gen.classifier.children())[:-1] # Remove last layer\n",
    "features.extend([nn.Linear(num_features, 1)]) # Add our layer with 1 output\n",
    "models_vgg_gen.classifier = nn.Sequential(*features) # Replace the model classifier\n",
    "model_filename = f'{data_cache}/{datetag}_{HOST}_train_{model_name}.pt'\n",
    "models_vgg['vgg16_tlc'].load_state_dict(torch.load(model_filename)) #on GPU\n",
    "\n",
    "m = 0\n",
    "input_lin_1 = 12544\n",
    "pruned_depths = [28, 26, 23, 20, 18, 15, 13, 11, 8, 6, 3, 2]\n",
    "all_models = ['vgg-1', 'vgg-2','vgg-3','vgg-4', 'vgg-5','vgg-6','vgg-7','vgg-8','vgg-9','vgg-10', 'vgg-11', 'vgg-12']\n",
    "for model_name, pruned_depth in zip(all_models, pruned_depths):\n",
    "\n",
    "    vgg_head = torchvision.models.vgg16(pretrained=True)\n",
    "    goal =  list(vgg_head.features.children())\n",
    "    model_filename = args.model_path + model_name + '_true.pt'\n",
    "\n",
    "    models_vgg[model_name] = torchvision.models.vgg16(pretrained=True)\n",
    "    conv_part = goal[:pruned_depth]\n",
    "    conv_part.append(goal[-1])\n",
    "    models_vgg[model_name].features = torch.nn.Sequential(*conv_part) #get the new features parameters in the model\n",
    "\n",
    "    num_features = models_vgg[model_name].classifier[6].in_features\n",
    "    features = list(models_vgg[model_name].classifier.children())[:-1] # Remove last layer  \n",
    "    \n",
    "    if model_name in ['vgg-6', 'vgg-7', 'vgg-8']:   \n",
    "        del features[0] #remove first layer\n",
    "        features.insert(0, nn.Linear(input_lin_1, num_features)) # Add your custom input layer\n",
    "        features.extend([nn.Linear(num_features, 1)]) # Add our layer with 1 output\n",
    "        models_vgg[model_name].classifier = nn.Sequential(*features) # Replace the model classifier   \n",
    "        m += 1\n",
    "        \n",
    "    elif model_name in ['vgg-9', 'vgg-10']:\n",
    "        del features[0] #remove first layer\n",
    "        features.insert(0, nn.Linear(input_lin_1//2, num_features)) # Add your custom input layer\n",
    "        features.extend([nn.Linear(num_features, 1)]) # Add our layer with 1 output\n",
    "        models_vgg[model_name].classifier = nn.Sequential(*features) # Replace the model classifier   \n",
    "        m += 1\n",
    "        \n",
    "    elif model_name in ['vgg-11', 'vgg-12']:\n",
    "        del features[0] #remove first layer\n",
    "        features.insert(0, nn.Linear(input_lin_1//4, num_features)) # Add your custom input layer\n",
    "        features.extend([nn.Linear(num_features, 1)]) # Add our layer with 1 output\n",
    "        models_vgg[model_name].classifier = nn.Sequential(*features) # Replace the model classifier   \n",
    "        m += 1\n",
    "\n",
    "    else : \n",
    "        features.extend([nn.Linear(num_features, 1)]) # Add our layer with n outputs\n",
    "        models_vgg[model_name].classifier = nn.Sequential(*features) # Replace the model classifier     \n",
    "        m += 1\n",
    "\n",
    "    # model_filename = args.model_path +model_name+\"_\"+ task +'.pt' # Uncomment to load dog models HACK ???\n",
    "\n",
    "    if os.path.isfile(model_filename):\n",
    "        print(\"Loading pretrained model for..\", model_name, 'from', model_filename)\n",
    "        #print(\"Resume_training : \", resume_training)\n",
    "        if device in ['cuda', 'mps']:\n",
    "            models_vgg[model_name].load_state_dict(torch.load(model_filename)) #on GPU\n",
    "        else:\n",
    "            models_vgg[model_name].load_state_dict(torch.load(model_filename, map_location=torch.device('cpu'))) #on CPU\n",
    "    else :\n",
    "        filename = f'{data_cache}/{datetag}_{HOST}_train_{model_name}.json'\n",
    "        print(\"Re-training pretrained model...\", model_filename)\n",
    "        since = time.time()\n",
    "        print(f\"Traning {model_name}, image_size={args.image_size}\")\n",
    "        (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(image_size=args.image_size)\n",
    "        models_vgg[model_name], df_train = train_model(models_vgg[model_name], num_epochs=args.num_epochs,\n",
    "                                                    dataloaders=dataloaders[task])\n",
    "        torch.save(models_vgg[model_name].state_dict(), model_filename)\n",
    "        df_train.to_json(filename)\n",
    "        elapsed_time = time.time() - since\n",
    "        print(f\"Training completed in {elapsed_time // 60:.0f}m {elapsed_time % 60:.0f}s\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing the networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exp1\"></a>\n",
    "### Experiment 7 a : Image processing and recognition for differents labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = f'{data_cache}/{datetag}_{HOST}_results_1_del_lay.json'\n",
    "print(f'{filename=}')\n",
    "task = 'animal'\n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    df_del_lay = pd.read_json(filename)\n",
    "else:\n",
    "    i_trial = 0\n",
    "    df_del_lay = pd.DataFrame([], columns=['model', 'goal', 'likelihood', 'fps', 'time', 'i_image', 'filename', 'top_1']) \n",
    "        # image preprocessing\n",
    "    (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(paths=paths, image_size=args.image_size, batch_size=1)\n",
    "    for i_image, (data, label) in enumerate(dataloaders[task]['test']):\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        for model_name in models_vgg.keys():\n",
    "            model = models_vgg[model_name].to(device)\n",
    "            with torch.no_grad():\n",
    "                goal = 'target' if 'target' in image_datasets[task]['test'].imgs[i_image][0] else 'distractor'\n",
    "                tic = time.time()\n",
    "                out = model(data).squeeze(0).cpu()\n",
    "                if model_name == 'vgg_16':\n",
    "                    model_task = 'Imagenet_challenge'\n",
    "                    percentage = torch.nn.functional.softmax(out, dim=0)\n",
    "                    likelihood = np.asarray(percentage[match[task]]).sum()\n",
    "                else:\n",
    "                    likelihood = torch.sigmoid(out).numpy()[0]\n",
    "                top_1 = 'target' if likelihood>0.50 else 'distractor'\n",
    "                elapsed_time = time.time() - tic\n",
    "            df_del_lay.loc[i_trial] = {'model':model_name, 'top_1':top_1, 'goal':goal, 'likelihood':likelihood, 'time':elapsed_time, 'fps': 1/elapsed_time,\n",
    "                                    'i_image':i_image, 'filename':image_datasets[task]['test'].imgs[i_image][0]}\n",
    "            print(f'The {model_name} model categorize an animal with {likelihood*100:.3f} % likelihood ({top_1}) in {elapsed_time:.3f} seconds, groundtruth= {task}, {goal}')\n",
    "            i_trial += 1\n",
    "    df_del_lay.to_json(filename)\n",
    "df_del_lay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_del_lay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "cmap = plt.cm.get_cmap('viridis')\n",
    "models = df_del_lay['model'].unique()\n",
    "N_model = len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models[2:] = models[1:-1] # HACK\n",
    "# models[1] = 'vgg16_tlc'\n",
    "# models[0] = 'vgg_16'\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc = pd.DataFrame({model_name: {'accuracy': accuracy_score(df_del_lay[(df_del_lay['model']==model_name)][\"top_1\"], \n",
    "                                                         df_del_lay[(df_del_lay['model']==model_name)][\"goal\"])\n",
    "                                    } \n",
    "                       for model_name in models})\n",
    "\n",
    "ax = df_acc.T.plot.bar(rot=20, figsize=(fig_width, fig_width//4), fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc = pd.Series({model_name: accuracy_score(df_del_lay[(df_del_lay['model']==model_name)][\"top_1\"], \n",
    "                                                         df_del_lay[(df_del_lay['model']==model_name)][\"goal\"])\n",
    "                       for model_name in models})\n",
    "\n",
    "ax = df_acc.plot.bar(rot=20, figsize=(fig_width, fig_width//4), fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc.rename({'vgg16_tlc': 'VGG TLC', 'vgg_16': 'VGG LUT'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = {model_name: cmap(i/N_model + .25) for i, model_name in enumerate(models)}\n",
    "colors = [cmap(1-.9*i_color/N_model) for i_color in range(N_model)]\n",
    "fig, ax = plt.subplots(figsize=(fig_width, fig_width/4))\n",
    "\n",
    "ax = df_acc.plot(kind='bar',    # Plot a bar chart\n",
    "                 rot=20, fontsize=fontsize,\n",
    "                 legend=False,    # Turn the Legend off\n",
    "                 width=0.75,      # Set bar width as 75% of space available\n",
    "                 ax=ax,\n",
    "                 color=colors)\n",
    "\n",
    "ax.hlines(xmin=-.5, xmax=N_model-.5, y=1/2, ls='--', ec='gray')\n",
    "ax.set_ylabel('Accuracy', font=font)\n",
    "\n",
    "for container in ax.containers: \n",
    "    # ax.bar_label(container, padding=-50, fmt='%.3f', rotation=90, color='white', fontsize=fontsize, weight='bold')\n",
    "    ax.bar_label(container, padding=-50, fmt='%.3f', rotation=90, color='black', fontsize=fontsize, weight='bold')\n",
    "\n",
    "if do_savefig: fig.savefig(os.path.join(figpath, 'pruning_accuracy.pdf'), **opts_savefig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exp1\"></a>\n",
    "### Experiment 7b: Image processing and recognition for differents labels with rotation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %rm {filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = f'{data_cache}/{datetag}_{HOST}_results_rotate_pruned.json'\n",
    "print(f'{filename=}')\n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    df_angle = pd.read_json(filename)\n",
    "else:\n",
    "    i_trial = 0\n",
    "    df_angle = pd.DataFrame([], columns=['mean_prediction', 'model', 'var'])\n",
    "    (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(paths=paths, image_size=args.image_size, batch_size=batch_size)\n",
    "    # image preprocessing\n",
    "    #for task in args.tasks:\n",
    "    task = 'animal'\n",
    "    for angle in angles:\n",
    "        print(f'Rotation by {angle=}°')\n",
    "        for model_name in models_vgg.keys():\n",
    "            model = models_vgg[model_name].to(device)\n",
    "            acc_= 0\n",
    "            for i_image, (data, label) in enumerate(dataloaders[task]['test']):\n",
    "                data = transforms.functional.rotate(data, angle=float(angle), expand = True)\n",
    "                data, label, = data.to(device), label.to(device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(data)\n",
    "                    if model_name == 'vgg_16':\n",
    "                        percentage = torch.nn.functional.softmax(outputs, dim=1)\n",
    "                        preds = torch.sum(percentage[:,match[task]], dim=1)\n",
    "                        acc_ += torch.sum(torch.round(preds) == label.data)\n",
    "                    else:\n",
    "                        preds = torch.round(torch.sigmoid(outputs[:,0].data))\n",
    "                        acc_ += torch.sum(preds == label.data)\n",
    "            avg_acc = acc_ / dataset_sizes[task]['test']\n",
    "            df_angle.loc[i_trial] = {'model':model_name, 'mean_prediction':float(avg_acc), 'var':angle}  \n",
    "            print(model_name, float(avg_acc), angle)\n",
    "            #print(f'The {model_name} model categorize {model_task} with {likelihood*100:.3f} % likelihood ({top_1}) in {elapsed_time:.3f} seconds, groundtruth : {task}, {goal}')\n",
    "            i_trial += 1\n",
    "            # torch.cuda.empty_cache()\n",
    "    df_angle.to_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_angle.rename({'vgg16_tlc': 'VGG TLC', 'vgg_16': 'VGG LUT'}, axis=1, inplace=True)\n",
    "df_angle.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = f'{data_cache}/{datetag}_{HOST}_results_rotate_pruned.json'\n",
    "df_angle = pd.read_json(filename)\n",
    "df_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models, model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = {model:model for model in models}\n",
    "model_names['vgg16_tlc'] = 'VGG TLC'\n",
    "model_names['vgg_16'] = 'VGG LUT'\n",
    "model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "import statistics\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 10))\n",
    "angles = np.arange(-180, -55, 1)\n",
    "cmap = plt.cm.get_cmap('viridis_r')\n",
    "color_dict = pd.Series({k:cmap(i/len(df_angle['model'].unique())) for i,k in enumerate(df_angle['model'].unique())})\n",
    "\n",
    "all_models = ['vgg-1', 'vgg-2','vgg-3','vgg-4', 'vgg-5','vgg-6','vgg-7','vgg-8','vgg-9','vgg-10', 'vgg-11', 'vgg-12']\n",
    "\n",
    "for model_name in df_angle['model'].unique():         \n",
    "    df_angle[(df_angle['model']==model_name)].plot.scatter(x=\"var\", y=\"mean_prediction\", c=[color_dict[model_name],],\n",
    "                                                         marker='o', lw=0, label=model_name, ax=ax)\n",
    "    print(f'Mean for {model_name}', statistics.mean(df_angle[(df_angle['model']==model_name)]['mean_prediction'])*100)\n",
    "    print(f'Std for {model_name}', statistics.stdev(df_angle[(df_angle['model']==model_name)]['mean_prediction'])*100)\n",
    "ax.legend(loc='center right', fontsize = 12)\n",
    "ax.tick_params(axis='x', labelsize=14)\n",
    "ax.tick_params(axis='y', labelsize=14)\n",
    "ax.set_xlim(-185, 185)\n",
    "ax.set_xticks([-180, -90, 0, 90, 180])\n",
    "ax.set_ylabel('Mean Accuracies', size= 25, fontweight='bold')\n",
    "ax.set_xlabel('Rotation (°)', size= 25, fontweight='bold')\n",
    "plt.legend(bbox_to_anchor=(1.15, .5), loc='center', fontsize=20)\n",
    "plt.tight_layout();\n",
    "plt.xticks(fontsize=20,  fontweight='bold')\n",
    "plt.yticks(fontsize=20,  fontweight='bold');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(fig_width/1.2, fig_width/phi/1.2))\n",
    "colors = [cmap(1-.9*i_color/N_model) for i_color in range(N_model)]\n",
    "colors[0] = (0, 0, 0, 1)\n",
    "\n",
    "for i, model_name in enumerate(models):\n",
    "    ax = df_angle[(df_angle['model']==model_name)].plot(x=\"var\", y=\"mean_prediction\", color=colors[i],\n",
    "                                                         marker='o', lw=1, label=model_names[model_name], ax=ax)\n",
    "    print(f'Mean for {model_name}', np.mean(df_angle[(df_angle['model']==model_name)]['mean_prediction']))\n",
    "    # print(f'Std for {model_name}', np.std(df_angle[(df_angle['model']==model_name)]['mean_prediction']))\n",
    "ax = df_angle[(df_angle['model']=='vgg_16')].plot(x=\"var\", y=\"mean_prediction\", color=colors[0],\n",
    "                                                    marker='o', lw=1, ax=ax)\n",
    "\n",
    "\n",
    "ax.hlines(xmin=-185, xmax=185, y=1/2, ls='--', ec='gray')#, label='chance level')\n",
    "# ax.legend(loc='center right', fontsize=12)\n",
    "ax.tick_params(axis='x', labelsize=14)\n",
    "ax.tick_params(axis='y', labelsize=14)\n",
    "ax.set_xlim(-185, 185)\n",
    "ax.set_xticks([-180, -90, 0, 90, 180])\n",
    "ax.set_ylabel('Mean Accuracies', font=font)\n",
    "ax.set_xlabel('Rotation (°)', font=font)\n",
    "plt.legend(bbox_to_anchor=(1.1, .5), loc='center', fontsize=fontsize, edgecolor='none')\n",
    "plt.tight_layout();\n",
    "plt.xticks(font=font)\n",
    "plt.yticks(font=font)\n",
    "if do_savefig: fig.savefig(os.path.join(figpath, 'full_rot_pruned.pdf'), **opts_savefig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 7c: Image processing and recognition for different labels with shuffled patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'{data_cache}/{datetag}_{HOST}_results_shuffle.json'\n",
    "print(f'{filename=}')\n",
    "\n",
    "task = 'animal'\n",
    "shuffle_sizes = [256, 128, 64, 32]\n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    df_shuffle = pd.read_json(filename)\n",
    "else:\n",
    "    i_trial = 0\n",
    "    df_shuffle = pd.DataFrame([], columns=['mean_prediction', 'model', 'var'])\n",
    "    for shuffle_size in shuffle_sizes:\n",
    "        print(shuffle_size)\n",
    "        (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(\n",
    "            image_size=args.image_size, shuffle=shuffle_size, batch_size=batch_size)\n",
    "        for model_name in models_vgg.keys():\n",
    "            model = models_vgg[model_name].to(device)\n",
    "            acc_= 0\n",
    "            for i_image, (data, label) in enumerate(dataloaders[task]['test']):\n",
    "                data, label = data.to(device), label.to(device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(data)\n",
    "                    if model_name == 'vgg_16':\n",
    "                        percentage = torch.nn.functional.softmax(outputs, dim=1)\n",
    "                        preds = torch.sum(percentage[:,match[task]], dim=1)\n",
    "                        acc_ += torch.sum(torch.round(preds) == label.data)\n",
    "                    else:\n",
    "                        preds = torch.round(torch.sigmoid(outputs[:,0].data))\n",
    "                        acc_ += torch.sum(preds == label.data)\n",
    "            avg_acc = acc_ / dataset_sizes[task]['test']\n",
    "            df_shuffle.loc[i_trial] = {'model':model_name, 'mean_prediction':float(avg_acc), 'var':shuffle_size}  \n",
    "            print(model_name, float(avg_acc), shuffle_size)\n",
    "            i_trial += 1\n",
    "            torch.cuda.empty_cache()\n",
    "        df_shuffle.to_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shuffle = pd.read_json(filename)\n",
    "df_shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(fig_width/2, fig_width/(phi*2)))\n",
    "\n",
    "cmap = plt.cm.get_cmap('viridis')\n",
    "all_models = ['VGG 16', 'VGG TLC','vgg-1', 'vgg-2','vgg-3','vgg-4', 'vgg-5','vgg-6','vgg-7','vgg-8','vgg-9','vgg-10', 'vgg-11', 'vgg-12']\n",
    "\n",
    "color_dict = pd.Series({k:cmap(i/len(shuffle_sizes)) for i,k in enumerate(shuffle_sizes)})\n",
    "all_ = ['temp_','temp_','temp_', 'temp_', 'temp', 'temp_']\n",
    "cmap = plt.cm.get_cmap('inferno')\n",
    "color_dict_s = pd.Series({k:cmap(i/len(all_)) for i,k in enumerate(all_)})\n",
    "color_dict[64] = color_dict_s['temp']\n",
    "\n",
    "for shuffle_size in shuffle_sizes:\n",
    "    #temp = df_shuffle[(df_shuffle['var']==shuffle_size)]\n",
    "    #ax.scatter(temp['model'], temp['mean_prediction'],\n",
    "    #           color=[color_dict[shuffle_size],], marker= 'o', label=f'{shuffle_size} x {shuffle_size}')\n",
    "    \n",
    "    \n",
    "    ax = df_shuffle[(df_shuffle['var']==shuffle_size)].plot(x=\"model\", y=\"mean_prediction\", color=color_dict[shuffle_size],\n",
    "                                                         marker='o', label=f'{shuffle_size} x {shuffle_size}', ax=ax)\n",
    "    #ax.set_xticklabels(all_models)\n",
    "    fig.autofmt_xdate(rotation=45)\n",
    "    ax.set_ylabel('Mean Accuracies', font=font)\n",
    "    ax.hlines(xmin=-.5, xmax=len(all_models)-.5, y=1/2, ls='--', ec='gray')\n",
    "    plt.xticks(fontsize=fontsize)\n",
    "    plt.yticks(fontsize=fontsize)\n",
    "\n",
    "ax.set_xlabel('', font=font)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.12, .5), loc='center', fontsize=fontsize, edgecolor='none')\n",
    "if do_savefig: fig.savefig(os.path.join(figpath, 'shuffle.pdf'), **opts_savefig);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS: testing the model on images generated by stable diffusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [GitHub](https://github.com/CompVis/latent-diffusion).\n",
    "\n",
    "```\n",
    "@misc{rombach2021highresolution,\n",
    "      title={High-Resolution Image Synthesis with Latent Diffusion Models}, \n",
    "      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},\n",
    "      year={2021},\n",
    "      eprint={2112.10752},\n",
    "      archivePrefix={arXiv},\n",
    "      primaryClass={cs.CV}\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "Generated by\n",
    "```\n",
    "PYTORCH_ENABLE_MPS_FALLBACK=1 python scripts/txt2img.py --prompt \"A very realistic object that does not exist in reality.\" --plms --n_samples=4 --n_rows=2 --n_iter=1 --seed 1998 --outdir=\"outputs/impossible-artifact\"\n",
    "```\n",
    "\n",
    "with the following prompts :\n",
    "\n",
    "* `animal-artifact` = \"A very realistic animal that looks like an object.\" \n",
    "* `artifact-animal\"` = \"A very realistic object that looks like an animal.\" \n",
    "* `impossible-animal` = \"A very realistic animal that does not exist in reality.\" \n",
    "* `impossible-artifact` = \"A very realistic object that does not exist in reality.\" \n",
    "* `three-legged-animal` = \"A very realistic animal that has three legs.\" \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "nteract": {
   "version": "0.28.0"
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
