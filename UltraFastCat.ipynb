{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ultrafast Image Categorization in *Biology* and *Neural Models*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we train a network to categorize images which contain or not a given class, such as *animals* or *artifacts*. We will use the [Pytorch](https://pytorch.org/) library to run the networks and the [pandas](https://pandas.pydata.org/docs/getting_started/index.html) library to collect and display the results. \n",
    "\n",
    "I uses transfer learning to train the DCNN, starting from a VGG16 network, taken from the `torchvision.models` library, pre-trained on the [Imagenet](http://image-net.org/) dataset which allows to perform label detection on naturals images for $K = 1000$ labels. Then re-train the whole network to perfom the same task but in a sub-set of $K = 1$ synset from the ImageNet dataset. The dataset I used here to train the networks is not multilabel (as we have the information about the occurence of only one synset on the scene), in order to train networks on independant task I choose to limit the output of the DCNN to $K = 1$ synset. \n",
    "\n",
    "We are going to adopt differents strategies of transfer learning:\n",
    "\n",
    "* **VGG TLC** (transfer Learning on Classification layers): Substitute the last layer of the Pytorch VGG16 network ($K = 1000$ labels) with a new layer build from a specific subset ($K = 1$ synset), the training process is then applied on the new layer, \n",
    "* **VGG TLA** (transfer Learning on All layers): Same training process as TLC but applied on all the layers of the network, \n",
    "* **VGG TLDA** (Transfer Learning with Data Augmentation): Same training process as TLC but with a custom set of transformations applied on the dataset,\n",
    "* **VGG TLAA** (Transfer Learning with Auto Augment function): Same training process as TLC but with the [Auto Augment](https://pytorch.org/vision/master/transforms.html#torchvision.transforms.AutoAugmentdistractor) transformation applied on the dataset,\n",
    "* **VGG SLS** (Supervised Learning from Scratch): Same training process as TLC but the learning process start with random weigths. \n",
    "* **VGG LUT** (look up table): An adaptation of the outputs from a VGG16 network without any training process. \n",
    "\n",
    "The first part consist of the initialization of the librairies and the definition of the dataset. Then follow training of the network and the last part of this notebook is dedicated to the test of the robustness of the resulted networks while appliying various geometric tranformations to the input. Finally I analyse the similarities in the performances of the networks compared with physiological data and challenge that categorization by pruning the network.  \n",
    "\n",
    "This notebook was done by  [Jean-Nicolas Jérémie](https://github.com/JNJER) under the supervision of [Laurent PERRINET](https://laurentperrinet.github.io/) at the Neurosciences Institute of Timone (INT). It is curated in the following [github repo](https://github.com/SpikeAI/2022-09_UltraFastCat)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first install requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "* normalize the fonts\n",
    "\n",
    "* remove `HACK`s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:35.138195Z",
     "iopub.status.busy": "2023-04-26T16:40:35.137838Z",
     "iopub.status.idle": "2023-04-26T16:40:35.143058Z",
     "shell.execute_reply": "2023-04-26T16:40:35.142089Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install --upgrade -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:35.148334Z",
     "iopub.status.busy": "2023-04-26T16:40:35.147594Z",
     "iopub.status.idle": "2023-04-26T16:40:35.557142Z",
     "shell.execute_reply": "2023-04-26T16:40:35.556393Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of the libraries/variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our coding strategy is to include all scripts in this notebook. This notebook contains the scripts necessary to set the variables of the definition and the training of the networks and assumes you have generated the dataset based on the [ImageNet database](https://www.image-net.org/) dedicated to our specific ecologic task. \n",
    "\n",
    "The set of labels of the ImageNet database is based on a large lexical database of English: [Wordnet](https://wordnet.princeton.edu/) . The nouns, verbs, adjectives, and adverbs in this database are grouped into a graphical set of cognitive synonyms (synset), each expressing a distinct concept. These synsets are linked between them by employing a small number of conceptual relations. I used the hyperonym link, for instance, a German shepherd is kind of a dog and a dog is a kind of an animal thus defining an hyperonym path. So the synset 'animal' is in the hyperonym path of the synset 'German sheperd'. Based on this relation, we selected a specific subset of labels in the ImageNet database to build our datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:35.561834Z",
     "iopub.status.busy": "2023-04-26T16:40:35.561579Z",
     "iopub.status.idle": "2023-04-26T16:40:35.566037Z",
     "shell.execute_reply": "2023-04-26T16:40:35.565477Z"
    }
   },
   "outputs": [],
   "source": [
    "from time import strftime, gmtime\n",
    "datetag: str = strftime(\"%Y-%m-%d\", gmtime())\n",
    "print(datetag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:35.636021Z",
     "iopub.status.busy": "2023-04-26T16:40:35.635773Z",
     "iopub.status.idle": "2023-04-26T16:40:35.639582Z",
     "shell.execute_reply": "2023-04-26T16:40:35.638937Z"
    }
   },
   "outputs": [],
   "source": [
    "data_cache = 'cached_data'\n",
    "#%mkdir -p {data_cache}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:35.643025Z",
     "iopub.status.busy": "2023-04-26T16:40:35.642793Z",
     "iopub.status.idle": "2023-04-26T16:40:35.646903Z",
     "shell.execute_reply": "2023-04-26T16:40:35.646044Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "HOST = 'Tesla'\n",
    "HOST = 'babbage'\n",
    "# HOST, datetag = \"neo-ope-de04\", \"2023-03-06\" # comment to run again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, a `UltraFastCat/init.py` to define all our usefull variables like the new labels to learn, the number of training images or the root. Also, we importe libraries to train the networks and display the results. Then we define the `transform` functions for the datasets. \n",
    "HACK : To perform image augmentation we apply the Pytorch `AutoAugment` function to the `train` and `val` dataset. I also add a grayscale, shuffle and resize function in order to test different training strategies and test the networks on various conditions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:35.650902Z",
     "iopub.status.busy": "2023-04-26T16:40:35.650360Z",
     "iopub.status.idle": "2023-04-26T16:40:36.816157Z",
     "shell.execute_reply": "2023-04-26T16:40:36.815445Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import torch\n",
    "import torch.nn.functional as nnf\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "# from torchvision.datasets import ImageFolder\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(42*42)\n",
    "torch.set_printoptions(precision=3, linewidth=140, sci_mode=False)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "torch.__version__, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:36.819356Z",
     "iopub.status.busy": "2023-04-26T16:40:36.819145Z",
     "iopub.status.idle": "2023-04-26T16:40:36.989440Z",
     "shell.execute_reply": "2023-04-26T16:40:36.988688Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "# to store results\n",
    "import pandas as pd\n",
    "\n",
    "# https://docs.python.org/3/library/dataclasses.html?highlight=dataclass#module-dataclasses\n",
    "from dataclasses import dataclass, asdict, field\n",
    "@dataclass\n",
    "class Params:\n",
    "    DEBUG = 1\n",
    "    \n",
    "    datetag: str = datetag # Set the date of the result's file\n",
    "    loader: str = f'../DataSetMaker/Imagenet_urls_ILSVRC_2016.json' # File containing Imagenet's labels\n",
    "    model_path: str = f'{data_cache}/{datetag}_re-train_' # Set the parth to store the re-trained model\n",
    "    datasets_root: str = '../data/' # Directory containing images to perform the training\n",
    "    \n",
    "    folders: list = field(default_factory=lambda: ['test', 'val', 'train']) # Set the test, validation and training folders relative to the root\n",
    "    image_sizes: list = field(default_factory=lambda: [64, 128, 256, 512]) # Set the input sizes\n",
    "    model_names: list = field(default_factory=lambda: ['vgg16_tlaa' ,'vgg16_tlda', 'vgg16_tla','vgg16_tlc', 'vgg16_sls']) # Set the name of the networks\n",
    "    tasks: list = field(default_factory=lambda: ['animal', 'artifact', 'random']) # Synset used as a target during the transfer learning \n",
    "    goals: list = field(default_factory=lambda: ['target', 'distractor']) # Set the different categories for the tasks\n",
    "    \n",
    "    image_size: int = 244 # Set the default image_size of the input\n",
    "    num_epochs: int = 25//DEBUG # Set the number of epoch during the training process\n",
    "    batch_size: int = 8 # Set number of images per input batch\n",
    "    lr: int = 0.00005 # Set learning rate \n",
    "    momentum: int = 0.99 # Set the momentum\n",
    "\n",
    "\n",
    "#datetag = args.datetag\n",
    "args = Params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:36.994058Z",
     "iopub.status.busy": "2023-04-26T16:40:36.993856Z",
     "iopub.status.idle": "2023-04-26T16:40:37.701066Z",
     "shell.execute_reply": "2023-04-26T16:40:37.699899Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('omw-1.4', download_dir=args.datasets_root)\n",
    "nltk.download('wordnet', download_dir=args.datasets_root)\n",
    "nltk.data.path.append(args.datasets_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:37.705405Z",
     "iopub.status.busy": "2023-04-26T16:40:37.705189Z",
     "iopub.status.idle": "2023-04-26T16:40:37.735769Z",
     "shell.execute_reply": "2023-04-26T16:40:37.735051Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "fontsize = 15\n",
    "font = font_manager.FontProperties(weight='normal', size=fontsize)\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# matplotlib parameters\n",
    "tasks_color = f'{args.tasks[0]}-sandybrown,{args.tasks[1]}-lightsteelblue,{args.tasks[2]}-thistle'\n",
    "tasks_color = dict(map(lambda i: i.split('-'), tasks_color.split(',')))\n",
    "fig_width = 15\n",
    "phi = (np.sqrt(5)+1)/2 # golden ratio for the figures :-)\n",
    "\n",
    "#to plot & display \n",
    "def pprint(message): #display function\n",
    "    print('-'*len(message))\n",
    "    print(message)\n",
    "    print('-'*len(message))\n",
    "\n",
    "do_savefig = False\n",
    "# figpath = '../2022-12-15_Jérémie-etal-Vision_630dcf8012267a5bb40967dd/figures'\n",
    "figpath = 'figures'\n",
    "#%mkdir -p {figpath}\n",
    "opts_savefig = dict(dpi='figure', bbox_inches='tight', pad_inches=0, edgecolor=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset using WORDNET hierarchy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:37.740688Z",
     "iopub.status.busy": "2023-04-26T16:40:37.740483Z",
     "iopub.status.idle": "2023-04-26T16:40:38.305345Z",
     "shell.execute_reply": "2023-04-26T16:40:38.304630Z"
    }
   },
   "outputs": [],
   "source": [
    "#DCCN training\n",
    "print(f'On date {args.datetag}, Running benchmark on host with device {device}')\n",
    "\n",
    "paths = {}\n",
    "for task in args.tasks :\n",
    "    paths[task] = {}\n",
    "    for folder in args.folders:\n",
    "        paths[task][folder] = os.path.join(args.datasets_root, task, folder) # data path sorted per tasks then folders\n",
    "\n",
    "\n",
    "#----------------Get the label for the Imagenet categorization------------------------\n",
    "import json\n",
    "with open(args.loader) as json_file:\n",
    "    Imagenet_labels_ILSVRC_2016 = json.load(json_file)\n",
    "match = {}\n",
    "for task in args.tasks :\n",
    "    match[task] = []\n",
    "for i_img, img_id in enumerate(Imagenet_labels_ILSVRC_2016):\n",
    "    syn_= wn.synset_from_pos_and_offset('n', int(img_id.replace('n',''))) #get the corresponding synset\n",
    "    sem_ = syn_.hypernym_paths()[0] # get the hyperonym path of that synset\n",
    "    for task in args.tasks[:-1]:\n",
    "        for i in np.arange(len(sem_)):\n",
    "            if sem_[i].lemmas()[0].name() in task : # if, in this last hyperonym path a synset fits the task,\n",
    "                match[task].append(i_img)           # then it is placed in the corresponding match list \n",
    "                                          \n",
    "\n",
    "match['random'] = []\n",
    "random_file = '../data/random/random_choice_distractors.json'\n",
    "if os.path.exists(random_file):\n",
    "    print('Random mode : Using last random selection')\n",
    "    with open(random_file) as f:\n",
    "        class_wnids = list(json.load(f))\n",
    "    for i_img, img_id in enumerate(Imagenet_labels_ILSVRC_2016):\n",
    "        if img_id[:9] not in class_wnids:\n",
    "            match['random'].append(i_img)\n",
    "else:\n",
    "    print('Missing file: something is wrong here')\n",
    "\n",
    "\n",
    "# normalization used to train VGG\n",
    "# see https://pytorch.org/hub/pytorch_vision_vgg/\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "transforms_norm = transforms.Normalize(mean=mean, std=std) # to normalize colors on the ImageNet dataset\n",
    "\n",
    "image_datasets = {}\n",
    "dataloaders = {}\n",
    "dataset_sizes = {}\n",
    "\n",
    "# VGG-16 datasets initialisation : implementing different data augmentation startegies\n",
    "def datasets_transforms(paths=paths, image_size=args.image_size, p=0, shuffle=args.image_size , num_workers=0, angle=0, batch_size=args.batch_size, **kwargs):\n",
    "\n",
    "\n",
    "    if p == .5 : # TLDA transform \n",
    "        transforms_seq = transforms.Compose([\n",
    "                            # transforms.AutoAugment(), # https://pytorch.org/vision/master/transforms.html#torchvision.transforms.AutoAugmentdistractor\n",
    "                            transforms.RandomHorizontalFlip(p=p),\n",
    "                            transforms.RandomVerticalFlip(p=p),\n",
    "                            transforms.RandomRotation(degrees=180, expand=True),\n",
    "                            transforms.RandomGrayscale(p=p),\n",
    "#                             https://pytorch.org/vision/master/generated/torchvision.transforms.RandomResizedCrop.html#torchvision.transforms.RandomResizedCrop\n",
    "#                             scale (tuple of python:float) – Specifies the lower and upper bounds for the random area of the crop, before resizing. The scale is defined with respect to the area of the original image.\n",
    "# ratio (tuple of python:float) – lower and upper bounds for the random aspect ratio of the crop, before resizing.\n",
    "                            transforms.RandomResizedCrop(image_size, scale=(0.7, 1.0)),\n",
    "                            # transforms.Resize((int(image_size), int(image_size))),              \n",
    "                            transforms.ToTensor(),\n",
    "                            transforms_norm ])\n",
    "    elif p == 1 : # TLAA transform \n",
    "        transforms_seq = transforms.Compose([\n",
    "                    transforms.Resize((int(image_size), int(image_size))),\n",
    "                    transforms.AutoAugment(), # https://pytorch.org/vision/master/transforms.html#torchvision.transforms.AutoAugmentdistractor\n",
    "                    transforms.ToTensor(),    # Convert the image to pyTorch Tensor data type.\n",
    "                    transforms_norm ])\n",
    "        \n",
    "    else: # No data augmentation just a resize and a normalization\n",
    "        transforms_seq = transforms.Compose([\n",
    "                            transforms.Resize((int(image_size), int(image_size))),\n",
    "                            transforms.ToTensor(),    # Convert the image to pyTorch Tensor data type.\n",
    "                            transforms_norm ])\n",
    "    \n",
    "    data_transforms = {\n",
    "        'train': transforms_seq,\n",
    "        'val': transforms_seq,\n",
    "        'test': transforms_seq}\n",
    "    \n",
    "\n",
    "    for task in args.tasks:\n",
    "        image_datasets[task] = {\n",
    "            folder: datasets.ImageFolder(\n",
    "                paths[task][folder], \n",
    "                transform=data_transforms[folder]\n",
    "            )\n",
    "            for folder in args.folders\n",
    "        }\n",
    "\n",
    "        dataloaders[task] = {\n",
    "            folder: torch.utils.data.DataLoader(\n",
    "                image_datasets[task][folder], batch_size=batch_size,\n",
    "                shuffle=False if folder == \"test\" else True, num_workers=num_workers\n",
    "            )\n",
    "            for folder in args.folders\n",
    "        }\n",
    "\n",
    "        dataset_sizes[task] = {folder: len(image_datasets[task][folder]) for folder in args.folders}\n",
    "\n",
    "    return dataset_sizes, dataloaders, image_datasets, data_transforms\n",
    "\n",
    "(dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(paths=paths, image_size=args.image_size)\n",
    "\n",
    "for task in args.tasks :\n",
    "    pprint(f'Pre-selected tasks {task} : ')\n",
    "    for folder in args.folders : print(f\"Loaded {dataset_sizes[task][folder]} images under {folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Image display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:38.310101Z",
     "iopub.status.busy": "2023-04-26T16:40:38.309899Z",
     "iopub.status.idle": "2023-04-26T16:40:40.141354Z",
     "shell.execute_reply": "2023-04-26T16:40:40.140685Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "#----------------------------Display randomly picked images------------------------------------\n",
    "print('\\n')\n",
    "pprint(f'Some sample images picked at random :')\n",
    "# import imageio\n",
    "import imageio.v2 as imageio\n",
    "N_image_i = 4\n",
    "image_plot_paths = {}\n",
    "task_folder = {}\n",
    "x = 0\n",
    "folder = 'train'\n",
    "fig, axs = plt.subplots(len(args.tasks)*len(args.goals), N_image_i, figsize=(fig_width, fig_width))\n",
    "for task in args.tasks:\n",
    "    for goal in args.goals:\n",
    "        task_goal_folder = os.path.join(paths[task][folder], goal)\n",
    "        image_plot_paths = os.listdir(task_goal_folder)\n",
    "        for i_image in np.arange(N_image_i):\n",
    "            ax = axs[x][i_image]\n",
    "            path = os.path.join(task_goal_folder, random.choice(image_plot_paths))\n",
    "            ax.imshow(imageio.imread(path))\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])  \n",
    "            if i_image%5 == 0:\n",
    "                ax.set_ylabel(task + ' ' + goal, font=font)\n",
    "        x +=1\n",
    "fig.set_facecolor(color='white')\n",
    "#-------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the notebook focus on the training process of the network. \n",
    "\n",
    "The following script is a classic Training script with pyTorch. Since I only have one synset to discriminate in the scene I use `criterion = nn.BCEWithLogitsLoss()` to compute the loss during the training process. For further statistical analyses, we extract factors (like the accuracy and loss) in a `pandas`' `DataFrame` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:40.151680Z",
     "iopub.status.busy": "2023-04-26T16:40:40.151427Z",
     "iopub.status.idle": "2023-04-26T16:40:40.162087Z",
     "shell.execute_reply": "2023-04-26T16:40:40.161124Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, num_epochs, dataloaders, lr=args.lr, momentum=args.momentum, batch_size=args.batch_size):\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    # https://towardsdatascience.com/cross-entropy-for-classification-d98e7f974451\n",
    "    # https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#bcewithlogitsloss\n",
    "    criterion = nn.BCEWithLogitsLoss() #binary_cross_entropy_with_logits\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    \n",
    "    df_train = pd.DataFrame([], columns=['epoch', 'avg_loss', 'avg_acc', 'avg_loss_val', 'avg_acc_val']) \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        loss_train = 0\n",
    "        acc_train = 0\n",
    "        num_train = 0\n",
    "        \n",
    "        if epoch == 15 :\n",
    "            p=1\n",
    "        elif epoch == 5 :\n",
    "            p=.5\n",
    "        else:\n",
    "            p=0\n",
    "        (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(paths=paths, image_size=args.image_size, p=p, **new_kwarg)\n",
    "            \n",
    "        for i, (images, labels) in enumerate(dataloaders['animal']['train']):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(images).squeeze(1)\n",
    "            loss = criterion(outputs, labels.float())            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item() * images.size(0)\n",
    "            \n",
    "            preds = (outputs > 0).long()\n",
    "            acc_train += (preds == labels).sum().item() # count correct predictions\n",
    "            num_train += len(labels)\n",
    "            \n",
    "        avg_loss = loss_train / num_train\n",
    "        avg_acc = acc_train / num_train\n",
    "           \n",
    "        with torch.no_grad():\n",
    "            loss_val = 0\n",
    "            acc_val = 0\n",
    "            num_val = 0\n",
    "            \n",
    "            for i, (images, labels) in enumerate(dataloaders['animal']['val']):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(images).squeeze(1)\n",
    "\n",
    "                loss = criterion(outputs, labels.float())\n",
    "                loss_val += loss.item() * images.size(0)\n",
    "\n",
    "                preds = (outputs > 0).long()\n",
    "                acc_val += (preds == labels).sum().item() # count correct predictions\n",
    "                num_val += len(labels)\n",
    "                \n",
    "            avg_loss_val = loss_val / num_val\n",
    "            avg_acc_val = acc_val / num_val\n",
    "        \n",
    "        df_train.loc[len(df_scan)] = {'epoch':epoch, 'avg_loss':avg_loss, 'avg_acc':avg_acc, 'avg_loss_val':avg_loss_val, 'avg_acc_val':avg_acc_val}\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} : train= loss: {avg_loss:.4f} / acc : {avg_acc:.4f} - val= loss : {avg_loss_val:.4f} / acc : {avg_acc_val:.4f}, p={p}\")\n",
    "    return model, df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Scan some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:40.164823Z",
     "iopub.status.busy": "2023-04-26T16:40:40.164486Z",
     "iopub.status.idle": "2023-04-26T16:40:41.657132Z",
     "shell.execute_reply": "2023-04-26T16:40:41.656407Z"
    }
   },
   "outputs": [],
   "source": [
    "scan_dicts = {'batch_size' : [5, 8, 13, 21, 34],\n",
    "             'lr': args.lr * np.logspace(-1, 1, 7, base=10),\n",
    "             'momentum': 1 - np.logspace(-5, -2, 7, base=10),\n",
    "            }\n",
    "\n",
    "N_val_avg=10\n",
    "for key in scan_dicts:\n",
    "    filename = f'{data_cache}/{datetag}_train_scan_{key}_{HOST}.json'\n",
    "    print(f'{filename=}')\n",
    "    if os.path.isfile(filename):\n",
    "        df_scan = pd.read_json(filename)\n",
    "    else:\n",
    "        measure_columns = [key, 'avg_loss_val', 'avg_acc_val', 'time']\n",
    "\n",
    "        df_scan = pd.DataFrame([], columns=measure_columns) \n",
    "        for i_trial, value in enumerate(scan_dicts[key]):\n",
    "            new_kwarg = {key: value}\n",
    "            print('trial', i_trial, ' /', len(scan_dicts[key]))\n",
    "            print('new_kwarg', new_kwarg)\n",
    "            # Training and saving the network\n",
    "            models_vgg_ = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.DEFAULT)\n",
    "            # Freeze training for all layers\n",
    "            # Newly created modules have requires_grad=True by default\n",
    "            for param in models_vgg_.features.parameters():\n",
    "                param.requires_grad = False \n",
    "\n",
    "            num_features = models_vgg_.classifier[-1].in_features\n",
    "            features = list(models_vgg_.classifier.children())[:-1] # Remove last layer\n",
    "            features.extend([nn.Linear(num_features, 1)]) # Add our layer with `n_output=1` outputs\n",
    "            models_vgg_.classifier = nn.Sequential(*features) # Replace the model classifier\n",
    "\n",
    "            since = time.time()\n",
    "            (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(paths=paths, image_size=args.image_size, p=0, **new_kwarg)\n",
    "            models_vgg_, df_train = train_model(models_vgg_, num_epochs=args.num_epochs, dataloaders=dataloaders['animal'], **new_kwarg)\n",
    "            elapsed_time = time.time() - since\n",
    "\n",
    "            print(f\"Training completed in {elapsed_time // 60:.0f}m {elapsed_time % 60:.0f}s\")\n",
    "            print(df_train)\n",
    "            df_scan.loc[len(df_scan)] = {key:value, 'avg_loss_val':df_train.iloc[-N_val_avg:-1]['avg_loss_val'].mean(), \n",
    "                                'avg_acc_val':df_train.iloc[-N_val_avg:-1]['avg_acc_val'].mean(), 'time':elapsed_time}\n",
    "        df_scan.to_json(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:41.660619Z",
     "iopub.status.busy": "2023-04-26T16:40:41.660349Z",
     "iopub.status.idle": "2023-04-26T16:40:41.814666Z",
     "shell.execute_reply": "2023-04-26T16:40:41.813880Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "subplotpars = matplotlib.figure.SubplotParams(left=0.1, right=.95, bottom=0.25, top=.975, hspace=.6)\n",
    "dfs_ = {}\n",
    "for key in scan_dicts:\n",
    "    filename = f'{data_cache}/{datetag}_train_scan_{key}_{HOST}.json'\n",
    "    dfs_[str(key)]  = pd.read_json(filename)\n",
    "\n",
    "fig, axs = plt.subplots(len(dfs_), 1, figsize=(fig_width, fig_width*len(dfs_)/(phi*2)), subplotpars=subplotpars)\n",
    "plt.tick_params(axis='both', which='major', labelsize=10)\n",
    "\n",
    "for ax, df_train, key in zip(axs, dfs_, scan_dicts):\n",
    "    ax.plot(scan_dicts[key], dfs_[df_train][\"avg_acc_val\"], alpha=0.5, lw=2, marker='.')\n",
    "    ax.set_ylabel(f\"Accuracy for {key}\", size=18)\n",
    "    ax.set_xlabel(f\"Parameter : {key}\", size=18)\n",
    "    if type(scan_dicts[key][0]) == np.float64: ax.xaxis.set_major_formatter(FormatStrFormatter('%.4f'))\n",
    "    ax.spines['left'].set_position(('axes', -0.01))\n",
    "    ax.set_yscale(\"logit\", one_half=\"1/2\", use_overline=True)\n",
    "    ax.grid(which='both')\n",
    "    for side in ['top', 'right'] :ax.spines[side].set_visible(False)\n",
    "axs[0].set_title(f'Average values of the accuracy for different parameters :', size=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:41.819313Z",
     "iopub.status.busy": "2023-04-26T16:40:41.819004Z",
     "iopub.status.idle": "2023-04-26T16:40:41.889323Z",
     "shell.execute_reply": "2023-04-26T16:40:41.888533Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfs_ = {}\n",
    "for key in scan_dicts:\n",
    "    filename = f'{data_cache}/{datetag}_train_scan_{key}_{HOST}.json'\n",
    "    dfs_[str(key)]  = pd.read_json(filename)\n",
    "fig, axs = plt.subplots(len(dfs_), 1, figsize=(fig_width, fig_width*len(dfs_)/(phi*2)), subplotpars=subplotpars)\n",
    "plt.tick_params(axis='both', which='major', labelsize=10)\n",
    "for ax, df_train, key in zip(axs, dfs_, scan_dicts):\n",
    "    ax.plot(scan_dicts[key], dfs_[df_train][\"avg_loss_val\"], alpha=0.5, lw=2, marker='.')\n",
    "    ax.set_ylabel(f\"Loss value for {key}\", size=18)    \n",
    "    ax.set_xlabel(f\"Parameter :{key}\", size=16)\n",
    "    if type(scan_dicts[key][0]) == np.float64: ax.xaxis.set_major_formatter(FormatStrFormatter('%.2e'))\n",
    "    ax.grid(which='both')\n",
    "    for side in ['top', 'right'] :ax.spines[side].set_visible(False)\n",
    "axs[0].set_title(f'Average values of the loss for different parameters :', size=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning with fine tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:41.894107Z",
     "iopub.status.busy": "2023-04-26T16:40:41.893907Z",
     "iopub.status.idle": "2023-04-26T16:40:44.812185Z",
     "shell.execute_reply": "2023-04-26T16:40:44.811464Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training and saving the networks\n",
    "models_vgg = {}\n",
    "opt = {}\n",
    "\n",
    "\n",
    "# updating dictionary of studied models with the pretrained VGG16 and freezing all layers\n",
    "models_vgg['vgg_lut'] = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.DEFAULT)\n",
    "for param in models_vgg['vgg_lut'].features.parameters():\n",
    "    param.requires_grad = False \n",
    "\n",
    "# now loading pretrained VGG16 and retrain them\n",
    "for task in args.tasks :\n",
    "    for model in args.model_names:\n",
    "        model_name = f'{model}_{task}'\n",
    "\n",
    "        filename = f'{data_cache}/{datetag}_{HOST}_train_{model_name}.json'\n",
    "        model_filename = filename.replace('json', 'pt')\n",
    "        print(filename)\n",
    "        if 'sls' in model_name:\n",
    "            models_vgg[model_name] = torchvision.models.vgg16(weights=None)\n",
    "        else:\n",
    "            models_vgg[model_name] = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.DEFAULT)\n",
    "        \n",
    "        if 'tla' in model_name:\n",
    "            for param in models_vgg[model_name].features.parameters():\n",
    "                param.requires_grad = True\n",
    "        else: # model = 'tlc' or 'tlda'\n",
    "            for param in models_vgg[model_name].features.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        if 'tlaa' in model_name:\n",
    "            p = 1 \n",
    "        elif 'tlda' in model_name:\n",
    "            p = 0.5\n",
    "        else:\n",
    "            p = 0\n",
    "            \n",
    "        num_features = models_vgg[model_name].classifier[-1].in_features\n",
    "        features = list(models_vgg[model_name].classifier.children())[:-1] # Remove last layer\n",
    "        features.extend([nn.Linear(num_features, 1)]) # Add our layer with `n_output=1` outputs\n",
    "        models_vgg[model_name].classifier = nn.Sequential(*features) # Replace the model classifier\n",
    "        \n",
    "        if os.path.isfile(filename):\n",
    "            models_vgg[model_name].load_state_dict(torch.load(model_filename, map_location=torch.device(device)))\n",
    "        else:\n",
    "            print(\"Re-training pretrained model...\", model_filename)\n",
    "            print(f\"Traning {model_name}, image_size={args.image_size}\")\n",
    "            since = time.time()\n",
    "            (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(paths=paths, \n",
    "                                                                                                image_size=args.image_size, \n",
    "                                                                                                batch_size=args.batch_size, \n",
    "                                                                                                p=p)\n",
    "            models_vgg[model_name], df_train = train_model(models_vgg[model_name], num_epochs=args.num_epochs,\n",
    "                                                           dataloaders=dataloaders[task], lr=args.lr, \n",
    "                                                           momentum=args.momentum)\n",
    "            elapsed_time = time.time() - since\n",
    "            print(f\"Training completed in {elapsed_time // 60:.0f}m {elapsed_time % 60:.0f}s\")\n",
    "            print(f\"Saving...{model_filename}\")\n",
    "            torch.save(models_vgg[model_name].state_dict(), model_filename)\n",
    "            df_train.to_json(filename)\n",
    "            print()          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:44.814937Z",
     "iopub.status.busy": "2023-04-26T16:40:44.814739Z",
     "iopub.status.idle": "2023-04-26T16:40:44.821004Z",
     "shell.execute_reply": "2023-04-26T16:40:44.820310Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model_name in models_vgg.keys():\n",
    "    filename = f'{data_cache}/{datetag}_{HOST}_train_{model_name}.json'\n",
    "    if os.path.isfile(filename):\n",
    "        df_train = pd.read_json(filename)\n",
    "        fig, axs = plt.subplots(figsize=(fig_width, fig_width/phi/2))\n",
    "        ax = df_train['avg_loss'].plot(lw=2, marker='.', markersize=10)\n",
    "        ax = df_train['avg_loss_val'].plot(lw=2, marker='.', markersize=10)\n",
    "        ax.legend([\"avg_loss\", \"avg_loss_val\"]);\n",
    "        ax.set_xlabel(\"Epoch\", size=18)\n",
    "        ax.spines['left'].set_position(('axes', -0.01))\n",
    "        ax.set_xlim(-0.5, args.num_epochs)\n",
    "        ax.grid(which='both')\n",
    "        for side in ['top', 'right'] :ax.spines[side].set_visible(False)\n",
    "        # ax.set_ylim(0., 1.1)\n",
    "        axs.set_title(f'Average values of the loss by epoch : {filename}')\n",
    "        ax.get_legend().remove()\n",
    "        fig.legend(bbox_to_anchor=(1.05, .5), loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:44.823744Z",
     "iopub.status.busy": "2023-04-26T16:40:44.823549Z",
     "iopub.status.idle": "2023-04-26T16:40:44.828758Z",
     "shell.execute_reply": "2023-04-26T16:40:44.828255Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model_name in models_vgg.keys():\n",
    "    filename = f'{data_cache}/{datetag}_{HOST}_train_{model_name}.json'\n",
    "    if os.path.isfile(filename):\n",
    "        df_train = pd.read_json(filename)\n",
    "        fig, axs = plt.subplots(figsize=(fig_width, fig_width/phi/2))\n",
    "        ax = df_train['avg_acc'].plot(lw=2, marker='.', markersize=10)\n",
    "        ax = df_train['avg_acc_val'].plot(lw=2, marker='.', markersize=10)\n",
    "        ax.set_xlabel(\"Epoch\", size=18)\n",
    "        ax.spines['left'].set_position(('axes', -0.01))\n",
    "        # ax.set_ylim(0.70, .998)\n",
    "        ax.set_yscale(\"logit\", one_half=\"1/2\", use_overline=True)\n",
    "        ax.grid(which='both')\n",
    "        ax.set_xlim(-0.5, args.num_epochs+.5)\n",
    "        for side in ['top', 'right'] :ax.spines[side].set_visible(False)\n",
    "        axs.set_title(f'Average values of the accuracy by epoch : {filename}', size=20)\n",
    "        ax.legend([\"avg_acc\", \"avg_acc_val\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:44.831505Z",
     "iopub.status.busy": "2023-04-26T16:40:44.831269Z",
     "iopub.status.idle": "2023-04-26T16:40:49.204537Z",
     "shell.execute_reply": "2023-04-26T16:40:49.203825Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Downloading the model\n",
    "task = 'animal'\n",
    "models_vgg_pruned = {}\n",
    "m = 0\n",
    "input_lin_1 = 12544\n",
    "pruned_depths = [28, 26, 23, 20, 18, 15, 13, 11, 8, 6, 3, 2]\n",
    "pruned_model_names = ['vgg-1', 'vgg-2','vgg-3','vgg-4', 'vgg-5','vgg-6','vgg-7','vgg-8','vgg-9','vgg-10', 'vgg-11', 'vgg-12']\n",
    "models_vgg_pruned['VGG LUT'] = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.DEFAULT)\n",
    "models_vgg_pruned['VGG TLAA'] = models_vgg['vgg16_tlaa_animal']\n",
    "\n",
    "display_model_name = pd.DataFrame({'' : ['VGG LUT', 'VGG TLAA','vgg-1', 'vgg-2','vgg-3','vgg-4', 'vgg-5','vgg-6','vgg-7','vgg-8','vgg-9','vgg-10', 'vgg-11', 'vgg-12']}) # Hack to read our results, else coment\n",
    "\n",
    "for model_name, pruned_depth in zip(pruned_model_names, pruned_depths):\n",
    "    vgg_head = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.DEFAULT)\n",
    "    goal =  list(vgg_head.features.children())\n",
    "    filename = f'{data_cache}/{datetag}_{HOST}_train_{model_name}.json'\n",
    "    model_filename = filename.replace('json', 'pt')\n",
    "    print(filename)\n",
    "\n",
    "\n",
    "    models_vgg_pruned[model_name] = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.DEFAULT)\n",
    "    conv_part = goal[:pruned_depth]\n",
    "    conv_part.append(goal[-1])\n",
    "    models_vgg_pruned[model_name].features = torch.nn.Sequential(*conv_part) #get the new features parameters in the model\n",
    "\n",
    "    num_features = models_vgg_pruned[model_name].classifier[6].in_features\n",
    "    features = list(models_vgg_pruned[model_name].classifier.children())[:-1] # Remove last layer  \n",
    "    \n",
    "    if model_name in ['vgg-6', 'vgg-7', 'vgg-8']:   \n",
    "        del features[0] #remove first layer\n",
    "        features.insert(0, nn.Linear(input_lin_1, num_features)) # Add your custom input layer\n",
    "        features.extend([nn.Linear(num_features, 1)]) # Add our layer with 1 output\n",
    "        models_vgg_pruned[model_name].classifier = nn.Sequential(*features) # Replace the model classifier   \n",
    "        m += 1\n",
    "        \n",
    "    elif model_name in ['vgg-9', 'vgg-10']:\n",
    "        del features[0] #remove first layer\n",
    "        features.insert(0, nn.Linear(input_lin_1//2, num_features)) # Add your custom input layer\n",
    "        features.extend([nn.Linear(num_features, 1)]) # Add our layer with 1 output\n",
    "        models_vgg_pruned[model_name].classifier = nn.Sequential(*features) # Replace the model classifier   \n",
    "        m += 1\n",
    "        \n",
    "    elif model_name in ['vgg-11', 'vgg-12']:\n",
    "        del features[0] #remove first layer\n",
    "        features.insert(0, nn.Linear(input_lin_1//4, num_features)) # Add your custom input layer\n",
    "        features.extend([nn.Linear(num_features, 1)]) # Add our layer with 1 output\n",
    "        models_vgg_pruned[model_name].classifier = nn.Sequential(*features) # Replace the model classifier   \n",
    "        m += 1\n",
    "\n",
    "    else : \n",
    "        features.extend([nn.Linear(num_features, 1)]) # Add our layer with n outputs\n",
    "        models_vgg_pruned[model_name].classifier = nn.Sequential(*features) # Replace the model classifier     \n",
    "        m += 1\n",
    "\n",
    "    if os.path.isfile(model_filename):\n",
    "        print(\"Loading pretrained model for..\", model_name, 'from', model_filename)\n",
    "        models_vgg_pruned[model_name].load_state_dict(torch.load(model_filename, map_location=torch.device(device)))\n",
    "    else :\n",
    "        filename = f'{data_cache}/{datetag}_{HOST}_train_{model_name}.json'\n",
    "        print(\"Re-training pretrained model...\", model_filename)\n",
    "        since = time.time()\n",
    "        print(f\"Traning {model_name}, image_size={args.image_size}\")\n",
    "        (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(image_size=args.image_size, p=1)\n",
    "        models_vgg_pruned[model_name], df_train = train_model(models_vgg_pruned[model_name], num_epochs=args.num_epochs,\n",
    "                                                    dataloaders=dataloaders[task])\n",
    "        torch.save(models_vgg_pruned[model_name].state_dict(), model_filename)\n",
    "        df_train.to_json(filename)\n",
    "        elapsed_time = time.time() - since\n",
    "        print(f\"Training completed in {elapsed_time // 60:.0f}m {elapsed_time % 60:.0f}s\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performances on natural scenes containing animals without transfer learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:49.208548Z",
     "iopub.status.busy": "2023-04-26T16:40:49.208349Z",
     "iopub.status.idle": "2023-04-26T16:40:49.240870Z",
     "shell.execute_reply": "2023-04-26T16:40:49.240293Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = f'{data_cache}/{datetag}_{HOST}_results_rotate_imagenet.json'\n",
    "#filename = f'{data_cache}/{args.datetag}_results_rotate_imagenet.json'\n",
    "print(f'{filename=}')\n",
    "angles_imagenet = np.arange(-180, 181, 1)\n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    df_angle = pd.read_json(filename)\n",
    "else:\n",
    "    df_angle = pd.DataFrame([], columns=['mean_prediction', 'model', 'var', 'effect'])\n",
    "    # image preprocessing\n",
    "    (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(paths=paths, image_size=args.image_size, batch_size=25)\n",
    "\n",
    "    for task in tasks:\n",
    "        print(task)\n",
    "        for effect in ['basic', 'rot']:\n",
    "            model_name = 'vgg_lut'\n",
    "            model = models_vgg[model_name].to(device)\n",
    "            acc_= 0\n",
    "            for i_image, (data, label) in enumerate(dataloaders[task]['test']):\n",
    "                if effect == 'rot':\n",
    "                    data = transforms.functional.rotate(data, angle=float(random.choice(angles_imagenet)), expand=True)\n",
    "                    data = transforms.functional.resize(data, size=(args.image_size, args.image_size), antialias=True)\n",
    "                data, label, = data.to(device), label.to(device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(data)\n",
    "                    percentage = torch.nn.functional.softmax(outputs, dim=1)\n",
    "                    preds = torch.sum(percentage[:, match[task]], dim=1)\n",
    "                    acc_ += torch.sum(torch.round(preds) == label.data)\n",
    "            avg_acc = acc_ / dataset_sizes[task]['test']\n",
    "            df_angle.loc[len(df_angle)] = {'model':model_name, 'mean_prediction':float(avg_acc), 'var':task, 'effect':effect}  \n",
    "            print(model_name, float(avg_acc), task, effect)\n",
    "        df_angle.to_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:49.243154Z",
     "iopub.status.busy": "2023-04-26T16:40:49.242951Z",
     "iopub.status.idle": "2023-04-26T16:40:49.246323Z",
     "shell.execute_reply": "2023-04-26T16:40:49.245773Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = f'{data_cache}/{datetag}_{HOST}_results_rotate_imagenet.json'\n",
    "if os.path.isfile(filename):\n",
    "    df_rotate_imagenet = pd.read_json(filename)\n",
    "    df_rotate_imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:49.250866Z",
     "iopub.status.busy": "2023-04-26T16:40:49.250580Z",
     "iopub.status.idle": "2023-04-26T16:40:49.260341Z",
     "shell.execute_reply": "2023-04-26T16:40:49.259772Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(filename):\n",
    "\n",
    "    acc_dict = df_rotate_imagenet.to_dict()\n",
    "    data_rand = { 'x_abs': ['Random', 'Random + Rotation'],\n",
    "            'y_val' : [acc_dict['mean_prediction'][4], acc_dict['mean_prediction'][5]]}\n",
    "    data_ani = { 'x_abs': ['Animal', 'Animal + Rotation'],\n",
    "            'y_val' : [acc_dict['mean_prediction'][0], acc_dict['mean_prediction'][1]]}\n",
    "    data_art = { 'x_abs': ['Artifact', 'Artifact + Rotation'],\n",
    "            'y_val' : [acc_dict['mean_prediction'][2], acc_dict['mean_prediction'][3]]}\n",
    "\n",
    "    df_rand = pd.DataFrame(data_rand)\n",
    "    df_ani = pd.DataFrame(data_ani)\n",
    "    df_art = pd.DataFrame(data_art)\n",
    "\n",
    "    x= ['Vgg Imagenet', 'Vgg Animal', 'Vgg Artifact']\n",
    "    fig, axs = plt.subplots(1, 3, sharex=False, sharey=True, figsize=(fig_width, fig_width//4))\n",
    "\n",
    "    titles = [\"(A) Animal dataset\", \"(B) Artifact dataset\" ,\"(C) Random dataset\" ]\n",
    "\n",
    "    axs[2].bar(df_rand['x_abs'], df_rand['y_val'],  align='center', color=tasks_color['artifact'])\n",
    "    axs[0].bar(df_ani['x_abs'], df_ani['y_val'], color=tasks_color['animal'])\n",
    "    axs[1].bar(df_art['x_abs'], df_art['y_val'], color=tasks_color['random'])\n",
    "    axs[0].hlines(xmin=-.5, xmax=1.5, y=1/2, ls='--', ec='gray')\n",
    "    axs[1].hlines(xmin=-.5, xmax=1.5, y=1/2, ls='--', ec='gray')\n",
    "    axs[2].hlines(xmin=-.5, xmax=1.5, y=1/2, ls='--', ec='gray')\n",
    "    axs[1].set_xticklabels(labels=['Artifact', 'Artifact + Rotation'], font=font)\n",
    "    axs[1].set_yticklabels(labels=[0, .25, .5, .75, 1], font=font)\n",
    "    axs[0].set_xticklabels(labels=['Animal', 'Animal + Rotation'], font=font)\n",
    "    axs[0].set_ylabel('Accuracy', font=font)\n",
    "    axs[0].set_yticklabels(labels=[0, .25, .5, .75, 1], font=font)\n",
    "\n",
    "    # https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
    "    for j, ax in enumerate(axs):\n",
    "            ax.set_title(titles[j], font=font)\n",
    "            ax.tick_params(axis='both', labelsize=15)    \n",
    "            for i, container in enumerate(ax.containers):\n",
    "                    ax.bar_label(container, padding=-15, color='black', fmt='%.3f', rotation=0, label_type = 'edge', fontsize=fontsize, weight='bold')\n",
    "\n",
    "    plt.xticks(font=font)\n",
    "    plt.yticks(font=font)\n",
    "    plt.tight_layout()\n",
    "    if do_savefig: fig.savefig(os.path.join(figpath, 'robustness_imagenet.pdf'), **opts_savefig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exp1\"></a>\n",
    "## Performances on natural scenes containing animal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the networks seems ready for the comparison. This part of this notebook offers a comparison between:\n",
    "\n",
    "- A pre-trained image recognition's networks, here VGG, trained on the [ImageNet](http://image-net.org/) dataset which allows working on naturals images for $1000$ labels, taken from the `torchvision.models` library\n",
    "\n",
    "- And three re-trained version of the same network_ VGG16 based on a Wordnet semantic from the ImageNet database which allows working on naturals images for $1$ synsets. \n",
    "\n",
    "As a control I re-train networks on the artifact synset too. Another interesting analysis will be the study of the link between the performances on these two conditions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Image processing and recognition for different animal label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:49.262825Z",
     "iopub.status.busy": "2023-04-26T16:40:49.262550Z",
     "iopub.status.idle": "2023-04-26T16:40:49.417471Z",
     "shell.execute_reply": "2023-04-26T16:40:49.416753Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = f'{data_cache}/{datetag}_{HOST}_results_basic.json'\n",
    "print(f'{filename=}')\n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    df = pd.read_json(filename)\n",
    "else:\n",
    "    df = pd.DataFrame([], columns=['model', 'model_task', 'task', 'goal', 'likelihood', 'fps', 'time', 'i_image', 'filename', 'top_1']) \n",
    "        # image preprocessing\n",
    "    (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(paths=paths, image_size=args.image_size, batch_size=1)\n",
    "    for task in args.tasks:\n",
    "        pprint(task)\n",
    "        for i_image, (data, label) in enumerate(dataloaders[task]['test']):\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            for model_name in models_vgg.keys():\n",
    "                model = models_vgg[model_name].to(device)\n",
    "                with torch.no_grad():\n",
    "                    goal = 'target' if 'target' in image_datasets[task]['test'].imgs[i_image][0] else 'distractor'\n",
    "                    model_task = 'animal' if 'animal' in model_name else 'artifact'\n",
    "                    tic = time.time()\n",
    "                    out = model(data).cpu().squeeze(0)\n",
    "                    if model_name == 'vgg_lut':\n",
    "                        model_task = 'Imagenet_challenge'\n",
    "                        percentage = torch.nn.functional.softmax(out, dim=0)\n",
    "                        likelihood = np.asarray(percentage[match[task]]).sum()\n",
    "                    else:\n",
    "                        likelihood = torch.sigmoid(out).numpy()[0]\n",
    "                    top_1 = 'target' if likelihood>0.50 else 'distractor'\n",
    "                    elapsed_time = time.time() - tic\n",
    "                df.loc[len(df)] = { 'model':model_name,'model_task':model_task, 'task':task, 'top_1':top_1, 'goal':goal, \n",
    "                                    'likelihood':likelihood, 'time':elapsed_time, 'fps': 1/elapsed_time,\n",
    "                                    'i_image':i_image, 'filename':image_datasets[task]['test'].imgs[i_image][0]}\n",
    "                print(f'The {model_name} model categorize {model_task} with {likelihood*100:.3f} % likelihood ({top_1}) in {elapsed_time:.3f} seconds, groundtruth : {task}, {goal}')\n",
    "    df.to_json(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:49.421633Z",
     "iopub.status.busy": "2023-04-26T16:40:49.421432Z",
     "iopub.status.idle": "2023-04-26T16:40:49.483008Z",
     "shell.execute_reply": "2023-04-26T16:40:49.482344Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = f'{data_cache}/{datetag}_{HOST}_results_basic.json'\n",
    "df = pd.read_json(filename)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computation Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:49.486346Z",
     "iopub.status.busy": "2023-04-26T16:40:49.486083Z",
     "iopub.status.idle": "2023-04-26T16:40:49.793072Z",
     "shell.execute_reply": "2023-04-26T16:40:49.790656Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(df['model'].unique()), 1, figsize=(fig_width, fig_width*phi), sharex=True, sharey=True)\n",
    "cmap = plt.cm.get_cmap('viridis')\n",
    "color_dict = pd.Series({k:cmap(i/len(df['model'].unique())) for i,k in enumerate(df['model'].unique())})\n",
    "for ax, color, model_name in zip(axs, color_dict, models_vgg):\n",
    "    df[df['model']==model_name]['time'].plot.hist(bins=50, lw=1, label=model_name,ax=ax, color=[color_dict[model_name],], density=True)\n",
    "    ax.set_xlim(df['time'].quantile(.01), df['time'].quantile(.99))\n",
    "    ax.set_ylim(0, 600)\n",
    "    ax.legend(bbox_to_anchor=(1.31, .35), loc='lower right', fontsize=fontsize)\n",
    "    ax.grid(which='both', axis='y')\n",
    "    ax.set_ylabel('')\n",
    "axs[0].set_title('Distribution of the Processing time (s)', size=20)\n",
    "axs[len(df['model'].unique())//2].set_ylabel('Frequency', fontsize=fontsize);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy comparison for all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:49.796622Z",
     "iopub.status.busy": "2023-04-26T16:40:49.796161Z",
     "iopub.status.idle": "2023-04-26T16:40:50.005831Z",
     "shell.execute_reply": "2023-04-26T16:40:50.005050Z"
    }
   },
   "outputs": [],
   "source": [
    "df_acc = pd.DataFrame({model_name: {task: accuracy_score(df[(df['model']==model_name) & (df['task']==task)][\"top_1\"], \n",
    "                                                             df[(df['model']==model_name) & (df['task']==task)][\"goal\"])\n",
    "                                    for task in args.tasks} \n",
    "                       for model_name in df['model'].unique()})\n",
    "\n",
    "ax = df_acc.T.plot.bar(rot=60, figsize=(fig_width*phi, fig_width//3), fontsize=fontsize)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.hlines(xmin=-.5, xmax=len(df['model'].unique())-.5, y=1/2, ls='--', ec='k', label='chance level')\n",
    "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
    "for container in ax.containers: ax.bar_label(container, padding=-36, color='black', fontsize=fontsize, fmt='%.2f', rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1.13, .25), loc='lower right', fontsize=fontsize)\n",
    "ax.set_title(f'Animal vs Artifact images f1 score for re-trained models', size=22)\n",
    "ax.set_ylabel('Accuracy', size=20)\n",
    "ax.set_xlabel('Model', size=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A comparison between performances in biology and in models: Test on Serre (2007)'s dataset\n",
    "\n",
    " As a control, we tested the networks on the dataset of [Serre & al](http://vision.stanford.edu/teaching/cs131_fall1415/lectures/Serre2007.pdf). This dataset contains a total of $600$ targets (images containing an animal) and $600$ distractors (images not containing an animal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:50.010728Z",
     "iopub.status.busy": "2023-04-26T16:40:50.010524Z",
     "iopub.status.idle": "2023-04-26T16:40:50.038726Z",
     "shell.execute_reply": "2023-04-26T16:40:50.038224Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = f'{data_cache}/{datetag}_{HOST}_results_serre.json'\n",
    "print(f'{filename=}')\n",
    "\n",
    "(dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(paths=paths, image_size=args.image_size, batch_size=1)\n",
    "image_dataset_serre07 = datasets.ImageFolder('../data/Serre_2007/', transform=data_transforms['test'])\n",
    "dataloaders_serre07 = torch.utils.data.DataLoader(image_dataset_serre07, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "# Output's set up\n",
    "if os.path.isfile(filename):\n",
    "    df_serre = pd.read_json(filename)\n",
    "else:\n",
    "    df_serre = pd.DataFrame([], columns=['model', 'model_task', 'goal', 'likelihood', 'fps', 'time', 'i_image', 'filename', 'top_1']) \n",
    "    for i_image, (data, label) in enumerate(dataloaders_serre07):\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        for model_name in models_vgg.keys():\n",
    "            model = models_vgg[model_name].to(device)\n",
    "            with torch.no_grad():\n",
    "                goal = 'target' if 'target' in image_dataset_serre07.imgs[i_image][0] else 'distractor'\n",
    "                model_task = 'animal' if 'animal' in model_name else 'artifact'\n",
    "                tic = time.time()\n",
    "                out = model(data).squeeze(0).cpu()\n",
    "                if model_name == 'vgg_lut':\n",
    "                    model_task = 'Imagenet_challenge'\n",
    "                    percentage = torch.nn.functional.softmax(out, dim=0)\n",
    "                    likelihood = np.asarray(percentage[match['animal']]).sum()\n",
    "                else:\n",
    "                    likelihood = torch.sigmoid(out).numpy()[0]\n",
    "\n",
    "                top_1 = 'target' if likelihood>0.50 else 'distractor'\n",
    "                elapsed_time = time.time() - tic\n",
    "            df_serre.loc[len(df_serre)] = {'model':model_name, 'model_task':model_task, 'likelihood':likelihood, 'time':elapsed_time, \n",
    "                                        'fps': 1/elapsed_time, 'goal': goal, 'i_image':i_image, 'top_1':top_1,\n",
    "                                        'filename':image_dataset_serre07.imgs[i_image][0]}\n",
    "            print(f'The {model_name} model categorize {model_task} with {likelihood*100:.3f} % likelihood ({top_1}) in {elapsed_time:.3f} seconds, groundtruth : animal, {goal}')\n",
    "    df_serre.to_json(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:50.044311Z",
     "iopub.status.busy": "2023-04-26T16:40:50.043902Z",
     "iopub.status.idle": "2023-04-26T16:40:50.102687Z",
     "shell.execute_reply": "2023-04-26T16:40:50.101892Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = f'{data_cache}/{datetag}_{HOST}_results_serre.json'\n",
    "df_serre = pd.read_json(filename)\n",
    "df_serre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Image display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the images with the highest likelihood of the networks :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:50.107440Z",
     "iopub.status.busy": "2023-04-26T16:40:50.107152Z",
     "iopub.status.idle": "2023-04-26T16:40:52.154504Z",
     "shell.execute_reply": "2023-04-26T16:40:52.153889Z"
    }
   },
   "outputs": [],
   "source": [
    "N_image_i = 4\n",
    "N_image_j = 4\n",
    "fig, axs = plt.subplots(N_image_i, N_image_j, figsize=(fig_width*1.3, fig_width))\n",
    "for idx, i_image in enumerate(df_serre.sort_values(by=['likelihood'], ascending=False).head(N_image_i*N_image_j).index):\n",
    "    ax = axs[idx%N_image_i][idx//N_image_i]\n",
    "    img_address = image_dataset_serre07.imgs[df_serre.loc[idx]['i_image']][0]\n",
    "    ax.imshow(imageio.imread(img_address))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    color = 'g' if df_serre.loc[idx]['top_1'] == df_serre.loc[idx]['goal'] else 'r'\n",
    "    ax.set_xlabel(df_serre.loc[idx]['top_1'] + ' | ' + df_serre.loc[idx]['model'], color=color)\n",
    "    likelihood = df_serre.loc[idx]['likelihood']\n",
    "    ax.set_ylabel(f'P ={likelihood * 100:2.3f}%', color=color)\n",
    "fig.set_facecolor(color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:52.159035Z",
     "iopub.status.busy": "2023-04-26T16:40:52.158830Z",
     "iopub.status.idle": "2023-04-26T16:40:54.026271Z",
     "shell.execute_reply": "2023-04-26T16:40:54.025547Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(N_image_i, N_image_j, figsize=(fig_width*1.3, fig_width))\n",
    "for i_image, idx in enumerate(df_serre.sort_values(by=['likelihood'], ascending=True).head(N_image_i*N_image_j).index):\n",
    "    ax = axs[i_image%N_image_i][i_image//N_image_i]\n",
    "    img_address = image_dataset_serre07.imgs[df_serre.loc[idx]['i_image']][0]\n",
    "    ax.imshow(imageio.imread(img_address))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    color = 'g' if df_serre.loc[idx]['top_1'] == df_serre.loc[idx]['goal'] else 'r'\n",
    "    ax.set_xlabel(df_serre.loc[idx]['top_1'] + ' | ' + df_serre.loc[idx]['model'], color=color)\n",
    "    likelihood = df_serre.loc[idx]['likelihood']\n",
    "    ax.set_ylabel(f'P ={likelihood:2.3f}%', color=color)\n",
    "fig.set_facecolor(color='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:54.031042Z",
     "iopub.status.busy": "2023-04-26T16:40:54.030832Z",
     "iopub.status.idle": "2023-04-26T16:40:54.055502Z",
     "shell.execute_reply": "2023-04-26T16:40:54.054924Z"
    }
   },
   "outputs": [],
   "source": [
    "df_acc_serre = pd.DataFrame({'accuracy': [accuracy_score(df_serre[df_serre['model']==model_name][\"top_1\"], df_serre[df_serre['model']==model_name][\"goal\"]) for model_name in df_serre['model'].unique()]}, index=df_serre['model'].unique())\n",
    "ax = df_acc_serre.plot.bar(rot=30, figsize=(fig_width*phi, fig_width//3), fontsize=fontsize)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.hlines(xmin=-.5, xmax=len(df_serre['model'].unique())-.5, y=1/2, ls='--', ec='k', label='chance level')\n",
    "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
    "ax.bar_label(ax.containers[0], padding=-24, color='black', fontsize=fontsize, fmt='%.3f')\n",
    "plt.legend(bbox_to_anchor=(1.1, .5), loc='lower right')\n",
    "ax.grid(which='both', axis='y')\n",
    "for side in ['top', 'right'] :ax.spines[side].set_visible(False)\n",
    "ax.set_title('Average accuracy top_1 : for each models - experiment 1', size=20)\n",
    "ax.set_xlabel('Model', size=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:54.059903Z",
     "iopub.status.busy": "2023-04-26T16:40:54.059637Z",
     "iopub.status.idle": "2023-04-26T16:40:54.224193Z",
     "shell.execute_reply": "2023-04-26T16:40:54.223452Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "df_mean = pd.DataFrame([], columns=['heuristic', 'var', 'goal'])\n",
    "color_dict = pd.Series({k:cmap(i/len(df_serre['goal'].unique())) for i,k in enumerate(df_serre['goal'].unique())})\n",
    "model_animal = []\n",
    "model_artifact = []\n",
    "data_animal = df_serre.loc[df_serre['model'] == 'vgg16_tlc_animal']\n",
    "data_artifact = df_serre.loc[df_serre['model'] == 'vgg16_tlc_artifact']\n",
    "for i_image in df_serre['i_image'].unique():\n",
    "    heuristic = 1 - (1 - float(data_animal[(data_animal['i_image']==i_image)][\"likelihood\"])) * float(data_artifact[(data_artifact['i_image']==i_image)][\"likelihood\"])\n",
    "    df_mean.loc[i_image] = {'heuristic':heuristic, 'var':i_image, 'goal':list(data_animal.goal)[i_image]}      \n",
    "for goal in df_serre['goal'].unique():\n",
    "    df_mean[(df_mean['goal']==goal)].plot.scatter(x=\"var\", y=\"heuristic\",  c=[color_dict[goal],],\n",
    "                                                         alpha=0.5, marker='o', lw=0, ax=ax, label=goal)\n",
    "ax.tick_params(axis='x', labelsize=14)\n",
    "ax.tick_params(axis='y', labelsize=14)\n",
    "ax.set_ylabel('Heuristic', size= 20)\n",
    "ax.set_xlabel('i image', size= 20)\n",
    "plt.legend(bbox_to_anchor=(1.1, .5), loc='center')\n",
    "ax.set_title('Heuristic : likelihood = 1 - (1 - likelihood_animal)  * likelihood_artifact', size=20);\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Comparison with psychophysical data from Serre & al 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:54.227598Z",
     "iopub.status.busy": "2023-04-26T16:40:54.227399Z",
     "iopub.status.idle": "2023-04-26T16:40:54.580056Z",
     "shell.execute_reply": "2023-04-26T16:40:54.578787Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_serre_07 = pd.read_csv('https://raw.githubusercontent.com/laurentperrinet/PerrinetBednar15/master/database/AnimalnessIndex.csv', header=None)#, index_col=['A', 'B', 'C'])\n",
    "\n",
    "likes_ = []\n",
    "goals = []\n",
    "top1_human = []\n",
    "top1_model = []\n",
    "data_vgg_gen = df_serre.loc[df_serre['model'] == 'vgg16_tlc_animal']\n",
    "for i in range(len(df_serre_07)):\n",
    "    top_1_human = 'target' if df_serre_07.iloc[i, 1]>50 else 'distractor'\n",
    "    top_1_model = 'target' if df_serre_07.iloc[i, 2]>50 else 'distractor'\n",
    "    top1_human.append(top_1_human)\n",
    "    top1_model.append(top_1_model)\n",
    "    for j in range(len(data_vgg_gen)):\n",
    "        if df_serre_07.iloc[i, 0] in data_vgg_gen.iloc[j, 7] :\n",
    "            goals.append(data_vgg_gen.iloc[j, 2])\n",
    "            if data_vgg_gen.iloc[j, 2] == 'target':\n",
    "                likes_.append(round(data_vgg_gen.iloc[j, 3]))\n",
    "            else:\n",
    "                likes_.append(100-round(data_vgg_gen.iloc[j, 3] * 100))\n",
    "                df_serre_07.iloc[i, 1] = 100 - df_serre_07.iloc[i, 1]\n",
    "                df_serre_07.iloc[i, 2] = 100 - df_serre_07.iloc[i, 2]\n",
    "\n",
    "df_serre_07.insert( 3, 'human', top1_human)\n",
    "df_serre_07.insert( 4, 'model', top1_model)\n",
    "df_serre_07.insert(5, 'TLC', likes_)\n",
    "df_serre_07.insert(6, 'goals', goals)\n",
    "df_serre_07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:54.585769Z",
     "iopub.status.busy": "2023-04-26T16:40:54.585285Z",
     "iopub.status.idle": "2023-04-26T16:40:54.648120Z",
     "shell.execute_reply": "2023-04-26T16:40:54.647083Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "color_dict = pd.Series({k:cmap(i/3) for i,k in enumerate(range(3))})\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "names_ = ['Model Serre_07', 'Human', 'Vgg_tlc_Animal']\n",
    "plt.scatter(df_serre_07[1], df_serre_07[2], label='Model Serre_07', color=color_dict[0])\n",
    "plt.scatter(df_serre_07[1], df_serre_07['TLC'], label='Vgg_tlc_Animal', color=color_dict[2])\n",
    "\n",
    "ax.tick_params(axis='y', labelsize=14)\n",
    "plt.xticks(font=font)\n",
    "plt.yticks(font=font)\n",
    "plt.legend(bbox_to_anchor=(1.07, .5), loc='center')\n",
    "ax.set_ylabel(\"\"\"Model's likelihood\"\"\", font=font)\n",
    "ax.set_xlabel(\"\"\"Human's Accuracy\"\"\", font=font)\n",
    "plt.tight_layout()\n",
    "# if do_savefig: fig.savefig(os.path.join(figpath, 'physio_comparison.pdf'), **opts_savefig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "https://github.com/laurentperrinet/PerrinetBednar15/blob/master/notebooks/5%20notebook_figure_animalness.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "The networks obtain an accuracy on the 'animal' synset similar to those found in the model and neurophysiological data of [Serre & al (2007)](http://vision.stanford.edu/teaching/cs131_fall1415/lectures/Serre2007.pdf) (about 83\\%). Yet, the bias found in is also present in the dataset used by [Serre & al (2007)](http://vision.stanford.edu/teaching/cs131_fall1415/lectures/Serre2007.pdf).  when we compare the performances of humans on this dataset with the performances achieved by the network on an image-by-image basis we found in  [experiment 1](#exp1). However, we also found a high correspondence (about 84%) between the correct predictions of human and our model. Indeed for some images the networks failed the categorization but the human succeeds and vice versa. For some images both the network and human succeed or failed to categorize an animal. These exemples could be the reflection of the specific features on which humans or our models rely to perform their categorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exp2\"></a>\n",
    "## Robustness of the categorization with different geometric transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Image processing and recognition for differents flip transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:54.653333Z",
     "iopub.status.busy": "2023-04-26T16:40:54.653032Z",
     "iopub.status.idle": "2023-04-26T16:40:54.721536Z",
     "shell.execute_reply": "2023-04-26T16:40:54.720434Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "flips = {}\n",
    "flips['Horizontal'] = transforms.RandomHorizontalFlip(p=1) \n",
    "flips['Vertical'] = transforms.RandomVerticalFlip(p=1)\n",
    "\n",
    "filename = f'{data_cache}/{datetag}_{HOST}_results_flip.json'\n",
    "print(f'{filename=}')\n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    df = pd.read_json(filename)\n",
    "else:\n",
    "    df_flip = pd.DataFrame([], columns=['model', 'model_task', 'task', 'goal', 'likelihood', 'fps', 'time', 'i_image', 'filename', 'top_1', 'flip'])   \n",
    "    # image preprocessing\n",
    "    (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(paths=paths, image_size=args.image_size, batch_size=1)\n",
    "    for flip in flips:\n",
    "        pprint(flip)\n",
    "        for task in args.tasks:\n",
    "            for i_image, (data, label) in enumerate(dataloaders[task]['test']):\n",
    "                data = flips[flip](data)\n",
    "                data, label, = data.to(device), label.to(device)\n",
    "                for model_name in models_vgg.keys():\n",
    "                    model = models_vgg[model_name].to(device)\n",
    "                    with torch.no_grad():\n",
    "                        goal = 'target' if 'target' in image_datasets[task]['test'].imgs[i_image][0] else 'distractor'\n",
    "                        tic = time.time()\n",
    "                        out = model(data).squeeze(0).cpu()\n",
    "                        model_task = 'animal' if 'animal' in model_name else 'artifact'\n",
    "                        if model_name == 'vgg_lut':\n",
    "                            model_task = 'Imagenet_challenge'\n",
    "                            percentage = torch.nn.functional.softmax(out, dim=0)\n",
    "                            likelihood = np.asarray(percentage[match[task]]).sum()\n",
    "                        else:\n",
    "                            likelihood = torch.sigmoid(out).numpy()[0]\n",
    "                        elapsed_time = time.time() - tic\n",
    "                        top_1 = 'target' if likelihood>0.50 else 'distractor'\n",
    "                    df_flip.loc[len(df_flip)] = {'model':model_name,'model_task':model_task, 'task':task, 'top_1':top_1, 'goal':goal,        \n",
    "                                            'likelihood':likelihood, 'time':elapsed_time, 'fps': 1/elapsed_time,\n",
    "                                            'i_image':i_image, 'filename':image_datasets[task]['test'].imgs[i_image][0], 'flip':flip}\n",
    "                    print(f'The {model_name} model categorize {model_task} with {likelihood*100:.3f} % likelihood ({top_1}) in {elapsed_time:.3f} seconds, groundtruth : {task}, {goal}')\n",
    "\n",
    "\n",
    "    df_flip.to_json(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:54.724523Z",
     "iopub.status.busy": "2023-04-26T16:40:54.724209Z",
     "iopub.status.idle": "2023-04-26T16:40:54.803477Z",
     "shell.execute_reply": "2023-04-26T16:40:54.802677Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = f'{data_cache}/{datetag}_{HOST}_results_flip.json'\n",
    "df_flip = pd.read_json(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computation Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:54.806270Z",
     "iopub.status.busy": "2023-04-26T16:40:54.805888Z",
     "iopub.status.idle": "2023-04-26T16:40:54.983125Z",
     "shell.execute_reply": "2023-04-26T16:40:54.982559Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(fig_width*phi, fig_width))\n",
    "color_dict = pd.Series({k:cmap(i/len(df['model'].unique())) for i,k in enumerate(df['model'].unique())})\n",
    "for color, model_name in zip(color_dict, df_flip['model'].unique()):\n",
    "    axs = sns.violinplot(x=\"flip\", y=\"time\", data=df_flip, inner=\"quartile\", hue='model', label=None)\n",
    "    axs.set_title('Processing time (s) for each network at different image size. Processed on : ' + HOST, size=20)\n",
    "    axs.set_ylabel('Computation time (s)', size=18)\n",
    "    axs.set_xlabel('Image size', size=18)\n",
    "    axs.set_yscale('log')\n",
    "    axs.grid(which='both', axis='y')\n",
    "    for side in ['top', 'right'] :axs.spines[side].set_visible(False)\n",
    "h, l = axs.get_legend_handles_labels()\n",
    "axs.legend(h[:len(df_flip['model'].unique())], l[:len(df_flip['model'].unique())], loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy comparison for all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:54.986817Z",
     "iopub.status.busy": "2023-04-26T16:40:54.986337Z",
     "iopub.status.idle": "2023-04-26T16:40:55.056871Z",
     "shell.execute_reply": "2023-04-26T16:40:55.056066Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for task in args.tasks:\n",
    "    df_acc_flip = pd.DataFrame({model_name: {flip: accuracy_score(df_flip[(df_flip['model']==model_name) & (df_flip['task']==task) & (df_flip['flip']==flip) ][\"top_1\"], \n",
    "                                                                   df_flip[(df_flip['model']==model_name) & (df_flip['task']==task) & (df_flip['flip']==flip)][\"goal\"])\n",
    "                                        for flip in flips} \n",
    "                           for model_name in df['model'].unique()})\n",
    "\n",
    "    ax = df_acc_flip.T.plot.bar(rot=30, figsize=(fig_width*phi, fig_width//3), fontsize=fontsize)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.hlines(xmin=-.5, xmax=len(df['model'].unique())-.5, y=1/2, ls='--', ec='gray', label='chance level')\n",
    "    # https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
    "    for container in ax.containers: ax.bar_label(container, padding=-50, color='black', fontsize=fontsize, fmt='%.3f', rotation=90)\n",
    "    plt.legend(bbox_to_anchor=(1.13, .35), loc='lower right', fontsize=fontsize)\n",
    "    ax.set_title(f'Animal vs Artifact images f1 score for re-trained models, task: {task}', size=fontsize)\n",
    "    ax.set_ylabel('F1 score', size=fontsize)\n",
    "    ax.set_xlabel('Model', size=fontsize);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "We tested the Vgg TLC network on the ImageNet dataset with either a vertical or horizontal reflection applied on the input. The networks keeps a good mean accuracy on the flipped dataset (about 95%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Image processing and recognition on grayscale images :\n",
    "\n",
    "Again, same likelihood indicators but now with a grayscale transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:55.063978Z",
     "iopub.status.busy": "2023-04-26T16:40:55.063776Z",
     "iopub.status.idle": "2023-04-26T16:40:55.098188Z",
     "shell.execute_reply": "2023-04-26T16:40:55.097665Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = f'{data_cache}/{datetag}_{HOST}_results_grayscale.json'\n",
    "print(f'{filename=}')\n",
    "\n",
    "transforms_gray = transforms.RandomGrayscale(p=1)\n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    df_gray = pd.read_json(filename)\n",
    "else:\n",
    "    df_gray = pd.DataFrame([], columns=['model', 'model_task', 'task', 'goal', 'likelihood', 'fps', 'time', 'i_image', 'filename', 'top_1']) \n",
    "        # image preprocessing\n",
    "    (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(paths=paths, image_size=args.image_size, batch_size=1)\n",
    "    for task in args.tasks:\n",
    "        pprint(task)\n",
    "        for i_image, (data, label) in enumerate(dataloaders[task]['test']):\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            for model_name in models_vgg.keys():\n",
    "                model = models_vgg[model_name].to(device)\n",
    "                with torch.no_grad():\n",
    "                    goal = 'target' if 'target' in image_datasets[task]['test'].imgs[i_image][0] else 'distractor'\n",
    "                    model_task = 'animal' if 'animal' in model_name else 'artifact'\n",
    "                    tic = time.time()\n",
    "                    out = model(transforms_gray(data)).squeeze(0).cpu()\n",
    "                    if model_name == 'vgg_lut':\n",
    "                        model_task = 'Imagenet_challenge'\n",
    "                        percentage = torch.nn.functional.softmax(out, dim=0)\n",
    "                        likelihood = np.asarray(percentage[match[task]]).sum()\n",
    "                    else:\n",
    "                        likelihood = torch.sigmoid(out).numpy()[0]\n",
    "                    top_1 = 'target' if likelihood>0.50 else 'distractor'\n",
    "                    elapsed_time = time.time() - tic\n",
    "                df_gray.loc[len(df_gray)] = {'model':model_name,'model_task':model_task, 'task':task, 'top_1':top_1, 'goal':goal, 'likelihood':likelihood, 'time':elapsed_time, 'fps': 1/elapsed_time,\n",
    "                                        'i_image':i_image, 'filename':image_datasets[task]['test'].imgs[i_image][0]}\n",
    "                print(f'The {model_name} model categorize {model_task} with {likelihood*100:.3f} % likelihood ({top_1}) in {elapsed_time:.3f} seconds, groundtruth : {task}, {goal}')\n",
    "    df_gray.to_json(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:55.108010Z",
     "iopub.status.busy": "2023-04-26T16:40:55.107812Z",
     "iopub.status.idle": "2023-04-26T16:40:55.166866Z",
     "shell.execute_reply": "2023-04-26T16:40:55.166212Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = f'{data_cache}/{datetag}_{HOST}_results_grayscale.json'\n",
    "df_gray = pd.read_json(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:55.169362Z",
     "iopub.status.busy": "2023-04-26T16:40:55.169100Z",
     "iopub.status.idle": "2023-04-26T16:40:55.229849Z",
     "shell.execute_reply": "2023-04-26T16:40:55.229049Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(df_gray['model'].unique()), 1, figsize=(fig_width, fig_width*phi))\n",
    "for color, df_, label, legend in zip(['gray', 'red'], [df_gray, df], ['black', 'color'], ['Grayscale', 'Regular']):\n",
    "    for ax, model_name in zip(axs, models_vgg):\n",
    "        ax.set_ylabel('Frequency', fontsize=fontsize) \n",
    "        df_[df_['model']==model_name]['time'].plot.hist(bins=150, lw=1, label=str(legend+ ' ' + model_name), ax=ax, color=color, density=True)\n",
    "        ax.legend(loc='upper right', fontsize=fontsize)\n",
    "        ax.set_xlim(df_gray['time'].quantile(.01), df_gray['time'].quantile(.99))\n",
    "        ax.legend(bbox_to_anchor=(1.25, .35), loc='lower right')\n",
    "        ax.set_ylabel('')\n",
    "axs[0].set_title('Distribution of the Processing time (s)', size=20)\n",
    "axs[len(df_gray['model'].unique())//2].set_ylabel('Frequency', fontsize=fontsize);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:55.232504Z",
     "iopub.status.busy": "2023-04-26T16:40:55.232301Z",
     "iopub.status.idle": "2023-04-26T16:40:55.304610Z",
     "shell.execute_reply": "2023-04-26T16:40:55.303834Z"
    }
   },
   "outputs": [],
   "source": [
    "for task in args.tasks:\n",
    "    df_acc_gray = pd.DataFrame({model_name: {label: accuracy_score(df_[(df_['model']==model_name) & (df_['task']==task)][\"top_1\"], \n",
    "                                                                   df_[(df_['model']==model_name) & (df_['task']==task)][\"goal\"])\n",
    "                                        for label, df_ in zip(['original', 'gray'], [df, df_gray])} \n",
    "                           for model_name in df_gray['model'].unique()})\n",
    "\n",
    "    ax = df_acc_gray.T.plot.bar(rot=30, figsize=(fig_width*phi, fig_width//3), fontsize=fontsize)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.hlines(xmin=-.5, xmax=len(models_vgg)-.5, y=1/2, ls='--', ec='gray', label='chance level')\n",
    "    # https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
    "    for container in ax.containers: ax.bar_label(container, padding=-50, color='black', fontsize=fontsize, fmt='%.3f', rotation=90)\n",
    "    plt.legend(bbox_to_anchor=(1.12, .35), loc='lower right', fontsize=fontsize)\n",
    "    ax.grid(which='both', axis='y')\n",
    "    for side in ['top', 'right'] :ax.spines[side].set_visible(False)\n",
    "    ax.set_title(f'Experiment - color vs gray images, task: {task}', size=fontsize)\n",
    "    ax.set_ylabel('F1 Score', size=fontsize)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "We tested the networks on the ImageNet dataset with a grayscale filter applied on the input. The networks keeps a good mean accuracy on the grayscale dataset (about 94%). Note that the Vgg Gray networks achieves the task with a gray scale applied with a sightly better mean accuracy ([experiment 4](#exp4)) but not on the regular 'test' dataset ([experiment 1](#exp1))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  Image processing and recognition on rotated images :\n",
    "\n",
    "In addition we tested all networks on our IMAGENET dataset while rotating the image around the center by -180° to +180°."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:55.307709Z",
     "iopub.status.busy": "2023-04-26T16:40:55.307498Z",
     "iopub.status.idle": "2023-04-26T16:40:55.341551Z",
     "shell.execute_reply": "2023-04-26T16:40:55.340956Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "angles = np.arange(-180, 181, 15)\n",
    "filename = f'{data_cache}/{datetag}_{HOST}_results_rotate.json'\n",
    "print(f'{filename=}')\n",
    "   \n",
    "if os.path.isfile(filename):\n",
    "    df_angle = pd.read_json(filename)\n",
    "else:\n",
    "    df_angle = pd.DataFrame([], columns=['mean_prediction', 'model', 'var'])\n",
    "    (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(paths=paths, image_size=args.image_size, batch_size=64)\n",
    "    # image preprocessing\n",
    "    for task in args.tasks:\n",
    "        print(task)\n",
    "        for angle in angles:\n",
    "            print(f'Rotation by {angle=}°')\n",
    "            for model_name in models_vgg.keys():\n",
    "                model = models_vgg[model_name].to(device)\n",
    "                acc_= 0\n",
    "                for i_image, (data, label) in enumerate(dataloaders[task]['test']):\n",
    "                    data, label, = data.to(device), label.to(device)\n",
    "                    data = transforms.functional.rotate(data, angle=float(angle), expand=True)\n",
    "                    data = transforms.functional.resize(data, size=(args.image_size, args.image_size), antialias=True)\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(data)\n",
    "                        if model_name == 'vgg_lut':\n",
    "                            percentage = torch.nn.functional.softmax(outputs, dim=1)\n",
    "                            preds = torch.sum(percentage[:, match[task]], dim=1)\n",
    "                            acc_ += torch.sum(torch.round(preds) == label.data)\n",
    "                        else:\n",
    "                            preds = torch.round(torch.sigmoid(outputs[:,0].data))\n",
    "                            acc_ += torch.sum(preds == label.data)\n",
    "                avg_acc = acc_ / dataset_sizes[task]['test']\n",
    "                df_angle.loc[len(df_angle)] = {'model':model_name, 'mean_prediction':float(avg_acc), 'var':angle}  \n",
    "                print(model_name, float(avg_acc), angle)\n",
    "    df_angle.to_json(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:55.343953Z",
     "iopub.status.busy": "2023-04-26T16:40:55.343675Z",
     "iopub.status.idle": "2023-04-26T16:40:55.347293Z",
     "shell.execute_reply": "2023-04-26T16:40:55.346718Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = f'{data_cache}/{datetag}_{HOST}_results_rotate.json'\n",
    "if os.path.isfile(filename):\n",
    "    df_angle_animal = pd.read_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:55.351544Z",
     "iopub.status.busy": "2023-04-26T16:40:55.351348Z",
     "iopub.status.idle": "2023-04-26T16:40:55.357578Z",
     "shell.execute_reply": "2023-04-26T16:40:55.357073Z"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(filename):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(fig_width, fig_width//phi/2))\n",
    "    color_dict = pd.Series({k:cmap(i/len(df_angle_animal['model'].unique())) for i,k in enumerate(df_angle_animal['model'].unique())})\n",
    "\n",
    "    for model_name in df_angle['model'].unique():     \n",
    "        df_angle_animal[(df_angle_animal['model']==model_name)].plot.scatter(x=\"var\", y=\"mean_prediction\", c=[color_dict[model_name],],\n",
    "                                                            marker='o', lw=0, label=model_name, ax=ax)\n",
    "    ax.tick_params(axis='x', labelsize=14)\n",
    "    ax.tick_params(axis='y', labelsize=14)\n",
    "    ax.set_ylim(0.5, 1)\n",
    "    ax.set_xlim(-185, 185)\n",
    "    ax.set_xticks([-180, -90, 0, 90, 180])\n",
    "    ax.set_ylabel('Mean Accuracy', font=font)\n",
    "    ax.set_xlabel('Rotation (°)', font=font)\n",
    "    plt.legend(bbox_to_anchor=(1.13, .5), loc='center', fontsize=10, prop=font)\n",
    "    plt.xticks(font=font)\n",
    "    plt.yticks(font=font);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:55.359988Z",
     "iopub.status.busy": "2023-04-26T16:40:55.359608Z",
     "iopub.status.idle": "2023-04-26T16:40:55.366915Z",
     "shell.execute_reply": "2023-04-26T16:40:55.366421Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(filename):\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(fig_width, fig_width//phi), sharex=True)\n",
    "    grphe1 = ['vgg_lut', 'vgg16_tlc_animal']\n",
    "    grphe2 = ['vgg16_tlc_animal', 'vgg16_tlda_animal']\n",
    "    all_ = ['temp', 'vgg_lut', 'temp', 'vgg16_tlc_animal', 'temp', 'vgg16_tlda_animal']\n",
    "    all_ = ['temp', 'vgg_lut', 'temp', 'vgg16_tlc_animal', 'temp', 'vgg16_tlda_animal']\n",
    "\n",
    "    cmap = plt.cm.get_cmap('viridis')\n",
    "    color_dict = pd.Series({k:cmap(i/len(all_)) for i,k in enumerate(all_)})\n",
    "    label_dict = {'vgg_lut':'VGG LUT', 'vgg16_tlc_animal':'VGG TLC', 'vgg16_tlda_animal':'VGG Augment'}\n",
    "\n",
    "    graphes = [grphe1, grphe2]\n",
    "    for ax, graphe in zip(axs, graphes):\n",
    "        for model_name in graphe:\n",
    "            df_angle_model = (df_angle_animal['model']==model_name)\n",
    "            # df_angle_animal[df_angle_model].plot.scatter(x=\"var\", y=\"mean_prediction\", \n",
    "            #                                              c=[color_dict[model_name]],\n",
    "            #                                              marker='o', label=label_dict[model_name], lw=2., ax=ax)\n",
    "            df_angle_animal[df_angle_model].plot(x=\"var\", y=\"mean_prediction\", \n",
    "                                                         color=[color_dict[model_name]],\n",
    "                                                         marker='o', label=label_dict[model_name], lw=2., ax=ax)\n",
    "                                                                \n",
    "        ax.tick_params(axis='x', labelsize=14)\n",
    "        ax.tick_params(axis='y', labelsize=14)\n",
    "        ax.set_ylim(0.5, 1)\n",
    "        ax.set_xlim(-185, 185)\n",
    "        ax.set_xticks([-180, -90, 0, 90, 180])\n",
    "        ax.set_ylabel('Accuracy', font=font)\n",
    "        ax.set_xlabel('Rotation (°)', font=font)\n",
    "        ax.yaxis.set_major_locator(matplotlib.ticker.FixedLocator([.5, .75, 1]))\n",
    "        for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "            label.set_font(font)\n",
    "        # ax.get_legend().remove()\n",
    "        # ax_dict['top'].legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left',\n",
    "        #               ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "        ax.legend(loc='lower left', fontsize=fontsize, edgecolor='none')\n",
    "\n",
    "    if do_savefig: fig.savefig(os.path.join(figpath, 'full_rot.pdf'), **opts_savefig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "The image size or a gray filter during training does not seem to impact the robustness to rotation as all three networks follow the same accuracy level for the categorization. Furthermore,\n",
    "these three networks maintain a good accuracy during the process (VGG TLC 90.272 +/- 2.05, VGG GRAY 89.464 +/-2.72, VGG SCALE 89.222: +/- 2.722) (see Figure 5). As the networks seems robust to relection [experiment 2](#exp2), to the resolution [experiment 3](#exp3) (in a short range centered in the training resolution) and to grayscale [experiment 4](#exp4), the robustness of the categorization to different image transformations by the models are similar to that reported in psychophysics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What features are necessary to achieve the task? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exp1\"></a>\n",
    "### Performances on natural scenes containing animal with prunned networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:55.369445Z",
     "iopub.status.busy": "2023-04-26T16:40:55.369066Z",
     "iopub.status.idle": "2023-04-26T16:40:55.401843Z",
     "shell.execute_reply": "2023-04-26T16:40:55.401337Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = f'{data_cache}/{datetag}_{HOST}_results_1_del_lay.json'\n",
    "print(f'{filename=}')\n",
    "task = 'animal'\n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    df_del_lay = pd.read_json(filename)\n",
    "else:\n",
    "    df_del_lay = pd.DataFrame([], columns=['model', 'goal', 'likelihood', 'fps', 'time', 'i_image', 'filename', 'top_1']) \n",
    "        # image preprocessing\n",
    "    (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(paths=paths, image_size=args.image_size, batch_size=1)\n",
    "    for i_image, (data, label) in enumerate(dataloaders[task]['test']):\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        for model_name in models_vgg_pruned.keys():\n",
    "            model = models_vgg_pruned[model_name].to(device)\n",
    "            with torch.no_grad():\n",
    "                goal = 'target' if 'target' in image_datasets[task]['test'].imgs[i_image][0] else 'distractor'\n",
    "                tic = time.time()\n",
    "                out = model(data).squeeze(0).cpu()\n",
    "                if model_name == 'vgg_lut':\n",
    "                    model_task = 'Imagenet_challenge'\n",
    "                    percentage = torch.nn.functional.softmax(out, dim=0)\n",
    "                    likelihood = np.asarray(percentage[match[task]]).sum()\n",
    "                else:\n",
    "                    likelihood = torch.sigmoid(out).numpy()[0]\n",
    "                top_1 = 'target' if likelihood>0.50 else 'distractor'\n",
    "                elapsed_time = time.time() - tic\n",
    "            df_del_lay.loc[len(df_del_lay)] = {'model':model_name, 'top_1':top_1, 'goal':goal, 'likelihood':likelihood, 'time':elapsed_time, 'fps': 1/elapsed_time,\n",
    "                                    'i_image':i_image, 'filename':image_datasets[task]['test'].imgs[i_image][0]}\n",
    "            print(f'The {model_name} model categorize an animal with {likelihood*100:.3f} % likelihood ({top_1}) in {elapsed_time:.3f} seconds, groundtruth= {task}, {goal}')\n",
    "    df_del_lay.to_json(filename)\n",
    "df_del_lay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:55.407411Z",
     "iopub.status.busy": "2023-04-26T16:40:55.407210Z",
     "iopub.status.idle": "2023-04-26T16:40:56.339438Z",
     "shell.execute_reply": "2023-04-26T16:40:56.338762Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "colors = [cmap(1-.9*i_color/len(df_del_lay['model'].unique())) for i_color in range(len(df_del_lay['model'].unique()))]\n",
    "fig, ax = plt.subplots(figsize=(fig_width, fig_width/4))\n",
    "\n",
    "df_del_lay['mean_prediction'].T.plot(kind='bar',    # Plot a bar chart\n",
    "                 rot=20, fontsize=fontsize,\n",
    "                 legend=False,    # Turn the Legend off\n",
    "                 width=0.75,      # Set bar width as 75% of space available\n",
    "                 ax=ax,\n",
    "                 color=colors)\n",
    "\n",
    "ax.hlines(xmin=-.5, xmax=len(df_del_lay['model'].unique())-.5, y=1/2, ls='--', ec='gray')\n",
    "ax.set_ylabel('Accuracy', font=font)\n",
    "ax.set_xticklabels(display_model_name[''])\n",
    "for container in ax.containers: \n",
    "    # ax.bar_label(container, padding=-50, fmt='%.3f', rotation=90, color='white', fontsize=fontsize, weight='bold')\n",
    "    ax.bar_label(container, padding=-50, fmt='%.3f', rotation=90, color='black', fontsize=fontsize, weight='bold')\n",
    "\n",
    "if do_savefig: fig.savefig(os.path.join(figpath, 'pruning_accuracy.pdf'), **opts_savefig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exp1\"></a>\n",
    "### Image processing and recognition for differents labels with rotation with prunned networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:56.344760Z",
     "iopub.status.busy": "2023-04-26T16:40:56.344540Z",
     "iopub.status.idle": "2023-04-26T16:40:56.377394Z",
     "shell.execute_reply": "2023-04-26T16:40:56.376831Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = f'{data_cache}/{datetag}_{HOST}_results_rotate_pruned.json'\n",
    "print(f'{filename=}')\n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    df_angle = pd.read_json(filename)\n",
    "else:\n",
    "    df_angle = pd.DataFrame([], columns=['mean_prediction', 'model', 'var'])\n",
    "    (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(paths=paths, image_size=args.image_size, batch_size=batch_size)\n",
    "    # image preprocessing\n",
    "    #for task in args.tasks:\n",
    "    task = 'animal'\n",
    "    for angle in angles:\n",
    "        print(f'Rotation by {angle=}°')\n",
    "        for model_name in models_vgg_pruned.keys():\n",
    "            model = models_vgg_pruned[model_name].to(device)\n",
    "            acc_= 0\n",
    "            for i_image, (data, label) in enumerate(dataloaders[task]['test']):\n",
    "                data = transforms.functional.rotate(data, angle=float(angle), expand = True)\n",
    "                data, label, = data.to(device), label.to(device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(data)\n",
    "                    if model_name == 'vgg_lut':\n",
    "                        percentage = torch.nn.functional.softmax(outputs, dim=1)\n",
    "                        preds = torch.sum(percentage[:,match[task]], dim=1)\n",
    "                        acc_ += torch.sum(torch.round(preds) == label.data)\n",
    "                    else:\n",
    "                        preds = torch.round(torch.sigmoid(outputs[:,0].data))\n",
    "                        acc_ += torch.sum(preds == label.data)\n",
    "            avg_acc = acc_ / dataset_sizes[task]['test']\n",
    "            df_angle.loc[len(df_angle)] = {'model':model_name, 'mean_prediction':float(avg_acc), 'var':angle}  \n",
    "            print(model_name, float(avg_acc), angle)\n",
    "            #print(f'The {model_name} model categorize {model_task} with {likelihood*100:.3f} % likelihood ({top_1}) in {elapsed_time:.3f} seconds, groundtruth : {task}, {goal}')\n",
    "            # torch.cuda.empty_cache()\n",
    "    df_angle.to_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:56.382837Z",
     "iopub.status.busy": "2023-04-26T16:40:56.382639Z",
     "iopub.status.idle": "2023-04-26T16:40:56.389365Z",
     "shell.execute_reply": "2023-04-26T16:40:56.388830Z"
    }
   },
   "outputs": [],
   "source": [
    "df_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:56.393557Z",
     "iopub.status.busy": "2023-04-26T16:40:56.393361Z",
     "iopub.status.idle": "2023-04-26T16:40:56.576566Z",
     "shell.execute_reply": "2023-04-26T16:40:56.575824Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(fig_width/1.2, fig_width/phi/1.2))\n",
    "model_names = {model:model for model in df_angle['model'].unique()}\n",
    "model_names['vgg16_tlaa_animal'] = 'VGG TLAA'\n",
    "model_names['vgg_lut'] = 'VGG LUT'\n",
    "colors = [cmap(1-.9*i_color/len(model_names)) for i_color in range(len(model_names))]\n",
    "colors[0] = (0, 0, 0, 1)\n",
    "\n",
    "for i, model_name in enumerate(df_angle['model'].unique()):\n",
    "    if 'lut' in model_name:\n",
    "        ax = df_angle[(df_angle['model']==model_name)].plot(x=\"var\", y=\"mean_prediction\", color=colors[i],\n",
    "                                                         marker='o', lw=3, label=model_names[model_name], ax=ax)\n",
    "    else:\n",
    "        ax = df_angle[(df_angle['model']==model_name)].plot(x=\"var\", y=\"mean_prediction\", color=colors[i],\n",
    "                                                         marker='o', lw=1, label=model_names[model_name], ax=ax)\n",
    "    print(f'Mean for {model_name}', np.mean(df_angle[(df_angle['model']==model_name)]['mean_prediction']))\n",
    "    print(f'Std for {model_name}', np.std(df_angle[(df_angle['model']==model_name)]['mean_prediction']))\n",
    "\n",
    "\n",
    "ax.hlines(xmin=-185, xmax=185, y=1/2, ls='--', ec='gray')\n",
    "ax.tick_params(axis='x', labelsize=14)\n",
    "ax.tick_params(axis='y', labelsize=14)\n",
    "ax.set_xlim(-185, 185)\n",
    "ax.set_xticks([-180, -90, 0, 90, 180])\n",
    "ax.set_ylabel('Mean Accuracies', font=font)\n",
    "ax.set_xlabel('Rotation (°)', font=font)\n",
    "plt.legend(bbox_to_anchor=(1.15, .5), loc='center', fontsize=fontsize, edgecolor='none')\n",
    "plt.tight_layout()\n",
    "plt.xticks(font=font)\n",
    "plt.yticks(font=font);\n",
    "if do_savefig: fig.savefig(os.path.join(figpath, 'full_rot_pruned.pdf'), **opts_savefig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image processing and recognition for different labels with shuffled patches with prunned networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:56.580092Z",
     "iopub.status.busy": "2023-04-26T16:40:56.579888Z",
     "iopub.status.idle": "2023-04-26T16:40:56.614883Z",
     "shell.execute_reply": "2023-04-26T16:40:56.614157Z"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "class ShufflePatches(object): #https://stackoverflow.com/questions/66962837/shuffle-patches-in-image-batch\n",
    "    def __init__(self, patch_size):\n",
    "        self.ps = patch_size\n",
    "    def __call__(self, x):\n",
    "        # divide the batch of images into non-overlapping patches\n",
    "        u = nnf.unfold(x.unsqueeze(dim=0), kernel_size=self.ps, stride=self.ps, padding=0)\n",
    "        # permute the patches of each image in the batch\n",
    "        pu = torch.cat([b_[:, torch.randperm(b_.shape[-1])][None,...] for b_ in u], dim=0)\n",
    "        # fold the permuted patches back together\n",
    "        f = nnf.fold(pu, x.shape[-2:], kernel_size=self.ps, stride=self.ps, padding=0)\n",
    "        return f.squeeze(dim=0)\n",
    "\n",
    "def datasets_transforms_shuffle(paths=paths, image_size=args.image_size, p=0, shuffle=args.image_size , num_workers=0, angle=0, batch_size=args.batch_size, **kwargs): # Similar to the one used before but with a custom patch transforms\n",
    "    \n",
    "    transforms_seq = transforms.Compose([      \n",
    "                        transforms.Resize((int(image_size), int(image_size))),\n",
    "                        transforms.ToTensor(),   \n",
    "                        ShufflePatches((shuffle,shuffle)),  \n",
    "                        transforms_norm ])   \n",
    "    \n",
    "    image_datasets = datasets.ImageFolder(paths['animal']['test'], transform=transforms_seq)\n",
    "    dataloaders = torch.utils.data.DataLoader(image_datasets, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    dataset_sizes = len(image_datasets)\n",
    "\n",
    "    return dataset_sizes, dataloaders, image_datasets\n",
    "\n",
    "(dataset_sizes, dataloaders, image_datasets) = datasets_transforms_shuffle(paths=paths, image_size=args.image_size)\n",
    "\n",
    "for task in args.tasks :\n",
    "    pprint(f'Pre-selected tasks shuffle {task} : ')\n",
    "    print(f\"Loaded {dataset_sizes} images\")\n",
    "    \n",
    "#------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:56.618863Z",
     "iopub.status.busy": "2023-04-26T16:40:56.618662Z",
     "iopub.status.idle": "2023-04-26T16:40:56.653049Z",
     "shell.execute_reply": "2023-04-26T16:40:56.652455Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = f'{data_cache}/{datetag}_{HOST}_results_shuffle.json'\n",
    "print(f'{filename=}')\n",
    "\n",
    "\n",
    "shuffle_sizes = [256, 128, 64, 32]\n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    df_shuffle = pd.read_json(filename)\n",
    "else:\n",
    "    df_shuffle = pd.DataFrame([], columns=['mean_prediction', 'model', 'var'])\n",
    "    for shuffle_size in shuffle_sizes:\n",
    "        print(shuffle_size)\n",
    "        (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms_shuffle(\n",
    "            image_size=args.image_size, shuffle=shuffle_size, batch_size=batch_size)\n",
    "        for model_name in models_vgg_pruned.keys():\n",
    "            model = models_vgg_pruned[model_name].to(device)\n",
    "            acc_= 0\n",
    "            for i_image, (data, label) in enumerate(dataloaders):\n",
    "                data, label = data.to(device), label.to(device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(data)\n",
    "                    if model_name == 'vgg_lut':\n",
    "                        percentage = torch.nn.functional.softmax(outputs, dim=1)\n",
    "                        preds = torch.sum(percentage[:,match[task]], dim=1)\n",
    "                        acc_ += torch.sum(torch.round(preds) == label.data)\n",
    "                    else:\n",
    "                        preds = torch.round(torch.sigmoid(outputs[:,0].data))\n",
    "                        acc_ += torch.sum(preds == label.data)\n",
    "            avg_acc = acc_ / dataset_sizes\n",
    "            df_shuffle.loc[len(df_shuffle)] = {'model':model_name, 'mean_prediction':float(avg_acc), 'var':shuffle_size}  \n",
    "            print(model_name, float(avg_acc), shuffle_size)\n",
    "            torch.cuda.empty_cache()\n",
    "        df_shuffle.to_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:56.655478Z",
     "iopub.status.busy": "2023-04-26T16:40:56.655205Z",
     "iopub.status.idle": "2023-04-26T16:40:56.851848Z",
     "shell.execute_reply": "2023-04-26T16:40:56.851168Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(fig_width, fig_width/(phi*2)))\n",
    "\n",
    "cmap = plt.cm.get_cmap('viridis')\n",
    "color_dict = pd.Series({k:cmap(i/len(shuffle_sizes)) for i,k in enumerate(shuffle_sizes)})\n",
    "color_dict[64] = 'darkorange' # Get a different color to contrast the figure\n",
    "\n",
    "for shuffle_size in shuffle_sizes:    \n",
    "    ax = df_shuffle[(df_shuffle['var']==shuffle_size)].plot(x=\"model\", y=\"mean_prediction\", color=color_dict[shuffle_size],\n",
    "                                                         marker='o', label=f'{shuffle_size} x {shuffle_size}',\n",
    "                                                            ax=ax)\n",
    "    fig.autofmt_xdate(rotation=45)\n",
    "    ax.set_ylabel('Mean Accuracies', font=font)\n",
    "    ax.hlines(xmin=-.5, xmax=len(df_shuffle['model'].unique())-.5, y=1/2, ls='--', ec='gray')\n",
    "    plt.xticks(np.linspace(0,15,15,dtype=int), fontsize=fontsize)\n",
    "    plt.yticks(fontsize=fontsize)\n",
    "    \n",
    "ax.set_xticks(range(len(display_model_name['']))) \n",
    "ax.set_xlabel('', font=font)\n",
    "ax.set_xticklabels(display_model_name[''])\n",
    "ax.set_xlabel('', font=font)\n",
    "plt.legend(bbox_to_anchor=(1.08, .5), loc='center', fontsize=fontsize, edgecolor='none')\n",
    "if do_savefig: fig.savefig(os.path.join(figpath, 'shuffle.pdf'), **opts_savefig);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dependence of accuracy scores between the two tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limiting our classification to Strictly animal / artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:56.855468Z",
     "iopub.status.busy": "2023-04-26T16:40:56.855200Z",
     "iopub.status.idle": "2023-04-26T16:40:56.892069Z",
     "shell.execute_reply": "2023-04-26T16:40:56.891437Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = f'{data_cache}/{datetag}_{HOST}_results_strictly_basic.json'\n",
    "print(f'{filename=}')\n",
    "paths['animal']['test'] = '../data/strictly_animal/test'\n",
    "paths['artifact']['test'] = '../data/strictly_artifact/test'\n",
    "if os.path.isfile(filename):\n",
    "    df_strictly = pd.read_json(filename)\n",
    "else:\n",
    "    df_strictly = pd.DataFrame([], columns=['model', 'model_task', 'task', 'goal', 'likelihood', 'fps', 'time', 'i_image', 'filename', 'top_1', 'device_type']) \n",
    "        # image preprocessing\n",
    "    (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(image_size=args.image_size, batch_size=1)\n",
    "    for task in args.tasks:\n",
    "        pprint(task)\n",
    "        for i_image, (data, label) in enumerate(dataloaders[task]['test']):\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            for model_name in models_vgg.keys():\n",
    "                model = models_vgg[model_name].to(device)\n",
    "                with torch.no_grad():\n",
    "                    goal = 'target' if 'target' in image_datasets[task]['test'].imgs[i_image][0] else 'distractor'\n",
    "                    tic = time.time()\n",
    "                    out = model(data).squeeze(0).cpu()\n",
    "                    model_task = 'animal' if 'animal' in model_name else 'artifact'\n",
    "                    if model_name == 'vgg_lut':\n",
    "                        model_task = 'Imagenet_challenge'\n",
    "                        percentage = torch.nn.functional.softmax(out, dim=0)\n",
    "                        likelihood = np.asarray(percentage[match[task]]).sum()\n",
    "                    else:\n",
    "                        likelihood = torch.sigmoid(out).numpy()[0]\n",
    "                    elapsed_time = time.time() - tic\n",
    "                    top_1 = 'target' if likelihood>0.50 else 'distractor'\n",
    "                    elapsed_time = time.time() - tic\n",
    "                df.loc[len(df)] = {'model':model_name,'model_task':model_task, 'task':task, 'top_1':top_1, 'goal':goal, 'likelihood':likelihood, 'time':elapsed_time, 'fps': 1/elapsed_time,\n",
    "                                        'i_image':i_image, 'filename':image_datasets[task]['test'].imgs[i_image][0], 'device_type':device.type}\n",
    "                print(f'The {model_name} model categorize {model_task} with {likelihood:.3f} % likelihood ({top_1}) in {elapsed_time:.3f} seconds, groundtruth : {task}, {goal}')\n",
    "    df.to_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:56.896880Z",
     "iopub.status.busy": "2023-04-26T16:40:56.896591Z",
     "iopub.status.idle": "2023-04-26T16:40:56.962613Z",
     "shell.execute_reply": "2023-04-26T16:40:56.961830Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_acc_strictly = pd.DataFrame({model_name: {task: df_strictly[(df_strictly['model']==model_name) & (df_strictly['task']==task)][\"mean_prediction\"].item()\n",
    "                                    for task in df_strictly['task'].unique()} \n",
    "                       for model_name in df_strictly['model'].unique()})\n",
    "\n",
    "ax = df_acc_strictly.T.plot.bar(rot=20, figsize=(fig_width, fig_width/(phi*2)), fontsize=fontsize)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.hlines(xmin=-.5, xmax=len(models_vgg)-.5, y=1/2, ls='--', ec='k', label='chance level')\n",
    "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
    "for container in ax.containers: ax.bar_label(container, padding=-50, color='black', fontsize=fontsize, fmt='%.3f')\n",
    "plt.legend(bbox_to_anchor=(1.2, .35), loc='lower right', fontsize=fontsize)\n",
    "ax.set_title(f'Animal vs Artifact images mean accuracy on stritly animal (resp. artifact) data set', size=fontsize)\n",
    "ax.set_ylabel('Accuracy', size=fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T16:40:56.967004Z",
     "iopub.status.busy": "2023-04-26T16:40:56.966806Z",
     "iopub.status.idle": "2023-04-26T16:40:57.524690Z",
     "shell.execute_reply": "2023-04-26T16:40:57.523924Z"
    }
   },
   "outputs": [],
   "source": [
    "acc_dict = df_acc.to_dict()\n",
    "acc_dict_s = df_acc_strictly.to_dict()\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, sharex=False, sharey=True, figsize=(fig_width*1.5, fig_width//phi**2.5), gridspec_kw={'width_ratios': [2, 2, 2, 2, 1]})\n",
    "\n",
    "titles = [\"(A) Animal set\",  \"(B) Strictly Animal set\", \"(C) Artifact set\" , \"(D) Strictly Artifact set\", \"(E) Random set\" ]\n",
    "\n",
    "\n",
    "\n",
    "data_rand = { 'x_abs': ['Vgg Random'],\n",
    "        'y_val' : [ acc_dict['vgg16_tlaa_random']['random']]} \n",
    "data_ani = { 'x_abs': ['Animal', 'Artifact'],\n",
    "             'y_val' : [acc_dict['vgg16_tlaa_animal']['animal'], acc_dict['vgg16_tlaa_artifact']['animal']]}\n",
    "data_art = { 'x_abs': ['Vgg Animal', 'Vgg Artifact'],\n",
    "             'y_val' : [acc_dict['vgg16_tlaa_animal']['artifact'], acc_dict['vgg16_tlaa_artifact']['artifact']]}\n",
    "data_ani_s = { 'x_abs': ['Vgg Animal', 'Vgg Artifact'],\n",
    "             'y_val' : [acc_dict_s['vgg16_tlaa_animal']['animal'], acc_dict_s['vgg16_tlaa_artifact']['animal']]}\n",
    "data_art_s = { 'x_abs': ['Vgg Animal', 'Vgg Artifact'],\n",
    "             'y_val' : [acc_dict_s['vgg16_tlaa_animal']['artifact'], acc_dict_s['vgg16_tlaa_artifact']['artifact']]}\n",
    "\n",
    "# HACK à simplifier!\n",
    "df_rand = pd.DataFrame(data_rand)\n",
    "df_ani = pd.DataFrame(data_ani)\n",
    "df_art = pd.DataFrame(data_art)\n",
    "df_ani_s = pd.DataFrame(data_ani_s)\n",
    "df_art_s = pd.DataFrame(data_art_s)\n",
    "\n",
    "\n",
    "\n",
    "axs[4].bar(df_rand['x_abs'], df_rand['y_val'],  align='center', color= tasks_color['random'])\n",
    "axs[0].bar(df_ani['x_abs'], df_ani['y_val'], color= (tasks_color['animal'], tasks_color['artifact']))\n",
    "axs[2].bar(df_art['x_abs'], df_art['y_val'], color= (tasks_color['animal'], tasks_color['artifact']))\n",
    "axs[1].bar(df_ani_s['x_abs'], df_ani_s['y_val'], color= (tasks_color['animal'], tasks_color['artifact']))\n",
    "axs[3].bar(df_art_s['x_abs'], df_art_s['y_val'], color= (tasks_color['animal'], tasks_color['artifact']))\n",
    "\n",
    "\n",
    "axs[0].set_ylabel('Accuracy', fontsize = fontsize)\n",
    "\n",
    "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
    "for j, ax in enumerate(axs):\n",
    "    axs[j].hlines(xmin=-.5, xmax=1.5, y=1/2, ls='--', ec='gray') if j != 4 else axs[j].hlines(xmin=-.5, xmax=.5, y=1/2, ls='--', ec='gray')  \n",
    "    axs[j].set_xticklabels(labels=['Animal','Artifact'], font=font) if j != 4 else axs[j].set_xticklabels(labels=['VGG Random'], font=font) \n",
    "    ax.set_title(titles[j], fontsize = 13)\n",
    "    ax.tick_params(axis='both', labelsize=13)\n",
    "    for i, container in enumerate(ax.containers):\n",
    "        ax.bar_label(container, padding=-15, color='black', fontsize=fontsize, fmt='%.3f', rotation=0, label_type = 'edge', fontweight='bold')\n",
    "\n",
    "    \n",
    "plt.xticks(font=font)\n",
    "plt.yticks(font=font)        \n",
    "plt.tight_layout()\n",
    "if do_savefig: fig.savefig(os.path.join(figpath, 'mean_accuracy.pdf'), **opts_savefig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "We tested the networks Vgg TLC, Vgg Scale and Vgg Gray trained to detect an animal on the dataset where the targets are 'animal' (respectively train to detect an artifact on dataset where targets are artifacts). Then we tested the networks trained to detect an 'animal' on the dataset where the targets are 'artifact' and vice versa. Here, by exposing the predictions for the 'animal' and 'artifact' labels, we highlight a bias in the composition of the dataset. Although the outputs are independent, 'animal' images confidently correspond to 'non-artifact' images (and vice versa), thus facilitating the overall detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS: testing the model on images generated by stable diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [GitHub](https://github.com/CompVis/latent-diffusion).\n",
    "\n",
    "```\n",
    "@misc{rombach2021highresolution,\n",
    "      title={High-Resolution Image Synthesis with Latent Diffusion Models}, \n",
    "      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},\n",
    "      year={2021},\n",
    "      eprint={2112.10752},\n",
    "      archivePrefix={arXiv},\n",
    "      primaryClass={cs.CV}\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "Generated by\n",
    "```\n",
    "PYTORCH_ENABLE_MPS_FALLBACK=1 python scripts/txt2img.py --prompt \"A very realistic object that does not exist in reality.\" --plms --n_samples=4 --n_rows=2 --n_iter=1 --seed 1998 --outdir=\"outputs/impossible-artifact\"\n",
    "```\n",
    "\n",
    "with the following prompts :\n",
    "\n",
    "* `animal-artifact` = \"A very realistic animal that looks like an object.\" \n",
    "* `artifact-animal\"` = \"A very realistic object that looks like an animal.\" \n",
    "* `impossible-animal` = \"A very realistic animal that does not exist in reality.\" \n",
    "* `impossible-artifact` = \"A very realistic object that does not exist in reality.\" \n",
    "* `three-legged-animal` = \"A very realistic animal that has three legs.\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "nteract": {
   "version": "0.28.0"
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
